{
    "2112.01944": "arXiv:2112.01944v1  [cs.IR]  3 Dec 2021\nTowards Low-loss 1-bit Qantization of User-item\nRepresentations for Top-K Recommendation\nYankai Chen1, Yifei Zhang1, Yingxue Zhang2, Huifeng Guo3,\nJingjie Li3, Ruiming Tang3, Xiuqiang He3, Irwin King1\n1The Chinese University of Hong Kong, 2Huawei Noah\u2019s Ark Lab Canada, 3Huawei Noah\u2019s Ark Lab\n{ykchen,yfzhang,king}@cse.cuhk.edu.hk,{yingxue.zhang,huifeng.guo,lijingjie1,tangruiming,hexiuqiang1}@huawei.com\nAbstract\nDue to the promising advantages in space compression and in-\nference acceleration, quantized representation learning for recom-\nmender systems has become an emerging research direction re-\ncently. As the target is to embed latent features in the discrete em-\nbedding space, developing quantization for user-item representa-\ntions with a few low-precision integers confronts the challenge of\nhigh information loss, thus leading to unsatisfactory performance\nin Top-K recommendation. In this work, we study the problem\nof representation learning for recommendation with 1-bit quan-\ntization. We propose a model named Low-loss Quantized Graph\nConvolutional Network (L2Q-GCN). Di\ufb00erent from previous work\nthat plugs quantization as the \ufb01nal encoder of user-item embed-\ndings, L2Q-GCN learns the quantized representations whilst cap-\nturing the structural information of user-item interaction graphs\nat di\ufb00erent semantic levels. This achieves the substantial reten-\ntion of intermediate interactive information, alleviating the feature\nsmoothing issue for ranking caused by numerical quantization. To\nfurther improve the model performance, we also present an ad-\nvanced solution named L2Q-GCN\ud44e\ud45b\ud459with quantization approxima-\ntion and annealing training strategy. We conduct extensive experi-\nments on four benchmarks over Top-K recommendation task. The\nexperimental results show that, with nearly 9\u00d7 representation stor-\nage compression, L2Q-GCN\ud44e\ud45b\ud459attains about 90\u223c99% performance\nrecovery compared to the state-of-the-art model.\n1\nIntroduction\nRecommender systems (RSs), as a useful tool to perform per-\nsonalized information \ufb01ltering [12, 17], nowadays play a critical\nrole throughout various Web applications, e.g., social networks, E-\ncommerce platforms, and streaming media websites. Learning vec-\ntorized user-item representations (a.k.a. embeddings) for predic-\ntion has become the core of modern recommender systems [10, 18,\n19]. Among existing techniques, graph-based methods, i.e., Graph\nConvolutional Networks (GCNs), due to the ability of capturing high-\norder relations in user-item interaction topology, well simulate the\ncollaborative \ufb01ltering process and thus produce a remarkable se-\nmantic enrichment to the user-item representations [17, 18, 38, 43].\nApart from the representation informativeness, space overhead\nis another important criterion for realistic recommender systems.\nWith the explosive growth of interactive data encoded in the graph\nform, quantized representation learning recently provides an alter-\nnative option to GCN-based recommender methods for optimizing\nthe model scalability. Generally, quantization is the process of con-\nverting a continuous range of vectorized values into a \ufb01nite dis-\ncrete set, e.g., integers. Instead of using continuous embeddings,\n\u2026\nMulti-layer CNN\nQuantization\nUsers\nItems\nu1\nu2\nu3\ni3\ni2\ni1\nu2\ni3\ni2\ni1\nu1\nu3\nu3\n1-hop\n2-hop\nInteraction graph\nHigh-order connectivity\n(a) Conventional quantization-based CNN.\n(b) Semantic propagation.\nFigure 1: Illustration of conventional quantized CNN and se-\nmantic propagation within the user-item interaction graph.\ne.g., 32-bit \ufb02oating points, 1-bit quantization however embeds user-\nitem latent features into the binary embedding space, e.g., {\u22121, 1}\ud451.\nBy enabling the usage of low-precision integer arithmetics, 1-bit\nquantized representations have the promising potential in space\ncompression and inference acceleration for recommendation [39].\nDespite the previous attempts to quantize traditional model-based\nRS methods [24, 48, 49], quantizing graph-based RS models [40]\nhowever receive less attention so far. Due to their unique mes-\nsage passing mechanism [17, 26] in encoding high-order interac-\ntive information, it is emerging as a good research topic to study\nrepresentation quantization for GCN-based recommender models.\nCurrently, it is still challenging to approach this target as previous\nwork falls short of satisfaction in terms of recommendation accu-\nracy. The crux of this phenomenon is mainly twofold:\n\u2022 Intuitively, the performance degradation for quantizing repre-\nsentations is mainly caused by the limited expressivity of dis-\ncrete embeddings. Di\ufb00erent from applications in other domains,\ne.g., Natural Language Processing, Computer Vision [4, 6, 15],\nthe top principle of quantization for recommendation is ranking\npreserving. However, compared to full-precision embeddings, the\nvectorized latent features of both users and items tend to be\nsmoothed by the discreteness of quantization naturally. For in-\nstance, after the quantization into binary embedding space {\u22121, 1}\ud451,\nonly the digit signs are kept, no matter what speci\ufb01c values of\ncontinuous embeddings originally are. Consequently, this leads\nto the information loss when estimating users\u2019 preferences to-\nwards di\ufb00erent items, thus drawing a conspicuous performance\ndecay in ranking tasks, e.g., Top-K recommendation.\n\u2022 Technically, previous work mainly takes inspiration from the\nmethodology of Quantized Convolutional Neural Networks [31,\n34, 35] (illustrated in Figure 1(a)) to plug quantization as a sepa-\nrate encoder that is posterior to the GCN architecture. Being the\n\ufb01nal encoder for object embeddings (i.e., users and items), this\nhowever ignores the intermediate representations when graph\nconvolution performs on the sub-structures of interaction graphs.\nAs pointed out by [43], the intermediate information at di\ufb00er-\nent layers of graph convolution is important to reveal di\ufb00erent\nsemantics of user-item interactions. For example, as shown in\nFigure 1(b), when lower-layers propagate information between\nusers and items that have historical interactions, higher-layers\ncapture higher-order proximity of users (or items). Hence, ig-\nnoring intermediate semantics fails to the feature enrichment\nfor embedding quantization. Furthermore, due to the \u201cdouble-\nedged\u201d e\ufb00ect of GCN, \ufb01nal embeddings at the last graph convo-\nlution layer may be over-smoothed [28, 29] to become uninfor-\nmative accordingly. This implies that simply using the output\nembeddings may be risky and problematic [18], which leads to\nthe suboptimal quantized representations for recommendation.\nIn this paper, we investigate the problem of 1-bit quantized rep-\nresentation learning for recommendation with the GCN framework.\nWe proposea model named Low-loss 1-bit QuantizedGraph Convolutional\nNetwork (L2Q-GCN). L2Q-GCN interprets the user-item represen-\ntations in the discrete space {\u22121, 1}\ud451. We design the quantization-\nbased graph convolution such that L2Q-GCN achieves the embed-\nding quantization whilst capturing di\ufb00erent levels of interactive se-\nmantics in exploring the user-item interaction graphs. Intuitively,\nsuch topology-aware quantization makes the user-item represen-\ntations more comprehensive, and thus signi\ufb01cantly alleviates the\ninformation loss of numerical quantization that causes the perfor-\nmance decay. Speci\ufb01cally, we propose two solutions, namely L2Q-\nGCN\ud452\ud45b\ud451and L2Q-GCN\ud44e\ud45b\ud459, to provide \ufb02exibility towards di\ufb00erent\ndeployment scenarios. We conduct extensive experiments on four\nbenchmarks over Top-K recommendation task. To summarize, our\nmain contributions are as follows:\n(1) We implement our proposed network design in L2Q-GCN\ud452\ud45b\ud451\nthat is trained in an end-to-end manner. Our experimental re-\nsults demonstrate that, L2Q-GCN\ud452\ud45b\ud451achieves nearly 11\u00d7 rep-\nresentation compression and about 40% inference acceleration,\nwhile retaining over 80% recommendation capacity, compared\nto the state-of-the-art full-precision model.\n(2) To further improve the recommendation performance, we pro-\npose an advanced solution L2Q-GCN\ud44e\ud45b\ud459with approximation\nthat is trained by a two-step annealing training strategy. With\nslightly additional space cost, L2Q-GCN\ud44e\ud45b\ud459can attain 90\u223c99%\nperformance recovery.\n(3) We release codes and datasets to researchers via the link [1] for\nreproducing and validating.\nOrganization. We \ufb01rst review the related work in Section 2\nand present L2Q-GCN\ud452\ud45b\ud451and L2Q-GCN\ud44e\ud45b\ud459in Sections 3 and 4. We\nreport the experimental results on four benchmarks in Section 5\nand conclude the paper in Section 6.\n2\nRelated Work\n2.1\nFull-precision GCN-based RS Models\nRecently, GCN-based recommender systems have become new\nstate-of-the-art methods for Top-K recommendation [18, 43], thanks\nto their capability of capturing semantic relations and topological\nstructures for user-item interactions [17, 26, 50]. Motivated by the\nadvantage of graph convolution, prior work such as GC-MC [7],\nPinSage [47] and recent state-of-the-art models NGCF [43] and\nLightGCN [18] are proposed. Generally, they adapt the GCN frame-\nwork to simulate the collaborative \ufb01ltering process in high-order\ngraph neighbors for recommendation. For example, NGCF [43] fol-\nlows the GCN-based information propagation rule to learn em-\nbeddings: feature transformation, neighborhood aggregation, and\nnonlinear activation. LightGCN [18] further simpli\ufb01es the graph\nconvolution by retaining the most essential GCN components to\nachieve further improved recommendation performance. In our ex-\nperiments, we settle these two state-of-the-art full-precision meth-\nods as the benchmarking reference for L2Q-GCN in Top-K recom-\nmendation.\n2.2\nGeneral Binarized GCN Frameworks\nNetwork binarization [22] aims to binarize all parameters and\nactivations in neural networks so that they can even be trained by\nlogical units of CPUs. Binarized models will dramatically reduce\nthe memory usage. Despite the progress of binarization CNNs [34,\n35] for multimedia retrieval, this technique is not adequately stud-\nied in geometric deep learning [3, 41, 45]. Bi-GCN [41] and BGCN [3]\nare two recent trials. However, they are mainly designed for geo-\nmetric classi\ufb01cation tasks, but their capability of link prediction (a\ngeometric form of recommendation) is unclear. Furthermore, com-\npared to focusing on the quantization for user-item representa-\ntions only, a complete network binarization will further abate the\nnumerical expressivity of modeling ranking information for rec-\nommendation. This implies that a direct adaptation of binary GCN\nmodels may draw large performance decay in Top-K recommenda-\ntion.\n2.3\nQuantization-based RS Models\nQuantization-based recommender models are attracting grow-\ning attention recently [24, 37, 40, 46]. Compared to network bina-\nrization, they do not pursue extreme model compression, but focus\non quantization for user-item representations with a few integers.\nThese models can be generally categorized into model-based[24,\n46, 48, 49] and graph-based [40]. HashGNN [40], as the state-of-\nthe-art graph-based solution, takes both advantages of graph con-\nvolution and embedding quantization. Speci\ufb01cally, HashGNN [40]\ndesigns a two-step quantization framework by combining Graph-\nSage [17] and learn to hash methodology [13, 16, 42, 52]. Generally,\nLearn to hash aims to learn hash functions for generating discrim-\ninative codes. HashGNN \ufb01rst invokes the two-layer GraphSage as\nthe encoder to get the embeddings for users and items; then it\nstacks a hash layer to get the corresponding binary encodings af-\nterwards. However, the main inadequacy of HashGNN is that, the\nquantization process only proceeds at the end of multi-layer graph\nconvolution, i.e., using the aggregated output of two-layer Graph-\nSage for representation binarization. While multi-layer convolu-\ntion helps to aggregate local information that lives in the consecu-\ntive graph hops, HashGNN may thus not be able to capture inter-\nmediate semantics from nodes\u2019 di\ufb00erent layers of receptive \ufb01elds,\nproducing a suboptimal quantization of node embeddings. Com-\npared to HashGNN, our proposed L2Q-GCN model conducts the\nquantization-based graph convolution when exploring the user-\nitem interaction graph in a layer-wise manner. We justify the ef-\nfectiveness in Section 5.\nUser-item interaction graph\nu1\nu2\nu3\nu4\ni4\ni3\ni2\ni1\nl = 1\nl = 2\nl = 3\nv(0)\nu1\nv(1)\nu1\nv(2)\nu1\nv(3)\nu1\nu1\u2019s neighbors\nNormalized \nSum\ni4\nv(0)\ni1\nv(0)\ni4\ni1\nl = 1\nl = 2\nl = 3\nNormalized \nSum\ni1\u2019s neighbors\nu1\nu3\nv(0)\nu1\nv(0)\nu3\nu2\nv(0)\nu2\nv(0)\ni1\nv(1)\ni1\nv(2)\ni1\nv(3)\ni1\nq(0)\nu1\nq(1)\nu1\nq(2)\nu1\nq(3)\nu1\nq(3)\ni1\nGradient propagation\n\u02c6yu,i\nUsers\nItems\nGraph convolution of continuous item embeddings\nGraph convolution of continuous user embeddings\nLatent feature transformation\n1-bit quantized item representations\n1-bit quantized user representations\nFigure 2: Illustration of L2Q-GCN framwork (Best view in color).\n3\nL2Q-GCN Methodology\n3.1\nProblem Formulation\nUser-item interactions can be represented by a bipartite graph,\ni.e., G = {(\ud462,\ud456)|\ud462\u2208U, \ud456\u2208I}. U and I denote the sets of users and\nitems. We denote \ud466\ud462,\ud456= 1 to indicate there is an observed interac-\ntion between \ud462and \ud456, e,g., browse, click, or purchase, otherwise \ud466\ud462,\ud456\n= 0.\nNotations. We use bold lowercase, bold uppercase, and callig-\nraphy characters to denote vectors, matrices, and sets, respectively.\nNon-bold characters are used to denote graph nodes or scalars. Due\nto the page limit, we summarize all key notations in Appendix A.\nTask Description. Given an interaction graph, the problem\nstudied in this paper is to learn quantized representations Q\ud462and\nQ\ud456for user \ud462and item \ud456, such that the online recommender system\nmodel can predict the probability \u02c6\ud466\ud462,\ud456that user \ud462may adopt item\n\ud456.\n3.2\nBasic Implementation: L2Q-GCN\ud452\ud45b\ud451\nThe general idea of L2Q-GCN is to learn node representations\nby propagating latent features via the graph topology [18, 26, 44].\nIt performs iterative graph convolution, i.e., propagating and aggre-\ngating information of neighbors to update representations (embed-\ndings) of target nodes, which can be formulated as follows:\n\ud497(\ud459)\n\ud465\n= \ud434\ud43a\ud43a\n\u0010\n{\ud497(\ud459\u22121)\n\ud466\n: \ud466\u2208N (\ud465)}\n\u0011\n,\n(1)\nwhere \ud497(\ud459)\n\ud465\ndenotes node \ud465\u2019s embedding after \ud459layers of informa-\ntion propagation. N (\ud465) represents \ud465\u2019s neighbor set and \ud434\ud43a\ud43ais the\naggregation function aiming to transform the center node feature\nand the neighbor features. We illustrate the framework in Figure 2.\n3.2.1\nQantization-based Graph Convolution. We adopt the\ngraph convolution paradigm working on the continuous space from [18]\nthat recently shows good performance under recommendation sce-\nnarios. Let \ud497(\ud459)\n\ud462\n\u2208R\ud450and \ud497(\ud459)\n\ud456\n\u2208R\ud450denote the continuous fea-\nture embeddings of user \ud462and item \ud456computed in the \ud459-th layer.\nThey can be respectively updated by utilizing information from\nthe (\ud459\u22121)-th layer in iteration as follows:\n\ud497(\ud459)\n\ud462\n=\n\u00d5\n\ud456\u2208N(\ud462)\n1\np\n|N (\ud462)| \u00b7 |N (\ud456)|\n\ud497(\ud459\u22121)\n\ud456\n, \ud497(\ud459)\n\ud456\n=\n\u00d5\n\ud462\u2208N(\ud456)\n1\np\n|N (\ud456)| \u00b7 |N (\ud462)|\n\ud497(\ud459\u22121)\n\ud462\n.\n(2)\nAfter getting the intermediate embeddings, e.g., \ud497(\ud459)\n\ud462, we conduct\n1-bit quantization as:\n\ud492(\ud459)\n\ud462\n= sign \u0000\ud47e\ud447\ud497(\ud459)\n\ud462\n\u0001, \ud492(\ud459)\n\ud456\n= sign \u0000\ud47e\ud447\ud497(\ud459)\n\ud456\n\u0001,\n(3)\nwhere \ud47e\u2208\ud445\ud450\u00d7\ud451is a matrix that transforms \ud497(\ud459)\n\ud462\nto \ud451-dimensional\nlatent space for 1-bit quantization. Function sign(\u00b7) maps \ufb02oating-\npoint inputs into the discrete binary space, e.g., {\u22121, 1}\ud451. By do-\ning so, we can obtain the quantized embedding \ud492(\ud459)\n\ud462\nwhilst retain-\ning the latent user features directly from \ud497(\ud459)\n\ud462. For ease of L2Q-\nGCN\ud452\ud45b\ud451\u2019s binarization storage, we can further conduct a numer-\nical translation to these quantized embeddings from {\u22121, 1}\ud451to\n{0, 1}\ud451. After \ud43flayers of feature propagation and quantization, we\nhave built the targeted quantized representations Q\ud462and Q\ud456as:\nQ\ud462= {\ud492(0)\n\ud462, \ud492(1)\n\ud462, \u00b7 \u00b7 \u00b7 , \ud492(\ud43f)\n\ud462\n}, Q\ud456= {\ud492(0)\n\ud456\n, \ud492(1)\n\ud456\n, \u00b7 \u00b7 \u00b7 , \ud492(\ud43f)\n\ud456\n}.\n(4)\nBoth Q\ud462and Q\ud456track the intermediate information binarized from\nfull-precision embeddings at di\ufb00erent layers. Intuitively, they rep-\nresent the interactive information that is propagated back and forth\nbetween users and items within these layers, simulating the col-\nlaborative \ufb01ltering e\ufb00ect to exhibit in the quantized encodings for\nrecommendation.\n3.2.2\nModel Prediction. Based on the quantized representations\nof users and items, i.e., Q\ud462and Q\ud456, we predict the matching scores\nby naturally adopting the inner product as:\n\u02c6\ud466\ud462,\ud456= \ud453(Q\ud462)\ud447\u00b7 \ud453(Q\ud456),\n(5)\nwhere function \ud453abstracts the way to utilize Q\ud462and Q\ud456. In this\npaper, we implement \ud453by taking an element-wise summation:\n\ud453(Q\ud462) =\n\ud43f\n\u00d5\n\ud459=0\n\ud492(\ud459)\n\ud462, \ud453(Q\ud456) =\n\ud43f\n\u00d5\n\ud459=0\n\ud492(\ud459)\n\ud456\n.\n(6)\nClari\ufb01cation. In this work, we interpret quantized representations as\nintegers and apply integer arithmetics in computation. As we will\nshow later in experiments, this introduces about 40% of computation\nacceleration. Certainly, it can be extended to binary arithmetic logics\nfor further optimization, and we leave it for future work.\n3.2.3\nModel Optimization. We then introduce ourobjective and\nbackward-propagation strategy for optimization.\nObjective Function. Our objective function consists of two\ncomponents, i.e., graph reconstruction loss L\ud45f\ud452\ud450and BPR loss L\ud44f\ud45d\ud45f.\nThe motivation of such design is basically twofold:\n\u2022 L\ud45f\ud452\ud450reconstructs the observed topology of interaction graphs;\n\u2022 L\ud44f\ud45d\ud45flearns the relative rankings of user preferences towards\ndi\ufb00erent items.\nConcretely, we implement L\ud45f\ud452\ud450with the cross-entropy loss:\nL\ud45f\ud452\ud450=\n\u00d5\n\ud462\u2208U\n\u00d5\n\ud456\u2208I\n\ud466\ud462,\ud456ln\ud70e\n\u0010\n(\ud497(\ud43f)\n\ud462\n)\ud447\u00b7 \ud497(\ud43f)\n\ud456\n\u0011\n+ (1 \u2212\ud466\ud462,\ud456) ln\n\u0010\n1 \u2212\ud70e\u0000(\ud497(\ud43f)\n\ud462\n)\ud447\u00b7 \ud497(\ud43f)\n\ud456\n\u0001\u0011\n,\n(7)\nwhere \ud70eis the activation function, e.g., Sigmoid. L\ud45f\ud452\ud450bases on\nthe full-precision embeddings at the last layer, e.g., \ud497(\ud43f)\n\ud462, providing\nthe latest intermediate information for topology reconstruction as\nmuch as possible. As for L\ud44f\ud45d\ud45f, we employ Bayesian Personalized\nRanking (BPR) loss [36] as follows:\nL\ud44f\ud45d\ud45f= \u2212\n\u00d5\n\ud462\u2208U\n\u00d5\n\ud456\u2208N(\ud462)\n\u00d5\n\ud457\u2209N(\ud462)\nln\ud70e( \u02c6\ud466\ud462,\ud456\u2212\u02c6\ud466\ud462,\ud457).\n(8)\nL\ud44f\ud45d\ud45frelies on the quantized node representations, e.g., Q\ud462, to en-\ncourage the prediction of an observed interaction to be higher than\nits unobserved counterparts [18]. Finally, our \ufb01nal objective func-\ntion is de\ufb01ned as:\nL = L\ud45f\ud452\ud450+ L\ud44f\ud45d\ud45f+ \ud706||\u0398||2\n2,\n(9)\nwhere \u0398 is the set of trainable parameters and embeddings, and\n||\u0398||2\n2 is the \ud43f2-regularizer parameterized by \ud706to avoid over-\ufb01tting.\nClari\ufb01cation. Please notice that at the training stage, we usually take\nthe mean value of \ud453(Q\ud462) by shrinking it as \ud453(Q\ud462) = \ud453(Q\ud462)/(\ud43f+ 1)\n(likewise for \ud453(Q\ud456)). Essentially, this useful training strategy [18, 21]\nreduces the absolute value of predicted scores to a smaller scale, which\nsigni\ufb01cantly stabilizes the training process to avoid the undesirable\ndivergence in embedding optimization; but most importantly, it has\nno e\ufb00ect on the relative rankings of all scores.\nBackward-propagation Strategy.Unfortunately, the sign func-\ntion is not di\ufb00erentiable. This means that the original derivative of\nthe sign function is 0 almost everywhere, making the intermediate\ngradients accumulated before quantization zeroed here. To avoid\nthis and approximate the gradients for backward propagation, we\nadopt the Straight-Through Estimator [5] with gradient clipping as:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n\ud492(\ud459)\n\ud462\n= sign \u0000\ud719\u0001,\nforward propagation\n\ud715\ud492(\ud459)\n\ud462\n\ud715\ud719\n:= 1|\ud719|\u22641.\nbackward propagation\n(10)\nDerivative 1|\ud719|\u22641 can be viewed as passing gradients e.g., \u0394, through\nhard-tanh function [11], i.e., max(\u22121, min(1, \u0394)). This passes gra-\ndients backwards unchanged when the input of sign function, e.g.,\n\ud719, is within range of {-1, 1}, and cancels the gradient \ufb02ow other-\nwise [2]. We illustrate the process of gradient propagation in Fig-\nure 2.\nSo far, we have already introduced the skeleton of our proposed\nnetwork that learns the quantized representations Q\ud462and Q\ud456via\n(a) Full-precision version.\n(b) Quantization version.\nFigure 3: Loss landscapes visualization.\nexploring the interaction graph topology. Since it can be trained in\nan end-to-end manner, we directly name it as L2Q-GCN\ud452\ud45b\ud451.\n4\nAdvanced Solution: L2Q-GCN\ud44e\ud45b\ud459\nAlthough L2Q-GCN\ud452\ud45b\ud451can achieve the quantization for user-\nitem representations, we argue that there may still exist avenues\nfor further improvement. In this section, we \ufb01rst explain the di\ufb03-\nculty of quantization in L2Q-GCN\ud452\ud45b\ud451, and then give an advanced\nsolutionwith the annealing training strategy, namely L2Q-GCN\ud44e\ud45b\ud459,\nto enhance the recommendation performance.\n4.1\nDi\ufb03culty of Quantization in L2Q-GCN\ud452\ud45b\ud451\nTo show it may be challenging to directly quantize L2Q-GCN\ud452\ud45b\ud451\nfor binary representation, we simulate the optimization trajecto-\nries of learnable embeddings and visually compare the loss land-\nscapes of it with its full-precision version (excluding the quanti-\nzation component) in Figure 3. Concretely, following [4, 33], we\nmanually assign perturbations to the learnable user-item embed-\ndings as follows:\n\ud497(\ud459)\n\ud462\n= \ud497(\ud459)\n\ud462\n\u00b1 \ud45d\u00b7 |\ud497(\ud459)\n\ud462| \u00b7 1(\ud459)\n\ud462,\n\ud497(\ud459)\n\ud456\n= \ud497(\ud459)\n\ud456\n\u00b1 \ud45d\u00b7 |\ud497(\ud459)\n\ud462| \u00b7 1(\ud459)\n\ud456,\n(11)\nwhere |\ud497(\ud459)\n\ud462| represents the absolute mean value of embedding \ud497(\ud459)\n\ud462\nand perturbation magnitudes \ud45dare from {0.01, 0.02, \u00b7 \u00b7 \u00b7 , 0.50}. 1\ud462\nis an all-one vector. For each pair of perturbed user-item represen-\ntations, we plot the loss distribution accordingly.\nAs we can observe, the full-precision version with no quanti-\nzation produces a \ufb02at and smooth loss surface, showing the local\nconvexity and thus easy to optimize. On the contrary, L2Q-GCN\ud452\ud45b\ud451\nhas a bumping and complex loss landscape. The steep loss curva-\nture re\ufb02ects L2Q-GCN\ud452\ud45b\ud451is more sensitive to perturbation, show-\ning the di\ufb03culty in quantization optimization.\n4.2\nUpgrading in L2Q-GCN\ud44e\ud45b\ud459\nTo alleviate the perturbation sensitivity and further improve the\nmodel performance, we propose L2Q-GCN\ud44e\ud45b\ud459.\n4.2.1\nQantization with Rescaling Approximation. Our pro-\nposedL2Q-GCN\ud44e\ud45b\ud459additionally includes layer-wise positive rescal-\ning factors for each node, e.g., \ud6fc(\ud459)\n\ud462\n\u2208R+, such that \ud497(\ud459)\n\ud462\n\u2248\ud6fc(\ud459)\n\ud462\ud492(\ud459)\n\ud462.\nIn this work, we introduce a simple but e\ufb00ective approach to di-\nrectly calculate these rescaling factors as:\n\ud6fc(\ud459)\n\ud462\n= ||\ud497(\ud459)\n\ud462||1\n\ud451\n, \ud6fc(\ud459)\n\ud456\n=\n||\ud497(\ud459)\n\ud456\n||1\n\ud451\n.\n(12)\nInstead of setting \ud6fc(\ud459)\n\ud462\nas learnable, such deterministic computa-\ntion substantially prunes the search space of parameters whilst at-\ntaining the approximation functionality. We demonstrate this in\nSection 5.6.\nBased on the quantized user-item representations and the cor-\nresponding rescaling factors, we have:\nA\ud462= {\ud6fc(0)\n\ud462\ud492(0)\n\ud462, \ud6fc(1)\n\ud462\ud492(1)\n\ud462, \u00b7 \u00b7 \u00b7 , \ud6fc(\ud43f)\n\ud462\n\ud492(\ud43f)\n\ud462\n}, A\ud456= {\ud6fc(0)\n\ud456\n\ud492(0)\n\ud456\n,\ud6fc(1)\n\ud456\n\ud492(1)\n\ud456\n, \u00b7 \u00b7 \u00b7 ,\ud6fc(\ud43f)\n\ud456\n\ud492(\ud43f)\n\ud456\n}.\n(13)\nConsequently, L2Q-GCN\ud44e\ud45b\ud459approximates Q\ud462and Q\ud456by A\ud462and\nA\ud456, and updates Equations 5 and 6 for model prediction accord-\ningly.\nSpace Cost Analysis. The total space cost of L2Q-GCN\ud452\ud45b\ud451for\nstoring the quantized user-item representations is \ud442((\ud43f+ 1)\ud441\ud451)\n(or bits), where \ud441is the number of users and items and \ud451is the di-\nmension of binary embeddings, e.g., \ud497(\ud459)\n\ud462. Furthermore, since L2Q-\nGCN\ud44e\ud45b\ud459develops quantization with approximation, supposing that\nwe use 32-bit \ufb02oating-points for those rescaling factors, the space\ncost is \ud442((\ud43f+1)\ud441(\ud451+32)) in total. Compared to the full-precision\nembedding table at each single one layer, e.g., 32-bit \ufb02oating-point\n\ud497(\ud459)\n\ud462, Q\ud462and Q\ud456have the following theoretical compression ratios:\n\ud45f\ud44e\ud461\ud456\ud45c\ud452\ud45b\ud451=\n32\ud441\ud451\n(\ud43f+ 1) \u00b7 \ud441\ud451=\n32\n\ud43f+ 1, \ud45f\ud44e\ud461\ud456\ud45c\ud44e\ud45b\ud459=\n32\ud441\ud451\n(\ud43f+ 1) \u00b7 \ud441(\ud451+ 32) =\n32\ud451\n(\ud43f+ 1)(\ud451+ 32) .\n(14)\nNormally, stacking too many layers will cause the over-smoothing\nproblem [28, 29], incurring performance detriment. Hence, A com-\nmon setting for \ud43fis \ud43f< 5 [17, 18, 26, 43], which can still achieve\nconsiderable embedding compression.\n4.2.2\nAnnealing Training Strategy. Another constructive de-\nsign of L2Q-GCN\ud44e\ud45b\ud459is the two-step annealing training strategy:\n(1) we \ufb01rst mask the quantization function and train L2Q-GCN\ud44e\ud45b\ud459\nwith the full-precision embeddings;\n(2) when it converges to optimum, we trigger the quantization af-\nterwards to \ufb01nd the targeted quantized representations.\nIntuitively, L2Q-GCN\ud44e\ud45b\ud459moves from a tractable training space\nto the targeted one for quantization. This can avoid unnecessary\nexploration towards di\ufb00erent optimization directions at the begin-\nning of quantization, guaranteeing the numerical stability in the\nwhole model training. Then at the middle period of training when\ntriggering quantization, as presented in Figure 4, L2Q-GCN\ud44e\ud45b\ud459\ufb01rstly\nmeets a performance retracement, but shortly afterwards, it recov-\ners and continues to converge. As we will demonstrate later in ex-\nperiments, this straightforward but e\ufb00ective strategy can further\nproduce better quantization representations with approximation\nfor Top-K recommendation.\nClari\ufb01cation. we pointout that the timecost of trainingL2Q-GCN\ud44e\ud45b\ud459\nshares the same order of magnitude with L2Q-GCN\ud452\ud45b\ud451. In our imple-\nmentation, we simply assign half of the total training epochs for the\n\ufb01rst step and leave the second half for quantization. One may also\nopt for more \ufb02exible strategy to trigger the quantization, e.g., early-\nstopping, or full-precision version pre-training.\nRecall@K\nNDCG@K\nK=40\nK=20\nK=60\nK=80\nK=100\nK=40\nK=20\nK=60\nK=80\nK=100\nFigure 4: Annealing performance on MovieLens dataset.\nSo far, we have introduced all technical details of the proposed\nL2Q-GCN\ud452\ud45b\ud451and L2Q-GCN\ud44e\ud45b\ud459. For the corresponding pseudocodes,\nplease refer to Appendix B. In the following section, we present the\nexperimental results and analysis on our models.\n5\nExperimental Results\nWe evaluate our model on Top-K recommendation task with the\naim of answering the following research questions:\n\u2022 RQ1. How does L2Q-GCN perform compared to state-of-the-art\nfull-precision and quantization-based recommender models?\n\u2022 RQ2. How is resource consumption of L2Q-GCN?\n\u2022 RQ3. How do proposed components of L2Q-GCN\ud452\ud45b\ud451and L2Q-\nGCN\ud44e\ud45b\ud459a\ufb00ect the performance?\n5.1\nDataset\n\u2022 MovieLens1 is a widely adopted benchmark for movie recom-\nmendation. Similar to the setting in [9, 20, 40], \ud466\ud462,\ud456= 1 if user\n\ud462has an explicit rating score towards item \ud456, otherwise \ud466\ud462,\ud456= 0.\nIn this paper, we use the MovieLens-1M data split.\n\u2022 Gowalla2 is the check-in dataset [30] collected from Gowalla,\nwhere users share their locations by check-in. To guarantee the\nquality of the dataset, we extract users and items with no less\nthan 10 interactions similar to [18, 40, 43].\n\u2022 Pinterest3 is an implicit feedback dataset for image recommen-\ndation [14]. Users and images are modeled in a graph. Edges rep-\nresent the pins over images initiated by users. In this dataset,\neach user has at least 20 edges. We delete repeated edges be-\ntween users and images to avoid data leakage in model evalua-\ntion.\n\u2022 Yelp20184 is collected from Yelp Challenge 2018 Edition. In this\ndataset, local businesses such as restaurants are treated as items.\nWe retain users and items with over 10 interactions similar to [43].\nTable 1: Datasets.\n# Users\n# Items\n# Interactions\nDensity\nMovieLens\n6,040\n3,952\n1,000,209\n0.04190\nGowalla\n29,858\n40,981\n1,027,370\n0.00084\nPinterest\n55,186\n9,916\n1,463,556\n0.00267\nYelp2018\n31,668\n38,048\n1,561,406\n0.00130\n1https://grouplens.org/datasets/movielens/1m/\n2https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/gowalla\n3https://sites.google.com/site/xueatalphabeta/dataset-1/pinterest_iccv\n4https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/yelp2018\nTable 2: Performance comparison (underline represents the best performing model; R and D refer to Recall and NDCG).\nModel\nMovieLens (%)\nGowalla (%)\nPinterest (%)\nYelp2018 (%)\nR@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100\nLSH\n9.35\n23.41\n13.45\n33.49\n6.23\n15.69\n10.23\n19.23\n6.41\n16.55\n8.56\n15.32\n2.73\n8.77\n4.73\n11.63\nHashNet\n14.98\n35.67\n23.41\n38.14\n10.11\n24.19\n15.31\n23.11\n8.93\n29.43\n10.37\n21.63\n2.64\n9.45\n6.42\n14.68\nHashGNN\n12.55\n34.54\n23.63\n36.54\n9.63\n22.13\n15.85\n24.01\n8.01\n25.27\n10.48\n21.19\n3.22\n10.96\n6.62\n14.61\nHashGNN-soft 18.54\n41.83\n36.56\n55.16\n11.49\n25.88\n17.84\n26.53\n10.87\n34.14\n12.35\n24.99\n4.29\n14.03\n8.30\n17.73\nQuant-gumbel 17.48\n42.08\n34.57\n56.50\n10.78\n26.58\n15.62\n25.08\n9.78\n30.63\n11.15\n22.92\n3.91\n13.00\n8.07\n17.08\nNeurCF\n19.43\n48.04\n36.85\n51.71\n13.95\n32.76\n22.21\n27.39\n9.09\n29.52\n13.55\n22.24\n3.12\n10.97\n6.45\n14.91\nNGCF\n24.35\n54.92\n37.30\n53.64\n15.53\n33.32\n23.21\n28.44\n14.30\n39.74\n13.01\n27.06\n5.45\n15.32\n8.73\n19.33\nLightGCN\n25.01\n55.71\n44.73\n65.09\n17.80\n37.02\n24.76\n34.91\n14.78\n39.98\n15.91\n28.93\n6.11\n18.05\n10.95\n21.59\nL2Q-GCN\ud452\ud45b\ud451\n20.52\n49.27\n38.41\n60.03\n14.62\n32.24\n21.24\n30.97\n12.52\n35.67\n13.92\n26.37\n5.10\n16.31\n9.62\n19.88\n% Capacity\n82.05% 88.44%\n85.87%\n92.23%\n82.13% 87.10%\n85.78%\n88.71%\n84.71% 89.22%\n87.49%\n91.15%\n83.47% 90.36%\n87.85%\n92.08%\nL2Q-GCN\ud44e\ud45b\ud459\n22.81\n51.96\n42.44\n62.96\n16.12\n34.39\n23.62\n33.52\n13.87\n38.25\n15.31\n28.13\n5.74\n17.63\n10.67\n21.32\n% Capacity\n91.20% 93.27%\n94.88%\n96.73%\n90.56% 92.90%\n95.40%\n96.02%\n93.84% 95.67%\n96.23%\n97.23%\n93.94% 97.67%\n97.44%\n98.75%\n5.2\nCompeting Methods\nWe compare our model with two main streams of methods: (1)\nfull-precision recommender systems including CF-based methods\n(NeurCF), and GCN-based models (NGCF, LightGCN), (2) 1-bit quantization-\nbased models for general item retrieval tasks (LSH, HashNet) and\nfor Top-K recommendation (HashGNN).\n\u2022 LSH [16] is a classical hashing method. LSH is \ufb01rstly proposedto\napproximate the similarity search for massive high-dimensional\ndata and we introduce it for Top-K recommendation by follow-\ning the adaptation in [40].\n\u2022 HashNet [8] is a state-of-the-art deep hashing method that is\noriginally proposedfor multimedia retrieval tasks. Similar to [40],\nwe adapt it for graph data mainly by replacing the used AlexNet [27]\nwith the general graph convolutional network.\n\u2022 HashGNN [40] is the state-of-the-art 1-bit quantization-based\nrecommender system method with GCN framework. We use HashGNN\nto denote the vanilla version with hard encoding proposedin [40],\nwhere each element of quantized user-item embeddings is strictly\nquantized. We use HashGNN-soft to represent the relaxed ver-\nsion proposed in [40], where it adopts a Bernoulli random vari-\nable to provide the probability of replacing the quantized digits\nwith continuous values in the original embeddings.\n\u2022 NeurCF [19] is one state-of-the-art neural network model for\ncollaborative \ufb01ltering. NeurCF models latent features of users\nand items to capture their nonlinear feature interactions.\n\u2022 NGCF [43] is one of the state-of-the-art GCN-based recommender\nmodels. We compare L2\ud444-GCN with NGCF mainly to study their\nperformance capabilities in Top-K recommendation.\n\u2022 LightGCN [18] is the latest state-of-the-art GCN-based recom-\nmendation model that has been widely evaluated. We include\nLightGCN in our experiment mainly to set up the benchmark-\ning as a reference of full-precision recommendation capability.\n\u2022 Quant-gumbel is a variance of L2Q-GCN with the implementa-\ntion of Gumbel-softmax for quantization [23, 32, 51]. We \ufb01rst ex-\npand each embedding bit to a size-two one-hot encoding. Then\nQuant-gumbel utilizes the Gumbel-softmax trick to replace sign\nfunction as relaxation for binary code generation.\n5.3\nExperiment Setup\nIn the evaluation of Top-K recommendation, we apply the learned\nuser-item representations to rank \ud43eitems for each user with the\nhighest predicted scores, i.e., \u02c6\ud466\ud462,\ud456. We choose two widely-used eval-\nuation protocols Recall@\ud43eand NDCG@\ud43eto evaluate Top-K rec-\nommendation capability.\nWe implement L2Q-GCN model under Python 3.7 and PyTorch\n1.14.0 with non-distributed training. The experiments are run on\na Linux machine with 4 NVIDIA V100 GPU, 4 Intel Core i7-8700\nCPUs, 32 GB of RAM with 3.20GHz. For all the baselines, we follow\nthe o\ufb03cial hyper-parameter settings from original papers or as de-\nfault in corresponding codes. For methods lacking recommended\nsettings, we apply a grid search for hyper-parameters. The embed-\nding dimension is searched in {32, 64, 128, 256, 512}. The learning\nrate \ud702is tuned within {10\u22123, 5 \u00d7 10\u22123, 10\u22122, 5 \u00d7 10\u22122} and the coe\ufb03-\ncient of \ud43f2 normalization \ud706is tuned among {10\u22125, 10\u22124, 10\u22123}. We\ninitialize and optimize all models with default normal initializer\nand Adam optimizer [25]. To guarantee reproducibility, we report\nall the hyper-parameter settings in Appendix C.\n5.4\nPerformance Analysis (RQ1)\nIn this section, we present a comprehensive performance analy-\nsis between L2\ud444-GCN with two layers and competing recommender\nmodels of full-precision-based and quantization-based. We eval-\nuate Top-K recommendation over four datasets by varying K in\n{20, 40, 60, 80, 100}. To achieve a more detailed performance com-\nparison, we summarize the results of Top@20 and Top@100 rec-\nommendation in Table 2. We also curve their complete results of\nRecall@K and NDCG@K metrics and attach them in Appendix D.\nGenerally, L2\ud444-GCN has made great improvements over quantization-\nbased models and shows the competitive performance compared to\nfull-precision models. We have the following observations:\n\u2022 The results demonstrate the superiority of L2\ud444-GCN over\nall quantization-based models. (1) As shown in Table 2, the\nstate-of-the-art quantization-based GCN model, i.e., HashGNN\n(and HashGNN-soft), works better than traditional quantization-\nbased baselines, e.g., LSH, HashNet. This shows the e\ufb00ectiveness\nof graph convolutional architecture in capturing latent informa-\ntion within interaction graphs for quantization preparation and\nindicates that a direct adaptation of conventional quantization\nmethods may not well handle the Top-K recommendation task.\n(2) Furthermore, thanks to our proposedquantization-based graph\nconvolution design, both L2\ud444-GCN\ud452\ud45b\ud451and L2\ud444-GCN\ud44e\ud45b\ud459consis-\ntently outperform HashGNN and its relaxed version HashGNN-\nsoft. The main reason is that, our topology-aware quantization\nsigni\ufb01cantly enriches the user-item representations and allevi-\nates the feature smoothing issue caused by the numerical quan-\ntization. We conduct the ablation study on this in the later sec-\ntion.\n\u2022 Compared to full-precision models, L2\ud444-GCN presents a\ncompetitive performance recovery. (1) L2\ud444-GCN\ud452\ud45b\ud451shows\nthe performance superiority over traditional collaborative \ufb01lter-\ning model, i.e., NeurCF. Considering the large improvement of\ntwo GCN-based models, i.e., NGCF and LightGCN, against NeurCF,\nwe can infer the performance gap between L2\ud444-GCN\ud452\ud45b\ud451and\nNeurCF mainly comes from the proposedquantization-based graph\nconvolution. (2) Compared to the best model LightGCN, taking\nRecall metric as an example, L2\ud444-GCN\ud452\ud45b\ud451shows about 82\u223c85%\nand 87\u223c90%performance capacity in terms of Top@20 and Top@100.\nThis indicates that, as the value of K increases, L2\ud444-GCN\ud452\ud45b\ud451can\nfurther improve the recommendation accuracy and narrow the\ngap to LightGCN. (3) Moreover, by utilizing the quantization ap-\nproximation and annealing training strategy, L2\ud444-GCN\ud44e\ud45b\ud459can\nachieve betterperformance than NGCF on Gowalla and Yelp2018\ndatasets. Compared to our basic implementation L2\ud444-GCN\ud452\ud45b\ud451,\nL2\ud444-GCN\ud44e\ud45b\ud459further improves the performance recovery by 8\u223c10%\nand 9\u223c10%in terms of Recall@20 and Recall@100 across all bench-\nmarks, proving the e\ufb00ectiveness of our proposed modi\ufb01cation in\nL2\ud444-GCN\ud44e\ud45b\ud459. (4) In addition, with K increasing up to 100, L2\ud444-\nGCN\ud44e\ud45b\ud459presents a similar trend with L2\ud444-GCN\ud452\ud45b\ud451such that it\nperforms even closely to LightGCN, i.e., 93\u223c98% and 96\u223c99% in\nterms of Recall and NDCG, respectively. In a nutshell, the pre-\ndiction capability of both L2\ud444-GCN\ud452\ud45b\ud451and L2\ud444-GCN\ud44e\ud45b\ud459devel-\nops from \ufb01ne-grained ranking tasks to coarse-grained ones, e.g.,\nTop-20 to Top-100 recommendation.\n\u2022 Both L2\ud444-GCN\ud452\ud45b\ud451and L2\ud444-GCN\ud44e\ud45b\ud459show the deployment\n\ufb02exibility towards di\ufb00erent application scenarios. In the\npipeline of industrial recommender systems, recall and re-ranking\nare two important stages that substantially in\ufb02uence recommen-\ndation quality. Recall refers to the process of quickly retriev-\ning candidate items from the whole item pool that a given user\nmay interest. Based on the more complex scoring algorithms, re-\nranking outputs a precise ranking list of candidate items.\nOn the one hand, as we will show later, L2\ud444-GCN\ud452\ud45b\ud451can speed\nup about 40% for candidate generation. Considering its perfor-\nmance improvement on coarse-grained ranking tasks, e.g., Top-\n100 recommendation, L2\ud444-GCN\ud452\ud45b\ud451actually provides an alterna-\ntive option to accelerate the recall stage. On the other hand, L2\ud444-\nGCN\ud44e\ud45b\ud459further optimizes the recommendation accuracy that\nperforms similarly to LightGCN. Since L2\ud444-GCN\ud44e\ud45b\ud459reduces the\nembedding storage cost to about 9\u00d7, we can treat L2\ud444-GCN\ud44e\ud45b\ud459\nas a substitute for the best model, i.e., LightGCN, as a trade-o\ufb00\nbetween space cost and prediction accuracy. We report the de-\ntails of space and time cost for embedding storage and online\ninference in the following section.\n5.5\nResource Consumption Analysis (RQ2)\nIn this section, we study the resource consumption in embed-\nding storage and inference time cost. We compare our models with\nthe state-all-the-art full-precision model and quantization-based\nmodel, i.e., LightGCN and HashGNN. We take their two-layer struc-\ntures with the same 128-dimensional embeddings as reference and\nillustrate with the largest dataset, i.e., Yelp2018, in Figure 5(a). Ob-\nservations in this section can be popularizedto other three datasets\nand we report the complete results in Appendix D.\nEmbeddingstorage compression. Quantized embeddings can\nlargely reduce the space consumption for o\ufb04ine disk storage. To\nmeasure the embedding size, we save these embeddings to the disk\nsuch that they can recover the well-trained user-item representa-\ntions for inference. As we can observe, after the binarization for\nuser-item embeddings, L2Q-GCN\ud452\ud45b\ud451and L2Q-GCN\ud44e\ud45b\ud459can achieve\nthe space reduction with a factor of about 11\u00d7 and 9\u00d7, respectively.\nThis basically follows the theoretical bounds that are computed in\nEquation 14, i.e., \ud45f\ud44e\ud461\ud456\ud45c\ud452\ud45b\ud451and \ud45f\ud44e\ud461\ud456\ud45c\ud44e\ud45b\ud459when \ud43f= 2. Furthermore,\nconsidering the performance improvement, the space usage of ap-\nproximation factors of L2Q-GCN\ud44e\ud45b\ud459is actually acceptable, which\nmay dispel the concerns of large additional storage overhead.\nOnline inference acceleration. We evaluate the time cost in-\ncluding the score estimation and sorting. L2Q-GCN predicts the\nscores between users and items by conducting embedding multi-\nplications. At the stage of online inference, we \ufb01rst interpret these\nbinary embeddings by signed integers, e.g., int8, and then conduct\ninteger arithmetics including integer summations and matrix mul-\ntiplications. Please notice that we leave the development of bit-\nwise operations for online inference for future work. To give a\nfair comparison on the inference time cost, we disable all arith-\nmetic optimization such as BLAS, MKL, and conduct the experi-\nments using the vanilla NumPy provided by 5. As shown in Fig-\nure 5(a), purely based on the quantized embeddings, L2Q-GCN\ud452\ud45b\ud451\ncan achieve 39.83% (118.23\u2192196.49) of inference acceleration. L2Q-\nGCN\ud44e\ud45b\ud459takes a similar running time with LightGCN as it intro-\nduces the approximation factors in score estimation. Furthermore,\nFigure 5(b) visualizes the overall evaluation in terms of resource\nconsumption and recommendation accuracy. As the cube\u2019s front-\nhigh corner means the ideal optimal performance, our proposed\nmethods make a good balance w.r.t consumption and accuracy.\n5.6\nAblation Study of L2Q-GCN Model (RQ3)\nWe evaluate the necessity of each model component in both\nL2Q-GCN\ud452\ud45b\ud451and L2Q-GCN\ud44e\ud45b\ud459. Due to the page limits, we report\nthe Top-20 recommendation results as a reference in Table 3.\n5https://www.lfd.uci.edu/~gohlke/pythonlibs/\nYelp2018 \n(a) Space and time cost.\n(b)  Overall evaluation \nModel\nYelp2018\nE.S.\nI.T.\nLightGCN\n34.04\n196.49\nHashGNN\n1.06\n117.43\nHashGNN-soft\n34.13\n197.44\nL2Q-GCN\ud452\ud45b\ud451\n3.19\n118.23\nratio\n10.66\u00d7\n1.66\u00d7\nL2Q-GCN\ud44e\ud45b\ud459\n3.98\n194.84\nratio\n8.55\u00d7\n1.01\u00d7\nFigure 5: Results of two-layer networks with 128-dimension\nembeddings (E.S. and I.T. are the abbreviations of embedding\nsize (MB) and inference time (s). Best view in color).\n5.6.1\nE\ufb00ect of Topology-aware Qantization. To substanti-\nate the impact of topology-aware quantization in graph convolu-\ntion, we give a variant of L2Q-GCN\ud452\ud45b\ud451, i.e., w/o TQ, by disabling the\nlayer-wise quantization and setting it as the \ufb01nal encoder of full-\nprecision graph convolution. As we can observe in Table 3, vari-\nant w/o TQ remarkably underperforms L2Q-GCN\ud452\ud45b\ud451. This demon-\nstrates that simply using the latest-updated embeddings from the\nGCN framework may not su\ufb03ciently model the unique latent fea-\ntures of both users and items, especially for the quantization-based\nranking. Via capturing the intermediate information for represen-\ntation enrichment, our topology-aware quantization can e\ufb00ectively\nalleviate the ranking smoothness issue caused by the limited ex-\npressivity of discrete embeddings. This helps to make the quan-\ntized representations of both users and items more discriminative,\nwhich leads to the performance improvement on Top-K recommen-\ndation.\n5.6.2\nE\ufb00ect of Multi-loss in Optimization. To study the ef-\nfect of BPR loss L\ud44f\ud45d\ud45fand graph reconstruction loss L\ud45f\ud452\ud450, we set\ntwo variants, termed by w/o L\ud44f\ud45d\ud45fand w/o L\ud45f\ud452\ud450, to optimize L2Q-\nGCN\ud452\ud45b\ud451separately. As shown in Table 3, with all other model com-\nponents, partially using one of L\ud44f\ud45d\ud45fand L\ud45f\ud452\ud450produces large per-\nformance decay to L2Q-GCN\ud452\ud45b\ud451. This con\ufb01rms the e\ufb00ectiveness\nof our proposed muti-loss design: while L\ud44f\ud45d\ud45fassigns higher pre-\ndiction values to observed interactions, 1.e., \ud466\ud462,\ud456= 1, than the un-\nobserved user-item pairs, L\ud45f\ud452\ud450transfers the graph reconstruction\nproblem to a classi\ufb01cation task by using the full-precision embed-\ndings in training. By collectively optimizing these two loss func-\ntions, L2Q-GCN\ud452\ud45b\ud451can learn precise intermediate embeddings from\nL\ud45f\ud452\ud450, and produce quantized representations with high-quality rel-\native order information regularized by L\ud44f\ud45d\ud45faccordingly.\n5.6.3\nE\ufb00ect of Rescaling Approximation. We now discuss the\ne\ufb00ect of approximation factors in L2Q-GCN\ud44e\ud45b\ud459. We create two vari-\nants, namely w/o RAF and w/in LF. w/o RAF directly removes the\nrescaling approximation factors and w/in LF means replacing our\noriginal approximation factors with learnable ones. We optimize\nboth variants w/o RAF and w/in LF with the annealing training\nstrategy. (1) The performance decline of w/o RAF proves the e\ufb00ec-\ntiveness of rescaling approximation for user-item representations.\nAlthough these factors are directly calculated and may not be the-\noretically optimal, they re\ufb02ect the numerical uniqueness of embed-\ndings for both users and items, which substantially improves L2Q-\nGCN\ud44e\ud45b\ud459\u2019s prediction capability. (2) As for w/in LF, the design of\nTable 3: Ablation study.\nVariant\nMovieLens\nGowalla\nPinterest\nYelp2018\nR@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20\nL2Q-GCN\ud452\ud45b\ud459\nw/o TQ\n17.73 35.31 11.63 15.58 10.29 11.60\n4.33\n8.58\n-13.60% -8.07% -20.45% -26.65% -17.81% -16.67% -15.10% -10.81%\nw/o L\ud44f\ud45d\ud45f\n19.67 37.22\n8.66\n13.33\n5.12\n6.01\n3.52\n7.33\n-4.14% -3.10% -40.77% -37.24% -59.11% -56.82% -30.98% -23.80%\nw/o L\ud45f\ud452\ud45016.98 32.76\n8.32\n9.28\n10.86 11.55\n3.33\n6.60\n-17.25% -14.71% -43.09% -56.31% -13.26% -17.03% -34.71% -31.39%\nBest\n20.52 38.41 14.62 21.24 12.52 13.92\n5.10\n9.62\nL2Q-GCN\ud44e\ud45b\ud459\nw/o RAF 20.86 38.14 10.29 12.10 11.19 12.11\n4.25\n8.09\n-8.55% -10.13% -36.17% -48.77% -19.32% -20.90% -25.96% -24.18%\nw/in LF\n20.05 38.25 14.53 21.23 12.35 13.65\n5.56\n10.20\n-12.10% -9.87% -9.86% -10.12% -10.96% -10.84% -3.13% -4.40%\nw/o AT\n21.24 40.05 15.09 22.70 13.37 14.75\n5.27\n9.96\n-6.88% -5.63% -6.39% -3.90% -3.60% -3.66% -8.19% -6.65%\nBest\n22.81 42.44 16.12 23.62 13.87 15.31\n5.74 10.67\nlearnable rescaling factors does not achieve good performance as\nexpected. One explanation is that, our proposed model currently\ndoes not post a direct mathematical constraint to learnable factors\n(\ud459\ud453), e.g., \ud459\ud453(\ud459)\n\ud462\n= argmin(\ud497(\ud459)\n\ud462, \ud459\ud453(\ud459)\n\ud462\n\ud492(\ud459)\n\ud462), mainly because they have\ndi\ufb00erent embedding dimensionality. This means that purely rely-\ning on the stochastic optimization may hardly reach the optimum.\nIn a word, considering the additional search space introduced by\nthis regularization term, we argue that our deterministic rescaling\nmethod is simple but e\ufb00ective in practice.\n5.6.4\nE\ufb00ect of Annealing Training Strategy. We disable the\nannealing training strategy by only adapting the rescaling approx-\nimation design to L2Q-GCN\ud452\ud45b\ud451and denote the variant as w/o AT.\nThe performance of w/o AT well demonstrate the usefulness of our\nutilized annealing training strategy in avoiding unnecessary opti-\nmization directions and the numerical stability for producing bet-\nter recommendation accuracy.\nIn conclusion, the ablation study well justi\ufb01es the necessity of\neach model component in both L2Q-GCN\ud452\ud45b\ud451and L2Q-GCN\ud44e\ud45b\ud459. We\nalso discuss the e\ufb00ect of di\ufb00erent hyperparameter settings, e.g.,\nlayer depth \ud43f, quantization dimension \ud451, to model performance\nand attached the results in Appendix D.\n6\nConclusion and Future Work\nIn this work, we propose L2Q-GCN to study the problem of 1-\nbit representation quantization for Top-K recommendation. While\nL2Q-GCN\ud452\ud45b\ud451implements the basic framework of L2Q-GCN to gen-\nerate the quantized user-item representations, L2Q-GCN\ud44e\ud45b\ud459fur-\nther improves the recommendation capability by attaining 90\u223c99%\nperformance recovery compared to the state-of-the-art model. The\nextensive experiments over four real benchmarks not only prove\nthe e\ufb00ectiveness of our proposed models but also justify the neces-\nsity of each model component.\nAs for future work, we point out two possible directions. (1) In-\nstead of relying on integer arithmetics, how to develop the bitwise-\noperation-supported computation for e\ufb03cient inference is an im-\nportant topic to investigate. (2) It is also worth studying the prob-\nlem of complete network binarization for the GCN framework, as\nit is more fundamental to many GCN-related methods for model\ncompression and computation acceleration.\nReferences\n[1] [n.d.].\nCodes and Datasets of L2Q-GCN.\nhttps://drive.google.com/\ufb01le/d/\n1-9_kP60af0r86dAvIy4mdq_rPkTyCfMA/view?usp=sharing.\n[2] Milad Alizadeh, Javier Fern\u00e1ndez-Marqu\u00e9s,Nicholas D Lane, and Yarin Gal. 2018.\nAn empirical study of binary neural networks\u2019 optimisation. In ICLR.\n[3] Mehdi Bahri, Ga\u00e9tan Bahl, and Stefanos Zafeiriou. 2021. Binary Graph Neural\nNetworks. In CVPR. 9492\u20139501.\n[4] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,\nMichael Lyu, and Irwin King. 2021.\nPushing the limit of bert quantization.\n(2021).\n[5] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or\npropagating gradients through stochastic neurons for conditional computation.\narXiv (2013).\n[6] William Ralph Bennett. 1948. Spectra of quantized signals. The Bell System\nTechnical Journal 27, 3 (1948), 446\u2013472.\n[7] Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu-\ntional matrix completion. arXiv (2017).\n[8] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S Yu. 2017. Hashnet:\nDeep learning to hash by continuation. In ICCV. 5608\u20135617.\n[9] Yankai Chen, Menglin Yang, Yingxue Zhang, Mengchen Zhao, Ziqiao Meng,\nJianye Hao, and Irwin King. 2021. Modeling Scale-free Graphs with Hyperbolic\nGeometry for Knowledge-aware Recommendation. arXiv (2021).\n[10] Zhiyong Cheng, Ying Ding, Lei Zhu, and Mohan Kankanhalli. 2018. Aspect-\naware latent factor model: Rating prediction with ratings and reviews. In WWW.\n639\u2013648.\n[11] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua\nBengio. 2016. Binarized neural networks: Training deep neural networks with\nweights and activations constrained to+ 1 or-1. arXiv (2016).\n[12] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for\nyoutube recommendations. In Recsys. 191\u2013198.\n[13] Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, and Jie Zhou. 2015.\nDeep hashing for compact binary codes learning. In CVPR. 2475\u20132483.\n[14] Xue Geng, Hanwang Zhang, Jingwen Bian, and Tat-Seng Chua. 2015. Learning\nimage and user features for recommendation in social networks. In ICCV. 4274\u2013\n4282.\n[15] Allen Gersho and Robert M Gray. 2012. Vector quantization and signal compres-\nsion. Vol. 159. Springer Science & Business Media.\n[16] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. 1999. Similarity search in\nhigh dimensions via hashing. In Vldb, Vol. 99. 518\u2013529.\n[17] William L Hamilton, Rex Ying, and Jure Leskovec.2017. Inductive representation\nlearning on large graphs. In NeurIPS. 1025\u20131035.\n[18] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network\nfor recommendation. In SIGIR. 639\u2013648.\n[19] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative \ufb01ltering. In WWW. 173\u2013182.\n[20] Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast\nmatrix factorization for online recommendation with implicit feedback. In SIGIR.\n549\u2013558.\n[21] Wang Hongwei, Zhao Miao, Xie Xing, Li Wenjie, and Guo Minyi. 2019. Knowl-\nedge Graph Convolutional Networks for Recommender Systems. (2019), 3307\u2013\n3313.\n[22] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua\nBengio. 2016. Binarized neural networks. NeurIPS 29 (2016).\n[23] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization\nwith gumbel-softmax. (2017).\n[24] Wang-Cheng Kang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Ting\nChen, Lichan Hong, and Ed H Chi. 2021. Learning to Embed CategoricalFeatures\nwithout Embedding Tables for Recommendation. In SIGKDD. 840\u2013850.\n[25] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\nmization. (2015).\n[26] Thomas N Kipf and Max Welling. 2017.\nSemi-supervised classi\ufb01cation with\ngraph convolutional networks. (2017).\n[27] Alex Krizhevsky, Ilya Sutskever, and Geo\ufb00rey E Hinton. 2012. Imagenet classi\ufb01-\ncation with deep convolutional neural networks. NeurIPS 25 (2012), 1097\u20131105.\n[28] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. 2019. Deepgcns:\nCan gcns go as deep as cnns?. In ICCV. 9267\u20139276.\n[29] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph\nconvolutional networks for semi-supervised learning. In AAAI.\n[30] Dawen Liang, Laurent Charlin, James McInerney, and David M Blei. 2016. Mod-\neling user exposure in recommendation. In WWW. 951\u2013961.\n[31] Xiaofan Lin, Cong Zhao, and Wei Pan. 2017. Towards accurate binary convolu-\ntional neural network. (2017).\n[32] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. 2017. The concrete distri-\nbution: A continuous relaxation of discrete random variables. (2017).\n[33] Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Ban-\nner, Alex M Bronstein, and Avi Mendelson. 2021. Loss aware post-training quan-\ntization. Machine Learning (2021), 1\u201318.\n[34] Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei\nYu, and Jingkuan Song. 2020. Forward and backward information retention for\naccurate binary neural networks. In CVPR. 2250\u20132259.\n[35] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.\nXnor-net: Imagenet classi\ufb01cation using binary convolutional neural networks.\nIn ECCV. Springer, 525\u2013542.\n[36] Ste\ufb00en Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-\nThieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv\n(2012).\n[37] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang.\n2020. Compositional embeddings using complementary partitions for memory-\ne\ufb03cient recommendation systems. In SIGKDD. 165\u2013175.\n[38] Jianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Ruiming\nTang, and Xiuqiang He. 2019. Multi-graph convolution collaborative \ufb01ltering.\nIn ICDM. IEEE, 1306\u20131311.\n[39] Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D Lane. 2021. Degree-\nquant: Quantization-aware training for graph neural networks. 9th ICLR (2021).\n[40] Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu.\n2020. Learning to hash with graph neural networks for recommender systems.\nIn WWW. 1988\u20131998.\n[41] Junfu Wang, Yunhong Wang, Zhen Yang, Liang Yang, and Yuanfang Guo. 2021.\nBi-gcn: Binary graph convolutional network. In CVPR. 1561\u20131570.\n[42] Jingdong Wang, Ting Zhang, Nicu Sebe, Heng Tao Shen, et al. 2017. A survey\non learning to hash. TPAMI 40, 4 (2017), 769\u2013790.\n[43] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural graph collaborative \ufb01ltering. In SIGIR. 165\u2013174.\n[44] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian\nWeinberger. 2019. Simplifying graph convolutional networks. In ICML. PMLR,\n6861\u20136871.\n[45] Wei Wu, Bin Li, Chuan Luo, and Wolfgang Nejdl. 2021. Hashing-Accelerated\nGraph Neural Networks for Link Prediction. In WWW. 2910\u20132920.\n[46] Yang Xu, Lei Zhu, Zhiyong Cheng, Jingjing Li, and Jiande Sun. 2020. Multi-\nfeature discrete collaborative \ufb01ltering for fast cold-start recommendation. In\nAAAI, Vol. 34. 270\u2013278.\n[47] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,\nand Jure Leskovec. 2018. Graph convolutional neural networks for web-scale\nrecommender systems. In SIGKDD. 974\u2013983.\n[48] Hanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, and Tat-\nSeng Chua. 2016. Discrete collaborative \ufb01ltering. In SIGIR. 325\u2013334.\n[49] Yan Zhang, Defu Lian, and Guowu Yang. 2017. Discrete personalized ranking\nfor fast collaborative \ufb01ltering from implicit feedback. In AAAI, Vol. 31.\n[50] Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz Ustebay. 2019.\nBayesian graph convolutional neural networks for semi-supervised classi\ufb01ca-\ntion. In AAAI, Vol. 33. 5829\u20135836.\n[51] Yifei Zhang and Hao Zhu. 2019. Doc2hash: Learning discrete latent variables\nfor documents retrieval. In ACL. 2235\u20132240.\n[52] Han Zhu, Mingsheng Long, Jianmin Wang, and Yue Cao. 2016. Deep hashing\nnetwork for e\ufb03cient similarity retrieval. In AAAI, Vol. 30.\nA\nNotation and Meanings\nTable 1 explains all key notations used in this paper.\nB\npseudocodes of L2Q-GCN\nWe attached the detailed pseudocodes of L2Q-GCN\ud452\ud45b\ud451and L2Q-\nGCN\ud44e\ud45b\ud459in Algorithms 1 and 2, respectively.\nC\nHyper-parameter Settings\nWe report all the hyper-parameter settings in Table 2. Please\nnotice that, in table 2, \ud435is the batch size, \ud702represents the learning\nrate, and \ud706is the coe\ufb03cient of \ud43f2 normalization.\nD\nComplete Results\nFor detailed curves of Top-K recommendation, please refer to\nFigure 1. For the study of di\ufb00erent hyperparameter setttings, i.e.,\nlayer depth \ud43fand quantization dimension \ud451, please refer to Table 4\nand Table 5 accordingly.\nTable 1: Notations and meanings.\nNotation\nMeaning\nU, I\nThe sets of users, items.\n\ud466\ud462,\ud456,\n\u02c6\ud466\ud462,\ud456\n\ud466\ud462,\ud456=1 means there is an observed interaction between user \ud462and\nitem \ud456, otherwise \ud466\ud462,\ud456=0. \u02c6\ud466\ud462,\ud456is the estimated matching score.\n\ud497(\ud459)\n\ud465,\ncontinuous embedding of node \ud465at the \ud459-th layer.\n\ud492(\ud459)\n\ud465,\nbinary embedding of node \ud465at the \ud459-th layer.\nQ\ud465,\nquantized representation of node \ud465.\nA\ud465,\napproximation-based quantized representation of node \ud465.\nAlgorithm 1: L2Q-GCN\ud452\ud45b\ud451algorithm\nInput: Interaction graph G; trainable parameters \u0398: {\ud497\ud462}\ud462\u2208U,\n{\ud497\ud456}\ud456\u2208I, \ud47e; hyper-parameters: \ud435, \ud451\ud452\ud45b\ud451, \ud43f, \ud702, \ud706.\nOutput: Prediction function F(\ud462,\ud456|\u0398, G)\n1 Q\ud462\u2190\u2205, Q\ud456\u2190\u2205;\n2 while L2Q-GCN\ud452\ud45b\ud451not converge do\n3\nfor (\ud462, \ud456) \u2208G that \ud466\ud462,\ud456= 1 do\n4\nfor \ud459= 1, \u00b7 \u00b7 \u00b7 , \ud43fdo\n5\n\ud497(\ud459)\n\ud462\n\u2190\u00cd\n\ud456\u2208N(\ud462)\n1\n\u221a\n|N(\ud462)|\u00b7|N(\ud456)| \ud463(\ud459\u22121)\n\ud456\n;\n6\n\ud497(\ud459)\n\ud456\n\u2190\u00cd\n\ud462\u2208N(\ud456)\n1\n\u221a\n|N(\ud456)|\u00b7|N(\ud462)| \ud463(\ud459\u22121)\n\ud462\n;\n7\n\ud45e(\ud459+1)\n\ud462\n\u2190sign \u0000\ud47e\ud447\ud497(\ud459)\n\ud462\n\u0001; \ud45e(\ud459)\n\ud456\n\u2190sign \u0000\ud47e\ud447\ud497(\ud459)\n\ud456\n\u0001;\n8\nUpdate (Q\ud462, Q\ud456);\n9\n\ud715\ud492(\ud459)\n\ud462\n\ud715\ud719,\n\ud715\ud492(\ud459)\n\ud456\n\ud715\ud719\n\u21901|\ud719|\u22641 for backforward propagation;\n10\nUpdate (Q\ud462, Q\ud456) with \ud45e(0)\n\ud462,\ud45e(0)\n\ud456\n;\n11\nL \u2190compute loss and optimize L2Q-GCN\ud452\ud45b\ud451model;\n12 return F.\nTable 3: Space and time results of two-layer networks with\n128-dimension embeddings.\nModel\nMovieLens\nGowalla\nPinterest\nYelp2018\nE.S.\nI.T.\nE.S.\nI.T.\nE.S.\nI.T.\nE.S.\nI.T.\nLightGCN\n4.88 3.42 34.59 190.41 28.45 82.66 34.04 196.49\nL2Q-GCN\ud452\ud45b\ud4510.47 2.19 3.24 118.97 2.98 53.48 3.19 118.23\nGap\n10.38\u00d7 1.56\u00d7 10.68\u00d7 1.60\u00d7 9.55\u00d7 1.55\u00d7 10.66\u00d7 1.66\u00d7\nL2Q-GCN\ud44e\ud45b\ud459\n0.58 3.55 4.05 191.32 3.73 81.77 3.98 194.84\nGap\n8.41\u00d7 0.96\u00d7 8.54\u00d7 1.00\u00d7 7.63\u00d7 1.01\u00d7 8.55\u00d7 1.01\u00d7\nTable 4: Top-20 recommendation (%) w.r.t di\ufb00erent \ud43f.\n\ud43f\nMovieLens\nGowalla\nPinterest\nYelp2018\nR@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20\nL2Q-GCN\ud452\ud45b\ud459\n\ud43f= 1 20.81 39.08 13.59\n19.86\n11.34\n12.79\n4.76\n9.28\n\ud43f= 2 20.52\n38.41 14.62 21.24 12.52 13.92\n5.10\n9.62\n\ud43f= 3 10.57\n33.44\n12.65\n19.23\n6.41\n7.39\n3.83\n7.95\n\ud43f= 4\n6.32\n19.25\n13.85\n20.26\n3.56\n4.51\n4.65\n9.03\nL2Q-GCN\ud44e\ud45b\ud459\n\ud43f= 1 20.56\n39.51\n15.82\n23.04\n13.11\n14.50\n5.52\n10.31\n\ud43f= 2 22.81 42.44 16.12 23.62 13.87 15.31\n5.74\n10.67\n\ud43f= 3 21.93\n40.85\n14.60\n22.22\n12.90\n14.52\n5.35\n10.14\n\ud43f= 4 21.72\n40.46\n14.63\n22.24\n11.31\n13.16\n4.96\n9.74\nTable 5: Top-20 recommendation (%) w.r.t di\ufb00erent \ud451.\n\ud43f\nMovieLens\nGowalla\nPinterest\nYelp2018\nR@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20\nL2Q-GCN\ud452\ud45b\ud459\n\ud451= 64\n16.65 28.23 14.62 21.24\n9.63\n9.69\n5.10\n9.62\n\ud451= 128 18.02 33.15\n14.41 19.91 12.52 13.92\n4.86\n9.63\n\ud451= 256 20.52 38.41 10.39 14.29\n10.37 11.62\n4.26\n8.55\n\ud451= 512 17.79 31.49\n13.23 20.65\n10.22 11.38\n3.49\n7.43\nL2Q-GCN\ud44e\ud45b\ud459\n\ud451= 64\n20.02 38.89\n14.29 21.35\n11.85 12.76\n4.21\n8.42\n\ud451= 128 21.64 40.82\n15.38 22.84 13.87 15.31\n4.94\n9.17\n\ud451= 256 22.81 42.44 16.12 23.62 12.63 14.20\n5.74\n10.67\n\ud451= 512 21.23 40.17\n15.54 23.11\n12.41 14.04\n5.28\n9.42\nAlgorithm 2: L2Q-GCN\ud44e\ud45b\ud459algorithm\nInput: Interaction graph G; trainable parameters \u0398: {\ud497\ud462}\ud462\u2208U,\n{\ud497\ud456}\ud456\u2208I, \ud47e; hyper-parameters: \ud435, \ud451\ud452\ud45b\ud451, \ud43f, \ud702, \ud706.\nOutput: Prediction function F(\ud462,\ud456|\u0398, G)\n1 Q\ud462\u2190\u2205, Q\ud456\u2190\u2205;\n2 while L2Q-GCN\ud44e\ud45b\ud459not converge do\n3\nfor (\ud462,\ud456) \u2208G that \ud466\ud462,\ud456= 1 do\n4\nfor \ud459= 1, \u00b7 \u00b7 \u00b7 , \ud43fdo\n5\n\ud497(\ud459)\n\ud462\n\u2190\u00cd\n\ud456\u2208N(\ud462)\n1\n\u221a\n|N(\ud462)|\u00b7|N(\ud456)| \ud463(\ud459\u22121)\n\ud456\n;\n6\n\ud497(\ud459)\n\ud456\n\u2190\u00cd\n\ud462\u2208N(\ud456)\n1\n\u221a\n|N(\ud456)|\u00b7|N(\ud462)| \ud463(\ud459\u22121)\n\ud462\n;\n7\n\ud45e(\ud459)\n\ud462\n\u2190\u0000\ud47e\ud447\ud497(\ud459)\n\ud462\n\u0001; \ud45e(\ud459)\n\ud456\n\u2190\u0000\ud47e\ud447\ud497(\ud459)\n\ud456\n\u0001;\n8\nif with quantization then\n9\n\ud45e(\ud459)\n\ud462\n\u2190sign \u0000\ud45e(\ud459)\n\ud462); \ud45e(\ud459)\n\ud456\n\u2190sign \u0000\ud45e(\ud459)\n\ud456\n);\n10\nSetting gradients for backforward propagation;\n11\n\ud6fc(\ud459)\n\ud462\n\u2190||\ud497(\ud459)\n\ud462||1\n\ud451\n, \ud6fc(\ud459)\n\ud462\n\u2190||\ud497(\ud459)\n\ud462||1\n\ud451\n;\n12\nUpdate (A\ud462, A\ud456) with \ud6fc(\ud459)\n\ud462\ud45e(\ud459)\n\ud462, \ud6fc(\ud459)\n\ud456\n\ud45e(\ud459)\n\ud456\n;\n13\nUpdate (A\ud462, A\ud456) with \ud6fc(0)\n\ud462\ud45e(0)\n\ud462,\ud6fc(0)\n\ud462\ud45e(0)\n\ud456\n;\n14\nL \u2190compute loss and optimize L2Q-GCN\ud44e\ud45b\ud459model;\n15 return F.\n(a) MovieLens.\n(b) Gowalla.\n(c) Pinterest.\n(d) Yelp2018.\nFigure 1: topk.\n(a) MovieLens.\nRecall@K\nNDCG@K\n(b) Gowalla.\n(c) Pinterest.\n(d) Yelp2018.\nFigure 2: Complete performance curves with annealing tranining strategy.\nTable 2: Hyper-parameter settings for the four datasets.\nMovieLens\nGowalla\nPinterest\nYelp2018\n\ud435\n2048\n2048\n2048\n2048\n\ud451\ud452\ud45b\ud451\n256\n64\n64\n64\n\ud451\ud44e\ud45b\ud459\n256\n256\n128\n256\n\ud702\n1 \u00d7 10\u22123\n1 \u00d7 10\u22123\n1 \u00d7 10\u22123\n1 \u00d7 10\u22123\n\ud706\n1 \u00d7 10\u22124\n1 \u00d7 10\u22124\n1 \u00d7 10\u22124\n1 \u00d7 10\u22124\n",
    "2006.04466": "Differentiable Neural Input Search for Recommender Systems\nWeiyu Cheng, Yanyan Shen, Linpeng Huang\nShanghai Jiao Tong University\n{weiyu cheng, shenyy, lphuang}@sjtu.edu.cn\nAbstract\nLatent factor models are the driving forces of the state-of-\nthe-art recommender systems, with an important insight of\nvectorizing raw input features into dense embeddings. The\ndimensions of different feature embeddings are often set to\na same value empirically, which limits the predictive perfor-\nmance of latent factor models. Existing works have proposed\nheuristic or reinforcement learning-based methods to search\nfor mixed feature embedding dimensions. For ef\ufb01ciency con-\ncern, these methods typically choose embedding dimensions\nfrom a restricted set of candidate dimensions. However, this\nrestriction will hurt the \ufb02exibility of dimension selection,\nleading to suboptimal performance of search results.\nIn this paper, we propose Differentiable Neural Input Search\n(DNIS), a method that searches for mixed feature embedding\ndimensions in a more \ufb02exible space through continuous re-\nlaxation and differentiable optimization. The key idea is to\nintroduce a soft selection layer that controls the signi\ufb01cance\nof each embedding dimension, and optimize this layer ac-\ncording to model\u2019s validation performance. DNIS is model-\nagnostic and thus can be seamlessly incorporated with ex-\nisting latent factor models for recommendation. We conduct\nexperiments with various architectures of latent factor mod-\nels on three public real-world datasets for rating prediction,\nClick-Through-Rate (CTR) prediction, and top-k item recom-\nmendation. The results demonstrate that our method achieves\nthe best predictive performance compared with existing neu-\nral input search approaches with fewer embedding parameters\nand less time cost.\n1\nIntroduction\nMost state-of-the-art recommender systems employ latent\nfactor models that vectorize raw input features into dense\nembeddings. A key question often asked of feature embed-\ndings is: \u201cHow should we determine the dimensions of fea-\nture embeddings?\u201d The common practice is to set a uniform\ndimension for all the features, and treat the dimension as a\nhyperparameter that needs to be tuned with time-consuming\ngrid search on a validation set. However, a uniform embed-\nding dimension is not necessarily suitable for different fea-\ntures. Intuitively, predictive and popular features that appear\nin a large number of data samples usually deserve a high\nembedding dimension, which encourages a high model ca-\npacity to \ufb01t these data samples [Joglekar et al. 2020; Zhao\net al. 2020]. Likewise, less predictive and infrequent features\nwould rather be assigned with lower embedding dimensions\nto avoid over\ufb01tting on scarce data samples. As such, it is\ndesirable to impose a mixed dimension scheme for differ-\nent features towards better recommendation performance.\nAnother notable fact is that embedding layers in industrial\nweb-scale recommender systems account for the majority of\nmodel parameters and can consume hundreds of gigabytes\nof memory space [Park et al. 2018; Covington, Adams, and\nSargin 2016]. Replacing a uniform feature embedding di-\nmension with varying dimensions can help to remove redun-\ndant embedding weights and signi\ufb01cantly reduce memory\nand storage costs for model parameters.\nSome recent works [Ginart et al. 2019; Joglekar et al.\n2020] have focused on searching for mixed feature embed-\nding dimensions automatically, which is de\ufb01ned as the Neu-\nral Input Search (NIS) problem. There are two existing ap-\nproaches to dealing with NIS: heuristic-based and reinforce-\nment learning-based. The heuristic-based approach [Ginart\net al. 2019] designs an empirical function to determine\nthe embedding dimensions for different features accord-\ning to their frequencies of occurrence (also called popular-\nity). The empirical function involves several hyperparam-\neters that need to be manually tuned to yield a good re-\nsult. The reinforcement learning-based approach [Joglekar\net al. 2020] \ufb01rst divides a base dimension space equally into\nseveral blocks, and then applies reinforcement learning to\ngenerate decision sequences on the selection of dimension\nblocks for different features. These methods, however, re-\nstrict each feature embedding dimension to be chosen from\na small set of candidate dimensions that is explicitly prede-\n\ufb01ned [Joglekar et al. 2020] or implicitly controlled by hy-\nperparameters [Ginart et al. 2019]. Although this restriction\nreduces search space and thereby improves computational\nef\ufb01ciency, another question then arises: how to decide the\ncandidate dimensions? Notably, a suboptimal set of candi-\ndate dimensions could result in a suboptimal search result\nthat hurts model\u2019s recommendation performance.\nIn this paper, we propose Differentiable Neural Input\nSearch (DNIS) to address the NIS problem in a differen-\ntiable manner through gradient descent. Instead of search-\ning over a prede\ufb01ned discrete set of candidate dimensions,\nDNIS relaxes the search space to be continuous and opti-\nmizes the selection of embedding dimensions by descend-\ning model\u2019s validation loss. More speci\ufb01cally, we introduce\narXiv:2006.04466v2  [cs.LG]  10 Sep 2020\na soft selection layer between the embedding layer and the\nfeature interaction layers of latent factor models. Each in-\nput feature embedding is fed into the soft selection layer\nto perform an element-wise multiplication with a scaling\nvector. The soft selection layer directly controls the signi\ufb01-\ncance of each dimension of the feature embedding, and it is\nessentially a part of model architecture which can be op-\ntimized according to model\u2019s validation performance. We\nalso propose a gradient normalization technique to solve the\nproblem of high variance of gradients when optimizing the\nsoft selection layer. After training, we employ a \ufb01ne-grained\npruning procedure by merging the soft selection layer with\nthe feature embedding layer, which yields a \ufb01ne-grained\nmixed embedding dimension scheme for different features.\nDNIS can be seamlessly applied to various existing architec-\ntures of latent factor models for recommendation. We con-\nduct extensive experiments with six existing architectures\nof latent factor model on three public real-world datasets\nfor rating prediction, Click-Through-Rate (CTR) prediction,\nand top-k item recommendation tasks. The results demon-\nstrate that the proposed method achieves the best predic-\ntive performance compared with existing NIS methods with\nfewer embedding parameters and less time cost.\nThe major contributions of this paper are summarized as\nfollows:\n\u2022 We propose DNIS, a method that addresses the NIS prob-\nlem in a differentiable manner by relaxing the embedding\ndimension search space to be continuous and optimizing\nthe selection of dimensions with gradient descent.\n\u2022 We propose a gradient normalization technique to deal\nwith the high variance of gradients during optimizing\nthe soft selection layer, and further design a \ufb01ne-grained\npruning procedure through layer merging to produce a\n\ufb01ne-grained mixed embedding dimension scheme for dif-\nferent features.\n\u2022 The proposed method is model-agnostic, and thus can be\nincorporated with various existing architectures of latent\nfactor models to improve recommendation performance\nand reduce the size of embedding parameters.\n\u2022 We conduct experiments with different model architec-\ntures on real-world datasets for three typical recom-\nmendation tasks: rating prediction, CTR prediction, and\ntop-k item recommendation. The results demonstrate\nDNIS achieves the best overall result compared with ex-\nisting NIS methods in terms of recommendation perfor-\nmance, embedding parameter size and training time cost.\n2\nDifferentiable Neural Input Search\n2.1\nBackground\nLatent factor models. We consider a recommender sys-\ntem involving M feature \ufb01elds (e.g., user ID, item ID, item\nprice). Typically, M is 2 (including user ID and item ID)\nin collaborative \ufb01ltering (CF) problems, whereas in the con-\ntext of CTR prediction, M is usually much larger than 2\nto include more feature \ufb01elds. Each categorical feature \ufb01eld\nconsists of a collection of discrete features, while a numer-\nical feature \ufb01eld contains one scalar feature. Let F denote\nthe list of features over all the \ufb01elds and the size of F is\nN. For the i-th feature in F, its initial representation is a N-\ndimensional sparse vector xi, where the i-th element is 1 (for\ndiscrete feature) or a scalar number (for scalar feature), and\nthe others are 0s. Latent factor models generally consists of\ntwo parts: one feature embedding layer, followed by the fea-\nture interaction layers. Without loss of generality, the input\ninstances to the latent factor model include several features\nbelonging to the respective feature \ufb01elds. The feature em-\nbedding layer transforms all the features in an input instance\ninto dense embedding vectors. Speci\ufb01cally, a sparsely en-\ncoded input feature vector xi \u2208RN is transformed into a\nK-dimensional embedding vector ei \u2208RK as follows:\nei = E\u22a4xi\n(1)\nwhere E \u2208RN\u00d7K is known as the embedding matrix. The\noutput of the feature embedding layer is the collection of\ndense embedding vectors for all the input features, which\nis denoted as X. The feature interaction layers, which are\ndesigned to be different architectures, essentially compose\na parameterized function G that predicts the objective based\non the collected dense feature embeddings X for the input\ninstance. That is,\n\u02c6y = G(\u03b8, X)\n(2)\nwhere \u02c6y is the model\u2019s prediction, and \u03b8 denotes the set of\nparameters in the interaction layers. Prior works have devel-\noped various architectures for G, including the simple inner\nproduct function [Rendle 2010], and deep neural networks-\nbased interaction functions [He et al. 2017; Cheng et al.\n2018; Lian et al. 2018; Cheng et al. 2016; Guo et al. 2017].\nMost of the proposed architectures for the interaction layers\nrequire all the feature embeddings to be in a uniform dimen-\nsion.\nNeural architecture search. Neural Architecture Search\n(NAS) has been proposed to automatically search for the\nbest neural network architecture. To explore the space of\nneural architectures, different search strategies have been ex-\nplored including random search [Li and Talwalkar 2019],\nevolutionary methods [Elsken, Metzen, and Hutter 2019;\nMiller, Todd, and Hegde 1989; Real et al. 2017], Bayesian\noptimization [Bergstra, Yamins, and Cox 2013; Domhan,\nSpringenberg, and Hutter 2015; Mendoza et al. 2016], re-\ninforcement learning [Baker et al. 2017; Zhong et al. 2018;\nZoph and Le 2017], and gradient-based methods [Cai, Zhu,\nand Han 2019; Liu, Simonyan, and Yang 2019; Xie et al.\n2019]. Since being proposed in [Baker et al. 2017; Zoph\nand Le 2017], NAS has achieved remarkable performance in\nvarious tasks such as image classi\ufb01cation [Real et al. 2019;\nZoph et al. 2018], semantic segmentation [Chen et al. 2018]\nand object detection [Zoph et al. 2018]. However, most of\nthese researches have focused on searching for optimal net-\nwork structures automatically, while little attention has been\npaid to the design of the input component. This is because\nthe input component in visual tasks is already given in the\nform of \ufb02oating point values of image pixels. As for recom-\nmender systems, an input component based on the embed-\nding layer is deliberately developed to transform raw fea-\ntures (e.g., discrete user identi\ufb01ers) into dense embeddings.\nIn this paper, we focus on the problem of neural input search,\n\ud835\udc51\"\n\ud835\udc63\"\n\ud835\udc1e\"\n0\n0.1\n0\n-0.2\n2\n4\n0.1\n-0.2\n\ud835\udc49\n\ud835\udc37\n(a) Example of notations.\n\ud835\udca2(\ud835\udf3d)\n\ud835\udc66&\n\ud835\udc1e(\n\ud835\udec2\n\ud835\udc1e\n\u2026\n\u2026\n\u2026\nEmbedding\nLayer\nSoft Selection\nLayer\nOutput\nEmbeddings\nInteraction\nFunction\nPrediction\n(b) Model structure.\nFigure 1: A demonstration of notations and model structure.\nwhich can be considered as NAS on the input component\n(i.e., the embedding layer) of recommender systems.\n2.2\nSearch Space and Problem\nSearch space. The key idea of neural input search is to\nuse embeddings with mixed dimensions to represent differ-\nent features. To formulate feature embeddings with different\ndimensions, we adopt the representation for sparse vectors\n(with a base dimension K). Speci\ufb01cally, for each feature, we\nmaintain a dimension index vector d which contains ordered\nlocations of the feature\u2019s existing dimensions from the set\n{1, \u00b7 \u00b7 \u00b7 , K}, and an embedding value vector v which stores\nembedding values in the respective existing dimensions. The\nconversion from the index and value vectors of a feature into\nthe K-dimensional embedding vector e is straightforward.\nFigure 1a gives an example of di, vi and ei for the i-th fea-\nture in F. Note that e corresponds to a row in the embedding\nmatrix E.\nThe size of d varies among different features to enforce a\nmixed dimension scheme. Formally, given the feature set F,\nwe de\ufb01ne the mixed dimension scheme D = {d1, \u00b7 \u00b7 \u00b7 , dN}\nto be the collection of dimension index vectors for all the\nfeatures in F. We use D to denote the search space of the\nmixed dimension scheme D for F, which includes 2NK pos-\nsible choices. Besides, we denote by V = {v1, \u00b7 \u00b7 \u00b7 , vN} the\nset of the embedding value vectors for all the features in F.\nThen we can derive the embedding matrix E with D and V\nto make use of conventional feature interaction layers.\nProblem formulation. Let \u0398 = {\u03b8, V } be the set of train-\nable model parameters, and Ltrain and Lval are model\u2019s\ntraining loss and validation loss, respectively. The two losses\nare determined by both the mixed dimension scheme D, and\nthe trainable parameters \u0398. The goal of neural input search\nis to \ufb01nd a mixed dimension scheme D \u2208D that minimizes\nthe validation loss Lval(\u0398\u2217, D), where the parameters \u0398\u2217\ngiven any mixed dimension scheme are obtained by mini-\nmizing the training loss. This can be formulated as:\nmin\nD\u2208D Lval(\u0398\u2217(D), D)\ns.t. \u0398\u2217(D) = argmin\n\u0398\nLtrain(\u0398, D)\n(3)\nThe above problem formulation is actually consistent with\nhyperparameter optimization in a broader scope [Maclaurin,\nDuvenaud, and Adams 2015; Pedregosa 2016; Franceschi\net al. 2018], since the mixed dimension scheme D can be\nconsidered as model hyperparameters to be determined ac-\ncording to model\u2019s validation performance. However, the\nmain difference is that the search space D in our problem\nis much larger than the search space of conventional hyper-\nparameter optimization problems.\n2.3\nFeature Blocking\nFeature blocking has been a novel ingredient used in the ex-\nisting neural input search methods [Joglekar et al. 2020; Gi-\nnart et al. 2019] to facilitate the reduction of search space.\nThe intuition behind is that features with similar frequen-\ncies could be grouped into a block sharing the same em-\nbedding dimension. Following the existing works, we \ufb01rst\nemploy feature blocking to control the search space of the\nmixed dimension scheme. We sort all the features in F in\nthe descending order of frequency (i.e., the number of fea-\nture occurrences in the training instances). Let \u03b7f denote the\nfrequency of feature f \u2208F. We can obtain a sorted list of\nfeatures \u02dcF = [f1, f2, \u00b7 \u00b7 \u00b7 , fN] such that \u03b7fi \u2265\u03b7fj for any\ni < j. We then separate \u02dcF into L blocks, where the fea-\ntures in a block share the same dimension index vector d.\nWe denote by \u02dcD the mixed dimension scheme after feature\nblocking. Then the length of the mixed dimension scheme\n| \u02dcD| becomes L, and the search space size is reduced from\n2NK to 2LK accordingly, where L \u226aN.\n2.4\nContinuous Relaxation and Differentiable\nOptimization\nContinuous relaxation. After feature blocking, in order to\noptimize the mixed dimension scheme \u02dcD, we \ufb01rst transform\n\u02dcD into a binary dimension indicator matrix \u02dcD \u2208RL\u00d7K,\nwhere each element in \u02dcD is either 1 or 0 indicating the exis-\ntence of the corresponding embedding dimension according\nto \u02dcD. We then introduce a soft selection layer to relax the\nsearch space of \u02dcD to be continuous. The soft selection layer\nis essentially a numerical matrix \u03b1 \u2208RL\u00d7K, where each\nelement in \u03b1 satis\ufb01es: 0 \u2264\u03b1l,k \u22641. That is, each binary\nchoice \u02dcDl,k (the existence of the k-th embedding dimension\nin the l-th feature block) in \u02dcD is relaxed to be a continuous\nvariable \u03b1l,k within the range of [0, 1]. We insert the soft\nselection layer between the feature embedding layer and in-\nteraction layers in the latent factor model, as illustrated in\nFigure 1b. Given \u03b1 and the embedding matrix E, the output\nembedding \u02dcei of a feature fi in the l-th block produced by\nthe bottom two layers can be computed as follows:\n\u02dcei = ei \u2299\u03b1l\u2217\n(4)\nwhere \u03b1l\u2217is the l-th row in \u03b1, and \u2299is the element-wise\nproduct. By applying Equation (4) to all the input features,\nwe can obtain the output feature embeddings X. Next, we\nsupply X to the feature interaction layers for \ufb01nal prediction\nas speci\ufb01ed in Equation (2). Note that \u03b1 is used to softly\nselect the dimensions of feature embeddings during model\ntraining, and the discrete mixed dimension scheme D will\nbe derived after training.\nDifferentiable optimization. Now that we relax the mixed\ndimension scheme \u02dcD (after feature blocking) via the soft\nselection layer \u03b1, our problem stated in Equation (3) can be\ntransformed into:\nmin\n\u03b1\nLval(\u0398\u2217(\u03b1), \u03b1)\ns.t. \u0398\u2217(\u03b1) = argmin\n\u0398\nLtrain(\u0398, \u03b1) \u2227\u03b1k,j \u2208[0, 1] (5)\nwhere \u0398 = {\u03b8, E} represents model parameters in both\nthe embedding layer and interaction layers. Equation 5 es-\nsentially de\ufb01nes a bi-level optimization problem [Colson,\nMarcotte, and Savard 2007], which has been studied in\ndifferentiable NAS [Liu, Simonyan, and Yang 2019] and\ngradient-based hyperparameter optimization [Chen et al.\n2019; Franceschi et al. 2018; Pedregosa 2016]. Basically, \u03b1\nand \u0398 are respectively treated as the upper-level and lower-\nlevel variables to be optimized in an interleaving way. To\ndeal with the expensive optimization of \u0398, we follow the\ncommon practice that approximates \u0398\u2217(\u03b1) by adapting \u0398\nusing a single training step:\n\u0398\u2217(\u03b1) \u2248\u0398 \u2212\u03be\u2207\u0398Ltrain(\u0398, \u03b1)\n(6)\nwhere \u03be is the learning rate for one-step update of model pa-\nrameters \u0398. Then we can optimize \u03b1 based on the following\ngradient:\n\u2207\u03b1Lval(\u0398 \u2212\u03be\u2207\u0398Ltrain(\u0398, \u03b1), \u03b1)\n=\u2207\u03b1Lval(\u0398\u2032, \u03b1) \u2212\u03be\u22072\n\u03b1,\u0398Ltrain(\u0398, \u03b1) \u00b7 \u2207\u03b1Lval(\u0398\u2032, \u03b1)\n(7)\nwhere \u0398\u2032 = \u0398 \u2212\u03be\u2207\u0398Ltrain(\u0398, \u03b1) denotes the model pa-\nrameters after one-step update. Equation (7) can be solved\nef\ufb01ciently using the existing deep learning libraries that al-\nlow automatic differentiation, such as Pytorch [Paszke et al.\n2019]. The second-order derivative term in Equation (7) can\nbe omitted to further improve computational ef\ufb01ciency con-\nsidering \u03be to be near zero, which is called the \ufb01rst-order\napproximation. Algorithm 1 (line 5-10) summarizes the bi-\nlevel optimization procedure for solving Equation (5).\nGradient normalization. During the optimization of \u03b1 by\nthe gradient \u2207\u03b1Lval(\u0398\u2032, \u03b1), we propose a gradient normal-\nization technique to normalize the row-wise gradients of \u03b1\nover each training batch:\ngnorm(\u03b1l\u2217) =\ng(\u03b1l\u2217)\n\u03a3K\nk=1|g(\u03b1l,k)|/K + \u03f5g\n,\nk \u2208[1, K] (8)\nwhere g and gnorm denote the gradients before and after\nnormalization respectively, and \u03f5g is a small value (e.g., 1e-\n7) to avoid numerical over\ufb02ow. Here we use row-wise gra-\ndient normalization to deal with the high variance of the\ngradients of \u03b1 during backpropogation. More speci\ufb01cally,\ng(\u03b1l\u2217) of a high-frequency feature block can be several or-\nders of magnitude larger than that of a low-frequency feature\nblock due to their difference on the number of related data\nsamples. By normalizing the gradients for each block, we\ncan apply the same learning rate to different rows of \u03b1 dur-\ning optimization. Otherwise, a single learning rate shared by\ndifferent feature blocks will fail to optimize most rows of \u03b1.\nAlgorithm 1: DNIS - Differentiable Neural Input Search\n1:1: Input: training dataset, validation dataset.\n2:2: Output: mixed dimension scheme D, embedding\nvalues V , interaction function parameters \u03b8.\n3: Sort features into \u02dcF and divide them into L blocks;\n4: Initialize the soft selection layer \u03b1 to be an all-one\nmatrix, and randomly initialize \u0398; // \u0398 = {\u03b8, E}\n5: while not converged do\n6:\nUpdate trainable parameters \u0398 by descending\n\u2207\u0398Ltrain(\u0398, \u03b1);\n7:\nCalculate the gradients of \u03b1 as:\n\u2212\u03be\u22072\n\u03b1,\u0398Ltrain(\u0398, \u03b1) \u00b7 \u2207\u03b1Lval(\u0398\u2032, \u03b1) +\n\u2207\u03b1Lval(\u0398\u2032, \u03b1);\n// (set \u03be = 0 if using first-order\napproximation)\n8:\nPerform Equation (8) to normalize the gradients in\n\u03b1;\n9:\nUpdate \u03b1 by descending the gradients, and then\nclip its values into the range of [0, 1];\n10: end\n11: Calculate the output embedding matrix E using \u03b1 and\n\u02dcE according to Equation (4);\n12: Prune E into a sparse matrix E\u2032 following Equation (9);\n13: Derive the mixed dimension scheme D and embedding\nvalues V with E\u2032;\n2.5\nDeriving Fine-grained Mixed Embedding\nDimension Scheme\nAfter optimization, we have the learned parameters for \u03b8, E\nand \u03b1. A straightforward way to derive the discrete mixed\ndimension scheme D is to prune non-informative embed-\nding dimensions in the soft selection layer \u03b1. Here we em-\nploy a \ufb01ne-grained pruning procedure through layer merg-\ning. Speci\ufb01cally, for feature fi in the l-th block, we can\ncompute its output embedding \u02dcei with ei and \u03b1l\u2217follow-\ning Equation (4). We collect the output embeddings \u02dcei for\nall the features in F and form an output embedding matrix\n\u02dcE \u2208RN\u00d7K. We then prune non-informative embedding di-\nmensions in \u02dcE as follows:\n\u02dcEi,j =\n\u001a0,\nif |\u02dcEi,j| < \u03f5\n\u02dcEi,j,\notherwise\n(9)\nwhere \u03f5 is a threshold that can be manually tuned according\nto the requirements on model performance and parameter\nsize. The pruned output embedding matrix \u02dcE is sparse and\ncan be used to derive the discrete mixed dimension scheme\nD and the embedding value vectors V for F accordingly.\nWith \ufb01ne-grained pruning, the derived embedding dimen-\nsions can be different even for features in the same feature\nblock, resulting in a more \ufb02exible mixed dimension scheme.\nRelation to network pruning. Network pruning, as one\nkind of model compression techniques, improves the ef\ufb01-\nciency of over-parameterized deep neural networks by re-\nmoving redundant neurons or connections without damag-\ning model performance [Cheng et al. 2017; Liu et al. 2019;\nFrankle and Carbin 2019]. Recent works of network prun-\ning [Han et al. 2015; Molchanov et al. 2017; Li et al.\nTable 1: Statistics of the datasets.\nDataset\nInstance#\nField#\nFeature#\nMovielens-20M\n20,000,263\n2\n165,771\nCriteo\n45,840,617\n39\n2,086,936\nMovielens-1M\n1,000,209\n2\n9,746\n2017] generally performed iterative pruning and \ufb01netun-\ning over certain pretrained over-parameterized deep net-\nwork. Instead of simply removing redundant weights, our\nproposed method DNIS optimizes feature embeddings with\nthe gradients from the validation set, and only prunes non-\ninformative embedding dimensions and their values in one\nshot after model training. This also avoids manually tuning\nthresholds and regularization terms per iteration. We con-\nduct experiments to compare the performance of DNIS and\nnetwork pruning methods in Section 3.4.\n3\nExperiments\n3.1\nExperimental Settings\nDatasets. We used two benchmark datasets Movielens-\n20M [Harper and Konstan 2016] and Criteo [Labs 2014]\nfor rating prediction and CTR prediction tasks, respectively.\nFor each dataset, we randomly split the instances by 8:1:1\nto obtain the training, validation and test sets. Besides, we\nalso conduct experiments on Movielens-1M dataset [Harper\nand Konstan 2016] to compare with NIS-ME [Joglekar et al.\n2020] for top-k item recommendation task. The statistics of\nthe three datasets are summarized in Table 1.\n(1) Movielens-20M is a CF dataset containing more than 20\nmillion user ratings ranging from 1 to 5 on movies.\n(2) Criteo is a popular industry benchmark dataset for CTR\nprediction, which contains 13 numerical feature \ufb01elds and\n26 categorical feature \ufb01elds. Each label indicates whether a\nuser has clicked the corresponding item.\n(3) Movielens-1M is a CF dataset containing over one mil-\nlion user ratings ranging from 1 to 5 on movies.\nEvaluation metrics. We adopt MSE (mean squared error)\nfor rating prediction task, and use AUC (Area Under the\nROC Curve) and Logloss for CTR prediction task. In ad-\ndition to predictive performance, we also report the em-\nbedding parameter size and the overall time cost of each\nmethod. When comparing with NIS-ME, we provide Recall,\nMRR (mean reciprocal rank) and NDCG results for top-k\nrecommendation.\nComparison methods. We compare our DNIS method with\nthe following four approaches.\n\u2022 Grid Search. This is the traditional approach to search-\ning for a uniform embedding dimension. In our experiments,\nwe searched 16 groups of dimensions, ranging from 4 to 64\nwith a stride of 4.\n\u2022 Random Search. Random search has been recognized\nas a strong baseline for NAS problems [Liu, Simonyan, and\nYang 2019]. When random searching a mixed dimension\nscheme, we applied the same feature blocking as we did for\nDNIS. Following the intuition that high-frequency features\ndesire larger numbers of dimensions, we generated 16 ran-\ndom descending sequences as the search space of the mixed\ndimension scheme for each model and report the best results.\n\u2022 MDE (Mixed Dimension Embeddings [Ginart et al.\n2019]). This method performs feature blocking and applies\na heuristic scheme where the number of dimensions per fea-\nture block is proportional to some fractional power of its\nfrequency. We tested 16 groups of hyperparameters settings\nas suggested in the original paper and report the best results.\n\u2022 NIS-ME (Neural Input Search with Multi-size Embed-\nding [Joglekar et al. 2020]). This method uses reinforcement\nlearning to \ufb01nd optimal embedding dimensions for differ-\nent features within a given memory budget. Since the imple-\nmentation is not available, we follow the same experimental\nsettings as detailed in [Joglekar et al. 2020]) and report the\nresults of our method for comparison.\nFor DNIS, we show its performance before and after the\ndimension pruning in Equation (9), and report the storage\nsize of the pruned sparse matrix E\u2032 using COO format of\nsparse matrix [Virtanen et al. 2020]. We provide the results\nwith different compression rates (CR), i.e., the division of\nunpruned embedding parameter size by the pruned size.\nImplementation details. We implement our method using\nPytorch [Paszke et al. 2019]. We apply Adam optimizer with\nthe learning rate of 0.001 for model parameters \u0398 and that\nof 0.01 for soft selection layer parameters \u03b1. The mini-batch\nsize is set to 4096 and the uniform base dimension K is\nset to 64 for all the models. We apply the same blocking\nscheme for Random Search, MDE and DNIS. The default\nnumbers of feature blocks L is set to 10 and 6 for Movielens\nand Criteo datasets, respectively. We employ various latent\nfactor models: MF, MLP [He et al. 2017] and NeuMF [He\net al. 2017] for rating prediction, and FM [Rendle 2010],\nWide&Deep [Cheng et al. 2016], DeepFM [Guo et al. 2017]\nfor CTR prediction, where the con\ufb01guration of latent fac-\ntor models are the same over different methods for a fair\ncomparison. Besides, we exploit early-stopping for all the\nmethods according to the change of validation loss during\nmodel training. All the experiments were performed using\nNVIDIA GeForce RTX 2080Ti GPUs.\n3.2\nComparison Results\nTable 2 and Table 3 show the comparison results of different\nNIS methods on rating prediction and CTR prediction tasks,\nrespectively. First, we can see that DNIS achieves the best\nprediction performance over all the model architectures for\nboth tasks. It is worth noticing that the time cost of DNIS is\nreduced by 2\u00d7 to over 10\u00d7 compared with the baselines.\nThe results con\ufb01rms that DNIS is able to learn discrimina-\ntive feature embeddings with signi\ufb01cantly higher ef\ufb01ciency\nthan the existing search methods. Second, DNIS with di-\nmension pruning achieves competitive or better performance\nthan baselines, and can yield a signi\ufb01cant reduction on em-\nbedding parameter size. For example, DNIS with the CR of 2\noutperforms all the baselines on Movielens, and yet reaches\nthe minimal parameter size. The advantages of DNIS with\nthe CR of 20 and 30 are more signi\ufb01cant on Criteo. Be-\nsides, we observe that DNIS can achieve a higher CR on\nCriteo than Movielens without sacri\ufb01cing prediction perfor-\nmance. This is because the distribution of feature frequency\non Criteo is severely skewed, leading to a signi\ufb01cantly large\nTable 2: Comparison between DNIS and baselines on the rating prediction task using Movielens-20M dataset. We also report\nthe storage size of the derived feature embeddings and the training time per method. For DNIS, we show its results with and\nw/o different compression rates (CR), i.e., the division of unpruned embedding parameter size by the pruned size.\nSearch Methods\nMF\nMLP\nNeuMF\nParams\nTime Cost\nMSE\nParams\nTime Cost\nMSE\nParams\nTime Cost\nMSE\n(M)\n(M)\n(M)\nGrid Search\n33\n16h\n0.622\n35\n8h\n0.640\n61\n4h\n0.625\nRandom Search\n33\n16h\n0.6153\n22\n4h\n0.6361\n30\n2h\n0.6238\nMDE\n35\n24h\n0.6138\n35\n5h\n0.6312\n27\n3h\n0.6249\nDNIS (unpruned)\n37\n1h\n0.6096\n36\n1h\n0.6255\n72\n1h\n0.6146\nDNIS (CR = 2)\n21\n1h\n0.6126\n20\n1h\n0.6303\n40\n1h\n0.6169\nDNIS (CR = 2.5)\n17\n1h\n0.6167\n17\n1h\n0.6361\n32\n1h\n0.6213\nTable 3: Comparison between DNIS and baselines on the CTR prediction task using Criteo dataset.\nSearch Methods\nFM\nWide&Deep\nDeepFM\nParams\nTime\nCost\nAUC\nLogloss Params\nTime\nCost\nAUC\nLogloss Params\nTime\nCost\nAUC\nLogloss\n(M)\n(M)\n(M)\nGrid Search\n441\n16h\n0.7987\n0.4525\n254\n16h\n0.8079\n0.4435\n382\n14h\n0.8080\n0.4435\nRandom Search\n73\n12h\n0.7997\n0.4518\n105\n16h\n0.8084\n0.4434\n105\n12h\n0.8084\n0.4434\nMDE\n397\n16h\n0.7986\n0.4530\n196\n16h\n0.8076\n0.4439\n396\n16h\n0.8077\n0.4438\nDNIS (unpruned)\n441\n3h\n0.8004\n0.4510\n395\n3h\n0.8088\n0.4429\n416\n3h\n0.8090\n0.4427\nDNIS (CR = 20)\n26\n3h\n0.8004\n0.4510\n29\n3h\n0.8087\n0.4430\n29\n3h\n0.8088\n0.4428\nDNIS (CR = 30)\n17\n3h\n0.8004\n0.4510\n19\n3h\n0.8085\n0.4432\n20\n3h\n0.8086\n0.4430\nnumber of redundant dimensions for low-frequency fea-\ntures. Third, among all the baselines, MDE performs the\nbest on Movielens and Random Search performs the best\non Criteo, while Grid Search gets the worst results on both\ntasks. This veri\ufb01es the importance of applying mixed dimen-\nsion embeddings to latent factor models. Fourth, we \ufb01nd that\nMF achieves better prediction performance on the rating pre-\ndiction task than the other two model architectures. The rea-\nson may be the over\ufb01tting problem of MLP and NeuMF that\nresults in poor generalization. Besides, DeepFM show the\nbest results on the CTR prediction task, suggesting that the\nensemble of DNN and FM is bene\ufb01cial to improving CTR\nprediction accuracy.\nTable 4 shows the performance of DNIS and NIS-ME\nwith respect to base embedding dimension K and embed-\nding parameter size for top-k item recommendation. From\nthe results, we can see that DNIS achieves the best perfor-\nmance on most metrics (on average, the relative improve-\nment over NIS-ME on Recall, MRR, and NDCG are 5.3%,\n7.2%, and 2.7%, respectively). This indicates the effective-\nness of DNIS by searching for the optimal mixed dimension\nscheme in a differentiable manner. Besides, NIS-ME shows\nconsistent improvements over NIS-SE, admitting the bene-\n\ufb01t of replacing single embedding dimension with mixed di-\nmensions.\n3.3\nHyperparameter Investigation\nWe investigate the effects of two important hyperparameters\nK and L in DNIS. Figure 2a shows the performance change\nof MF w.r.t. different settings of K. We can see that in-\ncreasing K is bene\ufb01cial to reducing MSE. This is because a\nMSE\n0.60\n0.61\n0.62\n0.63\n0.64\nK\n8 64 128\n256\n512\n1024\n(a) MSE vs K.\nMSE\n0.608\n0.61\n0.612\n0.614\n0.616\nL\n1 5\n10\n20\n40\n80\n(b) MSE vs L.\nFigure 2: Effects of hyperparameters on the performance of\nDNIS. We report the MSE results of MF on Movielens-20M\ndataset w.r.t. different base embedding dimensions K and\nfeature block numbers L.\nlarger K allows a larger search space that could improve the\nrepresentations of high-frequency features by giving more\nembedding dimensions. Besides, we observe a marginal de-\ncrease in performance gain. Speci\ufb01cally, the MSE is reduced\nby 0.005 when K increases from 64 to 128, whereas the\nMSE reduction is merely 0.001 when K changes from 512\nto 1024. This implies that K may have exceeded the largest\nnumber of dimensions required by all the features, leading\nto minor improvements. Figure 2b shows the effects of the\nnumber of feature blocks L. We \ufb01nd that increasing L im-\nproves the prediction performance of DNIS, and the perfor-\nmance improvement decreases as L becomes larger. This is\nbecause dividing features into more blocks facilitates a \ufb01ner-\ngrained control on the embedding dimensions of different\nfeatures, leading to more \ufb02exible mixed dimension schemes.\nSince both K and L affect the computation complexity of\nDNIS, we suggest to choose reasonably large values for K\nand L to balance the computational ef\ufb01ciency and predictive\nTable 4: Comparison between DNIS and NIS-ME on Movielens-1M dataset. NIS-SE is a variant of NIS-ME method with a\nshared number of embedding dimension. Here we use the results of the original paper [Joglekar et al. 2020].\nModel\nK\n#Params\nRecall@1\nRecall@5\n@Recall@10\nMRR@5\nMRR@10\nNDCG@5\nNDCG@10\nNIS-SE\n16\n390k\n9.32\n35.70\n55.31\n18.22\n20.83\n22.43\n28.63\nNIS-ME\n16\n390k\n9.41\n35.90\n55.68\n18.31\n20.95\n22.60\n28.93\nDNIS (CR = 2)\n16\n390k\n11.39\n35.79\n51.74\n19.82\n21.93\n23.77\n28.90\nNIS-SE\n16\n195k\n8.42\n31.37\n50.30\n15.04\n17.57\n19.59\n25.12\nNIS-ME\n16\n195k\n8.57\n33.29\n52.91\n16.78\n19.37\n20.83\n27.15\nDNIS (CR = 4)\n16\n195k\n11.15\n33.34\n49.62\n18.74\n20.88\n22.35\n27.58\nNIS-SE\n32\n780k\n9.90\n37.50\n55.69\n19.18\n21.60\n23.70\n29.58\nNIS-ME\n32\n780k\n10.40\n38.66\n57.02\n19.59\n22.18\n24.28\n30.60\nDNIS (CR = 2)\n32\n780k\n13.01\n38.26\n55.43\n21.83\n24.12\n25.89\n31.45\nNIS-SE\n32\n390k\n9.79\n34.84\n53.23\n17.85\n20.26\n22.00\n27.91\nNIS-ME\n32\n390k\n10.19\n37.44\n56.62\n19.56\n22.09\n23.98\n30.14\nDNIS (CR = 4)\n32\n390k\n11.95\n35.98\n51.92\n20.27\n22.39\n24.15\n29.30\n0.46\n0.53\n0.47\n0.43\n0.39\n0.35\n0.31\n0.26\n0.24\n0.14\n0.59\n0.65\n0.59\n0.5\n0.47\n0.44\n0.4\n0.36\n0.37\n0.31\n0.71\n0.75\n0.7\n0.61\n0.58\n0.52\n0.5\n0.52\n0.48\n0.46\n\u03b1\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nBlock Serial Number\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(a) \u03b1 in different feature blocks.\n(b) Embedding dimension vs \u03b7f.\nLoss of DNIS after Pruning\nAUC of DNIS after Pruning\nLoss of Network Pruning\nAUC of Network Pruning\nAUC\n0.77\n0.78\n0.79\n0.80\nLogloss\n0.45\n0.46\n0.47\n0.48\n0.49\n0.50\n0.51\nPruning Rate (%)\n0\n20\n40\n60\n80\n100\n(c) Performance vs Pruning rates.\nFigure 3: (a) The distribution of trained parameters \u03b1 of the soft selection layer. Here we show the result of MF on Movielens\ndataset, where L is set to 10. (b) The joint distribution plot of feature embedding dimensions and feature frequencies after\ndimension pruning. (c) Comparison of DNIS and network pruning performance over different pruning rates.\nperformance based on the application requirements.\n3.4\nAnalysis on DNIS Results\nWe\n\ufb01rst\nstudy\nthe\nlearned\nfeature\ndimensions\nof\nDNIS through the learned soft selection layer \u03b1 and\nfeature embedding dimensions after dimension pruning.\nFigure 3a depicts the distributions of the trained parameters\nin \u03b1 for the 10 feature blocks on Movielens. Recall that\nthe blocks are sorted in the descending order of feature fre-\nquency. We can see that the learned parameters in \u03b1 for the\nfeature blocks with lower frequencies converge to smaller\nvalues, indicating that lower-frequency features tend to be\nrepresented by smaller numbers of embedding dimensions.\nFigure 3b provides the correlation between embedding\ndimensions and feature frequency after dimension pruning.\nThe results show that features with high frequencies end up\nwith high embedding dimensions, whereas the dimensions\nare more likely to be pruned for low-frequency features.\nNevertheless, there is no linear correlationship between the\nderived embedding dimension and the feature frequency.\nNote that the embedding dimensions for low-frequency\nfeatures scatter over a long range of numbers. This is\nconsistent with the inferior performance of MDE which\ndirectly determines the embedding dimensions of features\naccording to their frequency.\nWe\nfurther\ncompare\nDNIS\nwith\nnetwork\npruning\nmethod [Han et al. 2015]. For illustration purpose, we pro-\nvide the results of the FM model on Criteo dataset. Figure 3c\nshows the performance of two methods on different pruning\nrates (i.e., the ratio of pruned embedding weights). From the\nresult, DNIS achieves better AUC and Logloss results than\nnetwork pruning over all the pruning rates. This is because\nDNIS optimizes feature embeddings with the gradients from\nthe validation set, which bene\ufb01ts the selection of predictive\ndimensions, instead of simply removing redundant weights\nin the embeddings.\n4\nConclusion\nIn this paper, we propose Differentiable Neural Input Search\n(DNIS), a method that searches for mixed features embed-\nding dimensions in a differentiable manner through gradi-\nent descent. The key idea of DNIS is to introduce a soft\nselection layer that controls the signi\ufb01cance of each em-\nbedding dimension, and optimize this layer according to\nmodel\u2019s validation performance. We propose a gradient nor-\nmalization technique and a \ufb01ne-grained pruning procedure\nin DNIS to produce a \ufb02exible mixed embedding dimen-\nsion scheme for different features. The proposed method is\nmodel-agnostic, and can be incorporated with various exist-\ning architectures of latent factor models. We conduct exper-\niments on three public real-world recommendation datasets.\nThe results show that DNIS achieves the best predictive per-\nformance compared with existing neural input search meth-\nods with fewer embedding parameters and less time cost.\nReferences\nBaker, B.; Gupta, O.; Naik, N.; and Raskar, R. 2017. De-\nsigning Neural Network Architectures using Reinforcement\nLearning. In 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings.\nBergstra, J.; Yamins, D.; and Cox, D. D. 2013. Making a\nScience of Model Search: Hyperparameter Optimization in\nHundreds of Dimensions for Vision Architectures. In Pro-\nceedings of the 30th International Conference on Machine\nLearning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013,\n115\u2013123.\nCai, H.; Zhu, L.; and Han, S. 2019. ProxylessNAS: Direct\nNeural Architecture Search on Target Task and Hardware. In\n7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nChen, L.; Collins, M. D.; Zhu, Y.; Papandreou, G.; Zoph,\nB.; Schroff, F.; Adam, H.; and Shlens, J. 2018.\nSearch-\ning for Ef\ufb01cient Multi-Scale Architectures for Dense Image\nPrediction. In Advances in Neural Information Processing\nSystems 31: Annual Conference on Neural Information Pro-\ncessing Systems 2018, NeurIPS 2018, 3-8 December 2018,\nMontr\u00b4eal, Canada, 8713\u20138724.\nChen, Y.; Chen, B.; He, X.; Gao, C.; Li, Y.; Lou, J.; and\nWang, Y. 2019. \u03bbOpt: Learn to Regularize Recommender\nModels in Finer Levels. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discov-\nery & Data Mining, KDD 2019, Anchorage, AK, USA, Au-\ngust 4-8, 2019, 978\u2013986.\nCheng, H.; Koc, L.; Harmsen, J.; Shaked, T.; Chandra, T.;\nAradhye, H.; Anderson, G.; Corrado, G.; Chai, W.; Ispir, M.;\nAnil, R.; Haque, Z.; Hong, L.; Jain, V.; Liu, X.; and Shah, H.\n2016. Wide & Deep Learning for Recommender Systems.\nIn Proceedings of the 1st Workshop on Deep Learning for\nRecommender Systems, DLRS@RecSys 2016, Boston, MA,\nUSA, September 15, 2016, 7\u201310.\nCheng, W.; Shen, Y.; Zhu, Y.; and Huang, L. 2018. DELF: A\nDual-Embedding based Deep Latent Factor Model for Rec-\nommendation. In IJCAI\u201918, July 13-19, 2018, Stockholm,\nSweden, 3329\u20133335.\nCheng, Y.; Wang, D.; Zhou, P.; and Zhang, T. 2017. A sur-\nvey of model compression and acceleration for deep neural\nnetworks. arXiv preprint arXiv:1710.09282 .\nColson, B.; Marcotte, P.; and Savard, G. 2007. An overview\nof bilevel optimization. Annals OR 153(1): 235\u2013256.\nCovington, P.; Adams, J.; and Sargin, E. 2016. Deep Neu-\nral Networks for YouTube Recommendations. In Proceed-\nings of the 10th ACM Conference on Recommender Systems,\nBoston, MA, USA, September 15-19, 2016, 191\u2013198.\nDomhan, T.; Springenberg, J. T.; and Hutter, F. 2015. Speed-\ning Up Automatic Hyperparameter Optimization of Deep\nNeural Networks by Extrapolation of Learning Curves. In\nProceedings of the Twenty-Fourth International Joint Con-\nference on Arti\ufb01cial Intelligence, IJCAI 2015, Buenos Aires,\nArgentina, July 25-31, 2015, 3460\u20133468.\nElsken, T.; Metzen, J. H.; and Hutter, F. 2019.\nEf\ufb01cient\nMulti-Objective Neural Architecture Search via Lamarck-\nian Evolution. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019.\nFranceschi, L.; Frasconi, P.; Salzo, S.; Grazzi, R.; and Pontil,\nM. 2018. Bilevel Programming for Hyperparameter Opti-\nmization and Meta-Learning. In Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML 2018,\nStockholmsm\u00a8assan, Stockholm, Sweden, July 10-15, 2018,\n1563\u20131572.\nFrankle, J.; and Carbin, M. 2019. The Lottery Ticket Hy-\npothesis: Finding Sparse, Trainable Neural Networks.\nIn\n7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nGinart, A.; Naumov, M.; Mudigere, D.; Yang, J.; and Zou,\nJ. 2019.\nMixed Dimension Embeddings with Applica-\ntion to Memory-Ef\ufb01cient Recommendation Systems. arXiv\npreprint arXiv:1909.11810 .\nGuo, H.; Tang, R.; Ye, Y.; Li, Z.; and He, X. 2017. DeepFM:\nA Factorization-Machine based Neural Network for CTR\nPrediction. In Proceedings of the Twenty-Sixth International\nJoint Conference on Arti\ufb01cial Intelligence, IJCAI 2017, Mel-\nbourne, Australia, August 19-25, 2017, 1725\u20131731.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning\nboth weights and connections for ef\ufb01cient neural network. In\nAdvances in neural information processing systems, 1135\u2013\n1143.\nHarper, F. M.; and Konstan, J. A. 2016. The MovieLens\nDatasets: History and Context. ACM Trans. Interact. Intell.\nSyst. 5(4): 19:1\u201319:19.\nHe, X.; Liao, L.; Zhang, H.; Nie, L.; Hu, X.; and Chua, T.\n2017. Neural Collaborative Filtering. In Proceedings of the\n26th International Conference on World Wide Web, WWW\n2017, Perth, Australia, April 3-7, 2017, 173\u2013182.\nJoglekar, M. R.; Li, C.; Chen, M.; Xu, T.; Wang, X.; Adams,\nJ. K.; Khaitan, P.; Liu, J.; and Le, Q. V. 2020. Neural In-\nput Search for Large Scale Recommendation Models.\nIn\nKDD \u201920: The 26th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, Virtual Event, CA, USA,\nAugust 23-27, 2020, 2387\u20132397.\nLabs, C. 2014.\nKaggle Display Advertising Challenge\nDataset,\nhttp://labs.criteo.com/2014/02/kaggle-display-\nadvertising-challenge-dataset/.\nLi, H.; Kadav, A.; Durdanovic, I.; Samet, H.; and Graf, H. P.\n2017. Pruning Filters for Ef\ufb01cient ConvNets. In 5th In-\nternational Conference on Learning Representations, ICLR\n2017, Toulon, France, April 24-26, 2017, Conference Track\nProceedings.\nLi, L.; and Talwalkar, A. 2019. Random Search and Repro-\nducibility for Neural Architecture Search. In Proceedings of\nthe Thirty-Fifth Conference on Uncertainty in Arti\ufb01cial In-\ntelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, 129.\nLian, J.; Zhou, X.; Zhang, F.; Chen, Z.; Xie, X.; and Sun,\nG. 2018. xDeepFM: Combining Explicit and Implicit Fea-\nture Interactions for Recommender Systems. In KDD\u201918,\nLondon, UK, August 19-23, 2018, 1754\u20131763.\nLiu, H.; Simonyan, K.; and Yang, Y. 2019. DARTS: Dif-\nferentiable Architecture Search. In 7th International Con-\nference on Learning Representations, ICLR 2019, New Or-\nleans, LA, USA, May 6-9, 2019.\nLiu, Z.; Sun, M.; Zhou, T.; Huang, G.; and Darrell, T. 2019.\nRethinking the Value of Network Pruning. In 7th Interna-\ntional Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019.\nMaclaurin, D.; Duvenaud, D.; and Adams, R. P. 2015.\nGradient-based Hyperparameter Optimization through Re-\nversible Learning.\nIn Proceedings of the 32nd Interna-\ntional Conference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, 2113\u20132122.\nMendoza, H.; Klein, A.; Feurer, M.; Springenberg, J. T.; and\nHutter, F. 2016. Towards Automatically-Tuned Neural Net-\nworks. In Proceedings of the 2016 Workshop on Automatic\nMachine Learning, AutoML 2016, co-located with 33rd In-\nternational Conference on Machine Learning (ICML 2016),\nNew York City, NY, USA, June 24, 2016, 58\u201365.\nMiller, G. F.; Todd, P. M.; and Hegde, S. U. 1989. Designing\nNeural Networks using Genetic Algorithms.\nIn Proceed-\nings of the 3rd International Conference on Genetic Algo-\nrithms, George Mason University, Fairfax, Virginia, USA,\nJune 1989, 379\u2013384.\nMolchanov, P.; Tyree, S.; Karras, T.; Aila, T.; and Kautz,\nJ. 2017. Pruning Convolutional Neural Networks for Re-\nsource Ef\ufb01cient Inference. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings.\nPark, J.; Naumov, M.; Basu, P.; Deng, S.; Kalaiah, A.; Khu-\ndia, D.; Law, J.; Malani, P.; Malevich, A.; Nadathur, S.; et al.\n2018.\nDeep learning inference in facebook data centers:\nCharacterization, performance optimizations and hardware\nimplications. arXiv preprint arXiv:1811.09886 .\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\nDesmaison, A.; K\u00a8opf, A.; Yang, E.; DeVito, Z.; Raison, M.;\nTejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;\nand Chintala, S. 2019. PyTorch: An Imperative Style, High-\nPerformance Deep Learning Library. In Advances in Neu-\nral Information Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019, NeurIPS\n2019, 8-14 December 2019, Vancouver, BC, Canada, 8024\u2013\n8035.\nPedregosa, F. 2016. Hyperparameter optimization with ap-\nproximate gradient.\nIn Proceedings of the 33nd Interna-\ntional Conference on Machine Learning, ICML 2016, New\nYork City, NY, USA, June 19-24, 2016, 737\u2013746.\nReal, E.; Aggarwal, A.; Huang, Y.; and Le, Q. V. 2019. Reg-\nularized Evolution for Image Classi\ufb01er Architecture Search.\nIn The Thirty-Third AAAI Conference on Arti\ufb01cial Intelli-\ngence, AAAI 2019, The Thirty-First Innovative Applications\nof Arti\ufb01cial Intelligence Conference, IAAI 2019, The Ninth\nAAAI Symposium on Educational Advances in Arti\ufb01cial In-\ntelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27\n- February 1, 2019, 4780\u20134789.\nReal, E.; Moore, S.; Selle, A.; Saxena, S.; Suematsu, Y. L.;\nTan, J.; Le, Q. V.; and Kurakin, A. 2017. Large-Scale Evo-\nlution of Image Classi\ufb01ers. In Proceedings of the 34th In-\nternational Conference on Machine Learning, ICML 2017,\nSydney, NSW, Australia, 6-11 August 2017, 2902\u20132911.\nRendle, S. 2010. Factorization Machines. In ICDM 2010,\nThe 10th IEEE International Conference on Data Mining,\nSydney, Australia, 14-17 December 2010, 995\u20131000.\nVirtanen, P.; Gommers, R.; Oliphant, T. E.; Haberland,\nM.; Reddy, T.; Cournapeau, D.; Burovski, E.; Peterson, P.;\nWeckesser, W.; Bright, J.; van der Walt, S. J.; Brett, M.; Wil-\nson, J.; Jarrod Millman, K.; Mayorov, N.; Nelson, A. R. J.;\nJones, E.; Kern, R.; Larson, E.; Carey, C.; Polat, \u02d9I.; Feng,\nY.; Moore, E. W.; Vand erPlas, J.; Laxalde, D.; Perktold, J.;\nCimrman, R.; Henriksen, I.; Quintero, E. A.; Harris, C. R.;\nArchibald, A. M.; Ribeiro, A. H.; Pedregosa, F.; van Mul-\nbregt, P.; and Contributors, S. . . 2020. SciPy 1.0: Funda-\nmental Algorithms for Scienti\ufb01c Computing in Python. Na-\nture Methods 17: 261\u2013272.\nXie, S.; Zheng, H.; Liu, C.; and Lin, L. 2019.\nSNAS:\nstochastic neural architecture search. In 7th International\nConference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019.\nZhao, X.; Wang, C.; Chen, M.; Zheng, X.; Liu, X.; and Tang,\nJ. 2020.\nAutoEmb: Automated Embedding Dimensional-\nity Search in Streaming Recommendations. arXiv preprint\narXiv:2002.11252 .\nZhong, Z.; Yan, J.; Wu, W.; Shao, J.; and Liu, C. 2018. Prac-\ntical Block-Wise Neural Network Architecture Generation.\nIn 2018 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2018, Salt Lake City, UT, USA, June 18-\n22, 2018, 2423\u20132432.\nZoph, B.; and Le, Q. V. 2017. Neural Architecture Search\nwith Reinforcement Learning.\nIn 5th International Con-\nference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings.\nZoph, B.; Vasudevan, V.; Shlens, J.; and Le, Q. V. 2018.\nLearning Transferable Architectures for Scalable Image\nRecognition. In 2018 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2018, Salt Lake City,\nUT, USA, June 18-22, 2018, 8697\u20138710.\n",
    "2103.06124": "SEMANTICALLY CONSTRAINED MEMORY ALLOCATION\n(SCMA) FOR EMBEDDING IN EFFICIENT RECOMMENDATION\nSYSTEMS\nAditya Desai\u2217\nDepartment of Computer Science\nRice University\nHouston, Texas\napd10@rice.edu\nYanzhou Pan\u2217\nDepartment of Computer Science\nRice University\nHouston, Texas\nyp24@rice.edu\nKuangyuan Sun\nDepartment of Computer Science\nRice University\nHouston, Texas\nks94@rice.edu\nLi Chou\nDepartment of Computer Science\nRice University\nHouston, Texas\nlchou@rice.edu\nAnshumali Shrivastava\nDepartment of Computer Science\nRice University\nHouston, Texas\nanshumali@rice.edu\nABSTRACT\nDeep learning-based models are utilized\nto achieve state-of-the-art performance\nfor recommendation systems.\nA key\nchallenge for these models is to work\nwith millions of categorical classes or\ntokens.\nThe standard approach is to\nlearn end-to-end, dense latent represen-\ntations or embeddings for each token.\nThe resulting embeddings require large\namounts of memory that blow up with\nthe number of tokens. Training and in-\nference with these models create stor-\nage, and memory bandwidth bottlenecks\nleading to signi\ufb01cant computing and\nenergy consumption when deployed in\npractice. To this end, we present the\nproblem of Memory Allocation under\nbudget for embeddings and propose a\nnovel formulation of memory shared em-\nbedding, where memory is shared in pro-\nportion to the overlap in semantic infor-\nmation. Our formulation admits a prac-\ntical and ef\ufb01cient randomized solution\nwith Locality sensitive hashing based\nMemory Allocation (LMA). We demon-\nstrate a signi\ufb01cant reduction in the mem-\nory footprint while maintaining perfor-\nmance.\nIn particular, our LMA em-\nbeddings achieve the same performance\ncompared to standard embeddings with\na 16\u00d7 reduction in memory footprint.\n*First two authors have equal contribution\nMoreover, LMA achieves an average im-\nprovement of over 0.003 AUC across\ndifferent memory regimes than standard\nDLRM models on Criteo and Avazu\ndatasets\n1\nIntroduction\nRecommendation systems are at the core of business\nfor companies such as Amazon, Facebook, NetFlix, and\nGoogle. These companies offer a wide array of products,\nmovies, ads, etc. for customers to view and to choose from.\nTherefore, developing automated, intelligent, and person-\nalized recommendation systems help guide customers to\nmake more informed choices. It is worthwhile to note\nthe signi\ufb01cant monetary impact of recommendation model\naccuracy on the aforementioned companies. With the size\nof notional business, even an increase of 0.001 in accu-\nracy/AUC metrics implies considerable gains in revenue\nand customer experience. However, categorical features\n(e.g., product history, pages liked, etc.) dominate most\nof the recommendation data [1, 2], thereby posing new\nmodeling challenges for learning. Following the lines of\nnatural language processing [3, 4], state-of-the-art meth-\nods in recommendation models [1, 5] have found success\nwith mapping each of the category values in the feature to\na dense representation. These representations are learned\nand stored in memory tables called embedding tables.\nHeavy embedding tables and memory bandwidth bot-\ntleneck: Embedding tables store learned dense represen-\ntation of each category value. If the set of all categories\nis S and the embedding dimension is d. The embedding\ntable size would be |S|\u00d7d. With the number of categories\narXiv:2103.06124v1  [cs.IR]  24 Feb 2021\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nas large as tens of millions for each feature, embedding ta-\nbles can take up over 99.9% of the total memory. Namely,\nmemory footprint can be multiple gigabytes or even ter-\nabytes [6, 7, 8]. In practice, deploying these large mod-\nels often requires the model to be decomposed and dis-\ntributed across different machines due to memory capacity\nrestrictions [9]. Extensive memory utilization creates mem-\nory bandwidth issues due to the highly irregular locality\nof accesses making training and inference considerably\nslower [1]. This issue exacerbates further when multiple\nmodels need to be co-located on a single machine [9].\nImpact of improving memory usage of Embedding Ta-\nbles: Memory consumption of embedding tables is a se-\nvere problem in recommendation models. Improving mem-\nory utilization can improve recommendation systems on\nvarious fronts. 1) It has been observed that larger embed-\nding size in the model leads to better performance [10].\nBetter memory utilization would imply scope to train and\ndeploy complex models. 2) Lower memory footprint will\nimprove the training and inference speed. With changing\nconsumer interests, recommendation data inherently suf-\nfers from concept shift [11], requiring a frequent refresh of\nmodels. With faster training, models can be retrained more\nfrequently, improving their average performance. Hence,\nmemory utilization forms a critical problem requiring at-\ntention.\nDeep learning recommendation model (DLRM) [1] gave\nrise to an increased interest in constructing more memory-\nef\ufb01cient embeddings. Recent SOTA works include com-\npositional embedding using complementary partition [12]\nand mixed dimension embeddings [6]. A simple mem-\nory sharing scheme for weight matrices in deep learning\nmodels was proposed by [13]. However, embeddings for\ntokens are weight matrices that have a structure for which\nwe can reduce the memory burden in an intelligent man-\nner by sharing memory for similar concept tokens. For\nexample, if two tokens represent the concepts \u201cNike\u201d and\n\u201cAdidas,\u201d we would expect the pair to share more weights\ncompared to \u201cNike\u201d and \u201cJaguar.\u201d In this paper, we ap-\nproach the problem of learning embedding tables under\nmemory budget by solving a generic problem, which we\nrefer to as semantically constrained memory allocation\n(SCMA) for embeddings. SCMA is a hard combinatorial\nallocation problem since it involves millions of variables\nand constraints. Surprisingly, as we show later, that there\nis a very neat dynamic allocation of memory using locality\nsensitive hashing which solves SCMA in approximation.\nThe memory allocation can be done in an online consistent\nmanner for each token with negligible overhead.\nThis paper is organized as follows: we \ufb01rst formally de-\nscribe the problem and notation in section 2, followed by\na recap of hashing schemes in section 3 and a generic so-\nlution to the problem in section 4. Section 5 onward we\nfocus on applying the solution to DLRM problem. Section\n6 discusses some related work and we present experimental\nevaluation in section 7.\n...\nFigure 1: Embedding table E \u2208R|S|\u00d7d. x \u2208{0, . . . , |S|\u2212\n1} is a categorical feature represented by a one-hot vector.\nF is a universal function approximator typically parame-\nterized by a neural network.\n2\nSemantically Constrained Memory\nAllocation (SCMA) for Embeddings\nLet S = {0, 1, . . . , |S|\u22121} denote the set of all values\nfor all categorical features in the dataset. The embedding\ntable E \u2208R|S|\u00d7d is a matrix such that each row represents\nembeddings, each with dimension d, for all values in S. An\nembedding, ev, for a particular value, is retrieved by ev =\nE[v, :] (see Figure 1). The embedding table E imposes a\nsimilarity structure S w.r.t a similarity measure obtained by\na kernel function \u03c6(., .). S \u2208R|S|\u00d7|S| and each entry of\nthis matrix S[v1, v2] = \u03c6(E[v1, :], E[v2, :]) also denoted\nas \u03c6(v1, v2)\nThe problem of end-to-end learning of full embedding\ntable E is well-known [1, 14, 3, 15, 4]. In this paper,\nwe consider the problem of learning E under a memory\nbudget m. Let M be the memory such that |M| = m.\nIf |S|\u00d7d > m, then multiple elements of the embedding\ntable have to share memory locations in M. We formally\nde\ufb01ne allocation function A as the mapping from elements\nof E to the actual memory locations in M.\nDe\ufb01nition 1 (Allocation Function) Allocation function\nfor an embedding table E \u2208R|S|\u00d7d using the memory\nM with budget |M| = m, is de\ufb01ned as the following map.\nA : {0, . . . , |S|\u22121} \u2192{0, 1}d\u00d7m\ns.t.\n\u2200v \u2208S, \u2200i \u2208{0, . . . , d\u22121},\nX\nj\n(A(v)[i, j]) = 1.\nThe ith row of the matrix output by A, for any value v is a\none-hot encoding of the location to which the ith element\nof the embedding vector for v maps to in M. Using the\nallocation function, we can retrieve the embedding by,\nE[v, i] = M[A(v)[i, :]]\nwhere we assume mask-based retrieval on M. We next de-\n\ufb01ne the notion of shared memory between two embeddings\nunder a allocation function A.\nDe\ufb01nition 2 (Consistent Memory Sharing) The\nFrac-\ntion of Consistently Shared Memory (fA) between two\nembeddings of size d for values v1 and v2 from E under\nallocation A is de\ufb01ned as\nfA(v1, v2) = 1\nd\u27e8A(v1), A(v2)\u27e9F\nwhere \u27e8., .\u27e9F is the Frobenius inner product.\n2\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nWe can describe various allocation schemes in terms of the\nallocation function. For example, Afull describes full em-\nbedding which is possible when m>(|S|d). Ah describes\nthe na\u00a8\u0131ve hashing trick (hash function h) based allocation\n(see section 6 for details) which works with any memory\nbudget. Speci\ufb01cally, \u2200v \u2208S and i \u2208{0, .., d\u22121}, we have\nthe following.\nAfull(v)[i, j] = 1,\niff j = mv + i.\nAh(v)[i, j] = 1,\niff j = h(v, i).\nIt is easy to verify that for every pair of values v1 and v2\nfAfull(v1, v2) = 0 whereas fAh(v1, v2) = 0 is random\nvariable with expected value 1/m. We now de\ufb01ne the\nnotation we will use throughout the paper.\nDe\ufb01nition 3 (General Memory Allocation (GMA))\nLet E\u2217be the true embedding table which encodes the\nsimilarity structure S\u2217. Let E be an embedding table\nrecovered from M with budget m under allocation A\nand S be the semantic similarity encoded by E Let both\nsimilarities be encoded by the kernel function \u03c6(, ). We\nwill refer to this as General Memory Allocation (GMA)\nsetup\nThought experiment: We can de\ufb01ne the problem of op-\ntimal allocation under memory budget when (m < |S|d),\nunder GMA setup as\nargmin\nA\nmin\nM \u03b6(S\u2217, S)\nwhere \u03b6 is a metric on R|S|\u00d7|S| (e.g., Euclidean). In or-\nder to incorporate the learning of M, we pose the prob-\nlem as \ufb01nding A with best possible associated M con-\nsidering that if we choose A beforehand and then learn\nM, the learning process will choose the best M. Solv-\ning this exact problem appears to be hard. Instead, let\nus think about an allocation scheme for which we have\nsome evidence of the existence of suitable M. Let us\nconsider a M with each element independently initialized\nusing Bernoulli(0.5, {\u22121, +1}). If we choose an alloca-\ntion with constraints based on similarity structure S\u2217as\nfA(v1, v2) = S\u2217[v1, v2]\n\u2200v1, v2 \u2208S\nthen one can verify (as we will show in Theorem 2) that\nunder this random initialization of memory, pairwise co-\nsine similarity of embeddings of any two values v1 and v2\nretrieved from M via A, denoted as Cs(v1, v2), is a ran-\ndom variable with expectation E(Cs(v1, v2)) = S\u2217[v1, v2]\nand variance Var(Cs(v1, v2)) \u221d1\nd. Hence, this provides\nevidence that for a semantic similarity based shared alloca-\ntion, AS\u2217, there is an assignment to M, which can produce\nreasonably small \u03b6(S\u2217, S). We interpret the \u00b11 assignment\nto each element of embedding as membership to particu-\nlar concepts, and the overlap in membership determines\nthe similarity. Following this insight, we formally de\ufb01ne\nthe semantically constrained memory problem allocation\n(SCMA) for embeddings as follows.\nDe\ufb01nition 4 (SCMA Problem) Under the GMA setup\n(see Def. 3), Semantically Constrained Memory Alloca-\ntion (SCMA) is a problem to \ufb01nd allocation A, under\nthe constraints that for every pair i, j \u2208S, we have\nfA(i, j) = S\u2217[i, j].\nSCMA problem can be posed as a mixed integer program-\nming (MIP) problem with O(|S|2) constraints appearing\nfrom similarity constraints along with O(|S|d) sparsity\nconstraint that the allocation matrix demands. Applying\nMIP to solve the SCMA problem has the following draw-\nbacks: 1) MIP is computationally intractable for large-\nscale problems; 2) The solution of MIP needs to be stored\nexplicitly so it is memory expensive; and 3) In case of\naddition of new values to categorical features, the problem\nneeds to be repeatedly solved. Due to the dif\ufb01culty of\nSCMA, in this paper, we formulate and solve the problem\nusing a randomized approach which we term randomized\nSCMA (RSCMA). We de\ufb01ne RSCMA as follows.\nDe\ufb01nition 5 (RSCMA Problem) Under the GMA setup\n(see Def. 3), the Randomized Semantically Constrained\nMemory Allocation (RSCMA) is a problem is to \ufb01nd\nallocation A under the constraints that for every pair\ni, j \u2208{0, ..., |S| \u22121}, we have\n|fA(i, j) \u2212\u03c6(i, j)| \u2264\u03f5\nwith probability (1 \u2212\u03b4) for some small \u03f5, \u03b4 > 0.\nCompared to applying MIP solvers to the SCMA problem,\nour approach to RSCMA has the following advantages:\n1) LSH based solution is cheaper to compute, 2) storage\ncost is cheap, and 3) the addition of new values does not\nrequire resolving the problem. Although the exact similar-\nity between embeddings is not known a priori, a notion of\nsimilarity exists in the data. For example, term-document\nestablishes a similarity between terms based on Jaccard\nsimilarity on term-document vectors. We use this simi-\nlarity to impose a structure on the allocation of memory.\nNamely, we leverage locality sensitive hashing (LSH) to\nsolve the RSCMA problem.\n3\nHashing Schemes\nWe describe important hashing schemes from randomized\ndata structures that will be used in our solution to RSCMA.\n3.1\nUniversal Hashing\nConsider the problem of mapping each element of a large\nuniverse U to a smaller range {0, . . . , r\u22121}. A family\nof hash functions H = {h : U \u2192{0, . . . , r\u22121}} is k\nuniversal if for any k distinct elements (x1, . . . , xk) \u2208U\nand any k indices (\u03b21, . . . , \u03b2k) \u2208{0, . . . , r\u22121}, we have\nPrh\u2208H((h(x1) = \u03b21) \u2227. . . \u2227(h(xk) = \u03b2k) = 1/rk [16].\nWe utilize the following k-universal hash function family.\nH = {h(x) = (a0 + \u03a3k\u22121\ni=1 aixi) % P % r\n| a0 \u2208{0, ..., P\u22121}, ai \u2208{1, ..., P\u22121}},\nwhere P is a large prime number. An instance of hash\nfunction drawn from this family is stored by using only k\nintegers {ai}k\u22121\ni=0 .\n3.2\nLocality Sensitive Hashing\nLocality sensitive hashing (LSH) [17] is a popular tool\nused in approximate near neighbour search. Consider a\nfunction l : U \u2192{0, ..., r\u22121}. If l is drawn randomly\nfrom a LSH family, say L, then the probability that two\n3\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nFigure 2: shows consistent sharing of 2 locations when\nusing LMA\nelements x and y share same hash value (probability of\ncollision pc) is dependent on a de\ufb01ned similarity measure,\nSim, between a and b. Speci\ufb01cally,\nPrl\u2208L(l(x) == l(y)) \u221dSim(x, y).\nThis probability of collision de\ufb01nes a kernel function\n\u03c6(x, y), which is bounded 0 \u2264\u03c6(x, y) \u22641, symmetric\n\u03c6(x, y) = \u03c6(y, x), and re\ufb02ective \u03c6(x, x) = 1. We can cre-\nate multiple LSH families parameterized by k, Lk, from\na given LSH family L by using k-independently drawn\nfunctions from L. Let {li}k\ni=1 be k independently drawn\nfunctions from L. The new LSH function \u03c8 \u2208Lk and its\nkernel function is de\ufb01ned as\n\u03c8(x) = (l1(x), l2(x), ...lk(x));\n\u03c6(x, y)Lk = \u03c6(x, y)k\nL.\nWe call parameter k, the power of LSH functions. The\nrange of certain LSH functions, particularly functions with\nlarge power, can be extremely large and needs rehashing\ninto a range, say {0, .., r\u22121}. This is generally achieved\nusing additional universal hash functions, say h with range\nr. Let the rehashed version of the function L be denoted\nby Lr. Then, the kernel of this rehashed LSH is\nlr(x) = h(l(x)); \u03c6(x, y)Lr = \u03c6(x, y)L + 1 \u2212\u03c6(x, y)L\nr\n.\n3.3\nMinwise Hashing\nThe minwise hash function is a LSH function that take sets\nas inputs. The minwise family is as de\ufb01ned below.\nLmh = {l\u03c0|\u03c0 : U \u2192U, \u03c0 is a permutation},\nl\u03c0(A) = min({\u03c0(x)|x \u2208A})\nwhere A \u2286U\nFor a particular function, l\u03c0, the hash value of A is the min-\nimum of the permuted values of elements in A. As it turns\nout, the kernel function de\ufb01ned by the collision probability\nof the minwise hash family is the Jaccard Similarity (J).\nJ measures the similarity between two given sets A and B\nas J(A, B) = |A \u2229B|/|A \u222aB|. It is easy to check that\n\u03c6(A, B)Lmh = J(A, B).\n4\nLSH based Memory Allocation(LMA):\nSolution to RSCMA\nConsider the standard GMA setup (see Def. 3) with the\nsemantic structure S\u2217de\ufb01ned by a LSH kernel \u03c6(, ) We\nprovide an LSH based memory allocation (LMA) solution\nto this RSCMA. Let the LSH family corresponding to this\nkernel be L. As de\ufb01ned in Section 3.2, the probability\nof collision of values v1 and v2 for corresponding LSH\n(a) fAL(v1, v2) vs \u03c6(v1, v2)\n(b) Cs(v1, v2) vs \u03c6(v1, v2)\nFigure 3: \u00b5 \u22131.96\u03c3 regions for different values of d as per\ntheorems 1 and 2. For \u02c6fAL this is 95% con\ufb01dence intervals.\nFor Cs, this is very close to 95% con\ufb01dence interval when\nm is large\nfunction l and rehashed LSH function lr can be written as\nPrl\u2208L(l(v1) == l(v2)) = \u03c6(v1, v2),\nPrl\u2208L(lr(v1) == lr(v2)) = \u03c6(v1, v2) + 1 \u2212\u03c6(v1, v2)\nm\n.\nWe use d independently drawn LSH functions {l(i)}d\ni=1.\nThe LMA solution de\ufb01nes the allocation AL as\nAL(v)[i, :] = one-hot(l(i)\nr (v))\n\u2200v \u2208S,\nwhere one-hot : {0, ..m \u22121} \u2192{0, 1}m such that for any\narbitrary i \u2208{0, .., m \u22121}, i \u0338= j, one-hot(i)[j] = 0 and\none-hot(i)[i] = 1. LMA scheme is illustrated in Figure 2.\nIn the following theorem, we prove that LMA with the allo-\ncation de\ufb01ned by AL indeed solves the RSCMA problem.\nThe proof of this theorem is present in the Supplementary.\nTheorem 1 (LMA solves RSCMA) Under\nthe\nGMA\nsetup (see def 3), for any two values v1 and v2, the fraction\nof consistently shared memory fAL as per allocation AL\nproposed by LMA is a random variable with distribution,\nE(fAL(v1, v2)) = \u0393 = \u03c6(v1, v2) + 1 \u2212\u03c6(v1, v2)\nm\n,\nV(fAL(v1, v2)) = \u0393(1 \u2212\u0393)\nd\n,\nPr\n\u0012\n|fAL(v1, v2)\u2212\u03c6(v1, v2)| > \u03b7\u0393 + 1\u2212\u03c6(v1, v2)\nm\n\u0013\n\u22642 exp\n\u001a\u2212d\u0393\u03b72\n3\n\u001b\n,\nfor all \u03b7 > 0. Hence, LMA solves RSCMA with \u03f5 =\n\u03b7\u0393 + 1\u2212\u03c6(v1,v2)\nm\nand \u03b4 = 2 exp\nn\n\u2212d\u0393\u03b72\n3\no\n.\nProof sketch: Proof consists of analyzing the random\nvariable for the fraction fAL(v1, v2) and applying Chernoff\nconcentration inequality to obtain the tail bounds.\nInterpretation: A reasonable memory M of 10Mb would\nhave |M| > 106. Hence for all practical purposes, we can\nignore the 1/m terms above. Then, the consistently shared\nfraction has expected value \u03c6(v1, v2) and variance that is\nproportional to 1/d. A good way to visualize the fact that\nLMA indeed gives a solution to RSCMA problem is to see\nthe 95% con\ufb01dence interval of the fraction, fAL(v1, v2),\n4\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nagainst the value of \u03c6(v1, v2) as shown in Figure 3. The\nparameter \u03b7 is a standard parameter that controls the trade-\noff between error (\u03f5) and con\ufb01dence (1 \u2212\u03b4). We can\nchoose \u03b7 very small to reduce error, but then we lose\ncon\ufb01dence, and instead if we choose \u03b7 large enough to\nreduce \u03b4, hence increase con\ufb01dence, then we have more\nerror.\nNext, we prove that if we use LMA to solve RSCMA, we\nindeed provide an allocation, for which there is an assign-\nment of values to M which can lead to an embedding table\nE, whose associated similarity S as measured by cosine\nsimilarity is closely distributed around S\u2217and hence gives\nsmaller \u03b6(S, S\u2217) (notation as introduced in section 2).\nTheorem 2 (Existence of M with LMA for S\u2217) Under\nthe GMA setup (see Def. 3), let us initialize each element\nof M independently from a Bernoulli(0.5, {\u22121, +1}).\nThen, the embedding table E generated via LMA on\nthis memory, has , for every pair of values v1 and v2,\nthe cosine similarity Cs(E[v1, :], E[v2, :]), denoted by\nCs(v1, v2) is distributed as\nE(Cs(v1, v2)) = \u0393 = \u03c6(v1, v2) + 1 \u2212\u03c6(v1, v2)\nm\n,\nV(Cs(v1, v2)) = 1 \u2212\u03932\nd\n+ 2(1 \u2212\u0393)(d \u22121)\ndm2\n,\nPr\n\u0012\n|Cs(v1, v2) \u2212\u03c6(v1, v2)| \u2265\u03b7\u0393 + 1 \u2212\u03c6(v1, v2)\nm\n\u0013\n\u22641 \u2212\u03932\nd\u03b72\u03932 for any \u03b7 > 0.\nProof sketch: Proof consists of analyzing the random\nvariable for the cosine similarity Cs(E[v1, :], E[v2, :]) and\napplying Chebyshev\u2019s concentration inequality to obtain\nthe tail bounds.\nInterpretation: We can ignore 1/m for any reasonably\nlarge memory. Then, the expected value of cosine similar-\nity is exactly \u03c6(v1, v2) and it is closely distributed around\nit. Chebyshev\u2019s inequality gives a looser bound and that\nis apparent from the probable region shown in Figure 3.\nIn this formulation, again \u03b7 is the parameter controlling\nerror and con\ufb01dence. Theorem 2 shows that if we have\nsuch a andomly initialized memory, then LMA will lead to\nintended similarities in approximation.\n4.1\nLMA: Dynamic Solution to RSCMA\nUnlike any static solution (eg. MIP) to SCMA, LMA solu-\ntion to RSCMA is highly relevant in a real-world setting.\nThe addition of new features values to datasets is generally\nfrequent, particularly in recommendation data. Any static\nsolution to SCMA will need re-computation every time a\nvalue is added. LSH based LMA solution is unaffected by\nthis and can gracefully incorporate new values.\n5\nLMA for Recommendation Systems\nLet us now consider the application of LMA for embed-\ndings of categorical values in ML recommendation sys-\ntems. DLRM can be used as a running example. However,\nFigure 4: LMA for DLRM model using common memory\nacross the tables\nour approach applies to any system that uses embedding\ntables. When learning from data in practical settings, S\u2217is\noften unknown as E\u2217is not known. However, we can use\nthe Jaccard similarity between pairs of values and use the\nsimilarity structure to obtain a proxy for S\u2217. We compute\nthe Jaccard similarity as follows. Let Dv be the set of all\nsample ids in data in which the value v for some categori-\ncal feature appears (e.g., a row in a term-document matrix).\nThen, we can de\ufb01ne the similarity S\u2217as\nS\u2217[v1, v2] = J(Dv1, Dv2).\nThe resulting kernel is the Jaccard kernel, which is an\nLSH kernel with the minwise LSH function. To hash a\nvalue, say v, with the minwise hash function, we will use\nthe corresponding set Dv. We can then use the general\nLMA setup described in Section 4 to adjust our training\nalgorithms.\nSize of data required: It may appear that storing the\ndata for minhash computations would diminish memory\nsavings. However, recall that we need data only to ob-\ntain a good estimate of Jaccard. We \ufb01nd that the actual\ndata required, D\u2032, is signi\ufb01cantly less than the total data.\nNamely,|D\u2032| \u226a|S|d. Therefore, to formalize, we bound\nthe number of non-zeros required for Jaccard computation.\nTheorem 3 (D\u2032 required is small) Assuming a uniform\nsparsity of each value, s, the Jaccard Similarity of two\nvalues x and y,say J = J(Dx, Dy) when estimated from a\ni.i.d subsample D\u2032 \u2286D, |D\u2032| = n is distributed around J\nas follows.\n|E( \u02c6J) \u2212J| \u2264\u03f5J,\n|V( \u02c6J) \u2212A| \u22642\u03f5(A + 2J2),\nwhere A =\nJ\n2ns(1 + J \u22122sJ) with probability (1 \u2212\u03b4)\nwhere \u03b4 = 1+J\u22122s\n2ns\u03f52 .\nInterpretation: The bound given by Theorem 3 is loose\ndue to the approximations done to analyze this random\nvariable (see Supplemental). Nonetheless, it can be seen\nthat for a given value of \u03b4, we can control \u03f5 and A through\nns, which is the number of non-zeros per value. As ns\nincreases, both the variance and deviation of the expected\nvalue from J decrease rapidly. In practice, Section 7 shows\nwe only need around 100K samples for the Criteo dataset\nout of 4.5M samples. Below we discuss a few considera-\n5\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\ntions in relation to LMA applied to DLRM model.\nMemory Comparison: The size of the memory used by\nfull embeddings is O(|S|d). The linear dependence on d\nand the typical very large size of set S makes it dif\ufb01cult\nto train and deploy large dimensional embedding models.\nLMA solutions to RSCMA make it possible to simulate\nthe embeddings of size O(|S|d) using any memory budget\nm. The memory cost with LMA procedure comprises of:\n1) Memory budget: O(m) m = |M|; 2) Cost of storing\nLSH function: O(kd + k\u2032) required to store d minhash\nfunctions with power k and k\u2032-universal hash function for\nrehashing. Generally these values are very small compared\nto O(|S|d) and can be ignored; and 3) The size of Data D\u2032\nstored and used for minhash functions. Generally, size of\nD\u2032 is much smaller than (|S|d). For example in Criteo, the\nsubsample we used had around 3M integers (when using\n125K samples) as compared to the range of 50M-540M\n\ufb02oating parameters of the models we train. We empirically\nanalyze the effect on various sizes of D\u2032 in the experi-\nmental section. This requirement of smaller D\u2032 is also an\neffect of the way we handle very sparse features, which is\ndiscussed next. To summarize, the memory cost of LMA\nis O(m + kd + k\u2032 + |D\u2032|) \u2248O(m).\nHandling very sparse features: For very sparse features,\nthe embedding quality does not signi\ufb01cantly affect the\naccuracy of the model [6]. Also, it is dif\ufb01cult to get a\ngood estimate of Jaccard similarity for this using a small\nsubsample. Due to these reasons, for very sparse features,\nwe randomly map each element of its embedding into M.\nEssentially, we revert to Ah (na\u00a8\u0131ve hashing trick) for such\nvalues.\nCommon Memory: We use a single common memory\nM across all embedding tables in DLRM. The idea is to\nfully utilize all similarities to share maximum memory and\nhence get the best memory utilization.\nForward Pass: The forward pass requires retrieving em-\nbeddings from M. Let Vbatch be the set of all values in\na batch. We collect the set {Dv|v\u2208Vbatch}, apply GPU\nfriendly implementation of d-minhashes to it to obtain a\nmatrix of locations, I \u2208R|Vbatch|\u00d7d. Using this we get\nEbatch = M[I].\nBackward Pass: The memory M contains the parame-\nters which are used to construct embeddings. Hence, in\nthe backward pass, the gradient of parameters in M is\ncomputed and these parameters are updated. The exact\nfunctional dependence of the result on a parameter in M\nis complex as it is implemented via LSH mappings. Auto\ngradient computation packages of deep learning libraries\n(e.g., PyTorch and TensorFlow) are used for gradient com-\nputation.\n6\nRelated Work\nWe focus on related works that signi\ufb01cantly reduce the size\nof the embedding matrix for recommendation and click-\nthrough prediction systems. Namely, hashing trick [13],\ncompositional embedding using complementary parti-\ntions [12], and mixed dimension embedding [6].\nNa\u00a8\u0131ve hashing trick: Given the embedding table E \u2208\nR|S|\u00d7d, two basic approaches that leverage hashing trick\nare presented.\n\u2022 Vector-wise (or row-wise): let \u02c6E \u2208Rm\u00d7d such that\nm\u00d7d = |M|, the memory budget, and m \u226a|S| be the\nreduced size embedding table. We use a hash function\nh : {0, 1, . . . , |S|\u22121} \u2192{0, 1, . . . , m\u22121} that maps\nthe (row-wise) indices of the embeddings from the full\nembedding table E to the reduced embedding table \u02c6E.\nThe size of the embedding table is reduced from O(|S|d)\nto O(md).\n\u2022 Element-wise (or entry-wise): let \u02c6E \u2208Rm such that\nm = |M|. We use a hash function h : {(i, j)}|S|,d\ni,j=0 \u2192\n{0, 1, . . . , m\u22121} that maps each element Ei,j to an ele-\nment in \u02c6E. The size of the embedding table is reduced\nfrom O(|S|d) to O(m) [13].\nCompositional embedding using complementary parti-\ntions: In the vector-wise hashing trick, multiple embed-\nding vectors are mapped to the same index, which results\nin loss of information on the unique categorical values and\nreduction in expressiveness of the model. To overcome this\nissue, [12] proposes to construct complementary partitions\nof E, from set theory, and apply compositional operators\non embedding vectors from each partition table to gener-\nated unique embeddings. One example of a complementary\npartition is the so-called quotient-remainder (QR) trick.\nTwo embedding tables \u02c6E1 \u2208Rm\u00d7d and \u02c6E2 \u2208R(|S|/m)\u00d7d\nare created, and two hash functions h1 and h2 respectively\nare used for mapping. h1 maps the i-th row of E to the j-th\nrow of \u02c6E1 using the remainder function: j = i mod m.\nh2 maps the i-th row of E to the k-th row of \u02c6E2 using\nthe function k = i \f m, where \f denotes integer division\n(quotient). Taking the embeddings ej \u2208\u02c6E1 and ek \u2208\u02c6E2\nand applying element-wise multiplication ej \u2299ek results\nin a unique embedding vector. The resulting memory com-\nplexity is O( |S|\nm d + md). In general, the optimal memory\ncomplexity is O(k|S|1/kd) with k complimentary parti-\ntions of sizes {mi \u00d7 d}k\ni=1 such that |S| \u2264\u03a0k\ni=1mi.\nMixed dimension (MD) embedding: Frequency of cate-\ngorical values are often skewed in real-world applications.\nInstead of a \ufb01xed (uniform) embedding dimension for all\ncategories, [6] proposes that embedding dimensions scale\naccording to popularity of categorical values. Namely,\nmore popular values are set to higher dimension embed-\ndings (i.e., allocate more memory) and vice versa. The idea\nis to create embedding tables, along with a projection ma-\ntrix, of the form {(\u02c6Ei, \u02c6Pi)}k\ni=1, such that \u02c6Ei \u2208R|Si|\u00d7di\n(MD embedding), \u02c6Pi \u2208Rdi\u00d7 \u00afd (projection), \u00afd \u2265di, and\n|S| = Pk\ni=1 |Si|. k and d = (d1, . . . , dk) are input pa-\nrameters. One approach proposed to set the parameters is\nbased on a power-law sizing scheme using a meta temper-\nature parameter \u03b1. Let p = {p1, . . . , pk}, where pi is a\nprobability value (e.g., pi = 1/|Si|). Then, \u03bb = \u00afd||p||\u2212\u03b1\n\u221e\nis the scaling factor and d = \u03bbp\u03b1 is the component-wise\n6\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\n(a)\nVarying\nnh\nwith\n\u03b1=32\u00d7,\nns=125K\n(b) Varying \u03b1 with nh=2, ns=125K\n(c) Varying ns with nh = 2, \u03b1=32\u00d7\nFigure 5: Effect of hyperparameters on performance of LMA-DLRM\nTable 1: Description of datasets. cat: categorical, int:\ninteger.\n#Samples\n#Features\n(cat+int)\nPositive\nrate\n#Values\nCriteo\n46M\n26 + 13\n26%\n33.76M\nAvazu\n41M\n21 + 0\n17%\n9.45M\nexponent. \u03b1 = 0 implies uniform dimensions and \u03b1 = 1\nsets dimensions proportional to popularity.\nComparison with LMA: Both QR Trick and MD Trick\nchange the embedding design. LMA does not affect em-\nbedding design directly. Instead, it solves the abstract\nproblem of RSCMA. We can apply LMA in conjunction\nwith any such embedding design tricks to obtain better\nmemory utilization.\n7\nExperiments\nDatasets: To evaluate the performance of LMA on DLRM\n(LMA-DLRM), we use two public click-through rate\n(CTR) prediction datasets: Criteo and Avazu. Criteo is\nused in the DLRM paper [1] and related works focused on\nmemory-ef\ufb01cient embeddings [6, 12]. Avazu is a mobile\nadvertisement dataset. A summary of dataset properties is\npresented in Table 1. Values represent the number of all\nthe categorical values. The number of values per feature\nvaries a lot : for example, some lower values are 10K and\nthey go as high as 10M.\nMetrics: We use loss, accuracy, and ROC-AUC as metrics\nfor our experiments. For imbalanced datasets like these,\nAUC is a better choice of metric than accuracy.\nBasic setup: For all our experiments (LMA-DLRM, base-\nlines), we follow the basic setup (e.g., optimizer parame-\nters, model architecture) suggested by the DLRM paper [1]\nand as implemented in their GitHub page. The batch size\nis set to 2,048 as we want to run experiments for a larger\nnumber of epochs. Also, a higher batch size is preferred\nfor CTR datasets [10]. We use the same settings for LMA-\nDLRM as well as all baselines for a fair comparison.\n7.1\nHyperparameter experiments\nThere are three hyperparameters: nh : power of LSH func-\ntion see 3.2, \u03b1: expansion rate, and ns : number of samples\nin D\u2032 for LMA-DLRM. The best values for each of these\nhyperparameters can be chosen via cross-validation. We\ndescribe these parameters and qualitatively analyze their\neffect on performance. The results of changing one param-\neter while keeping the other two \ufb01xed are shown in Figure\n5. We use 270M budget for varying \u03b1 and 35M budget for\nothers.\n\u2022 Power of LSH (nh) The nh used per LSH mapping (i.e.\npower in Section 3.2) controls the probability of colli-\nsion (i.e., Jnh) of corresponding elements of different\nembeddings as discussed in Section 3.2. Higher power\nleads to a lower probability of collision, making the re-\nhashed LSH function behave more like na\u00a8\u0131ve hashing\ntrick. Very low power will increase memory sharing and\nmight lead to its under-utilization. This phenomenon can\nbe observed in Figure 5a, where nh=1 gives the worst\nperformance. Increasing nh improves the performance\nuntil nh = 8. Performance worsens when nh = 32 and\ntends towards the hashing trick performance as expected.\n\u2022 Expansion rate (\u03b1) LMA-DLRM can simulate embed-\nding tables of any dimension d. We de\ufb01ne expansion\nrate as the ratio of simulated memory to actual memory.\nUsing GMA notation, \u03b1 = |S|d/|M|. Figure 5b shows\nthat 16 works best for memory budget of \u223c270M param-\neters. So increasing \u03b1 will not improve the performance\ninde\ufb01nitely.\n\u2022 Samples in D\u2032 (ns): Figure 5c shows that the per-\nformance boost saturates after the representation size\nreaches 75k data points, which means that most of the\nfrequently appearing values (v) get decent representa-\ntions (Dv) from a small number of samples. For very\nsparse values, we revert to element-wise na\u00a8\u0131ve hashing\ntrick based mapping. We also show the size of the sam-\nples in terms of the number of non-zeros integers to be\nstored. As compared to 540M parameter networks we\ntrain a sample of 125K only requires us to store 3.2M\nintegers.\n7.2\nMain Experiment\nWe compare LMA-DLRM against full embedding (embed-\nding tables used in DLRM [1]), HashedNet [13] embed-\nding (na\u00a8\u0131ve element-wise hashing trick based), and QR\nembeddings [12] across different memory budgets. Hyper-\nparameters nh=4, \u03b1=16, and ns=125, 000 were used for\nall LMA-DLRM experiments in this section. For baselines,\nwe use the con\ufb01gurations in their open source code. Train-\ning of models was cutoff at 15 epochs. We did not perform\n7\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nNumber of parameters/Million\nTest AUC\n0.796\n0.798\n0.800\n0.802\n0.804\n36\n70\n135\n270\n540\nFull\nLMA\nHashNet\nQR\nNumber of parameters/Million\nTest accuracy\n0.786\n0.787\n0.788\n0.789\n0.790\n36\n70\n135\n270\n540\nFull\nLMA\nHashNet\nQR\nNumber of parameters/Million\nTest loss\n0.450\n0.452\n0.454\n0.456\n0.458\n36\n70\n135\n270\n540\nFull\nLMA\nHashNet\nQR\nNumber of parameters/Million\nTest AUC\n0.750\n0.752\n0.754\n0.756\n0.758\n10\n19\n38\n76\n152\nFull\nLMA\nHashNet\nQR\nNumber of parameters/Million\nTest accuracy\n0.8476\n0.8478\n0.848\n0.8482\n0.8484\n10\n19\n38\n76\n152\nFull\nLMA\nHashNet\nQR\nNumber of parameters/Million\nTest loss\n0.374\n0.376\n0.378\n0.380\n10\n19\n38\n76\n152\nFull\nLMA\nHashNet\nQR\nFigure 6: AUC, Accuracy, and Loss against memory regimes (number of parameters) on Criteo (top) and Avazu\n(bottom)\nFigure 7: Evolution of test AUC with different parameters\non Criteo (left) and Avazu (right)\nextensive hyperparameter tuning. However, hyperparame-\nter tuning can achieve better results for LMA-DLRM. The\nLMA-DLRM code is attached with supplementary mate-\nrial.\nResults: Figure 6 shows AUC, accuracy, and loss against\ndifferent memory regimes (number of parameters) on both\ndatasets. Figure 7 shows the evolution of some models for\n\ufb01rst 5 epochs.\n\u2022 LMA-DLRM outperforms all baselines across different\nmemory regimes include those reported in [1], achiev-\ning average AUC improvement of 0.0032 in Criteo and\n0.0034 in Avazu across memory budgets and an average\nimprovement in Accuracy of 0.0017 on Criteo. Recall\nthat an improvement of 0.001 is signi\ufb01cant.\n\u2022 The AUC and accuracy of full embedding model with\n540M parameters can be achieved by LMA-DLRM with\nonly 36M parameters (16\u00d7 reduction) on Criteo. On\nAvazu, results with 10M parameter LMA model are\nmuch better than full embedding model with 150M pa-\nrameters (15\u00d7 reduction).\n\u2022 LMA-DLRM achieves best AUC 0.805 as opposed to\nthe best of full embedding 0.802 on Criteo. On Avazu,\nwe see improvement of 0.0025 on best AUCs as well.\n\u2022 The typical evolution of AUC metric for Criteo and\nAvazu on models of different sizes for LMA-DLRM\nand full embedding models clearly supports the better\nperformance of LMA-DLRM over full embeddings.\n8\nConclusion\nWe de\ufb01ne two problems namely SCMA (Semantically\nConstrained Memory Allocation) and Randomized SCMA\nfor ef\ufb01cient utilization of memory in Embedding tables.\nWe propose a neat LSH-based Memory Allocation (LMA)\nwhich solves the dynamic version of RSCMA under any\nmemory budget with negligible memory overhead. LMA\nwas applied to an important problem of heavy memory\ntables in widely used recommendation models and found\ntremendous success. In this paper, we focus on the memory\naspect of LMA. In future work, we would like to bench-\nmark the LMA method for its time ef\ufb01ciency for training\nand inference for recommendation models.\n8\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nReferences\n[1] Maxim\nNaumov,\nDheevatsa\nMudigere,\nHao-\nJun Michael Shi, Jianyu Huang, Narayanan Sundara-\nman, Jongsoo Park, Xiaodong Wang, Udit Gupta,\nCarole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhul-\ngakov, Andrey Mallevich, Ilia Cherniavskii, Ying-\nhai Lu, Raghuraman Krishnamoorthi, Ansha Yu,\nVolodymyr Kondratenko, Stephanie Pereira, Xianjie\nChen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong,\nand Misha Smelyanskiy. Deep learning recommenda-\ntion model for personalization and recommendation\nsystems. arXiv:1906.00091, 2019.\n[2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen,\nTal Shaked, Tushar Chandra, Hrishi Aradhye, Glen\nAnderson, Greg Corrado, Wei Chai, Mustafa Ispir,\net al.\nWide & deep learning for recommender\nsystems.\nIn Proceedings of the 1st workshop on\ndeep learning for recommender systems, pages 7\u201310,\n2016.\n[3] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. Ef\ufb01cient estimation of word representations in\nvector space. arXiv:1301.3781, 2013.\n[4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you\nneed. arXiv:1706.03762, 2017.\n[5] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang.\nDeep & cross network for ad click predictions. In\nProceedings of the ADKDD, 2017.\n[6] Antonio Ginart, Maxim Naumov, Dheevatsa Mudi-\ngere, Jiyan Yang, and James Zou. Mixed dimension\nembeddings with application to memory-ef\ufb01cient rec-\nommendation systems. arXiv:1909.11810, 2019.\n[7] Jongsoo Park, Maxim Naumov, Protonu Basu, Sum-\nmer Deng, Aravind Kalaiah, Daya Khudia, James\nLaw, Parth Malani, Andrey Malevich, Satish Na-\ndathur, et al. Deep learning inference in facebook\ndata centers: Characterization, performance opti-\nmizations and hardware implications. arXiv preprint\narXiv:1811.09886, 2018.\n[8] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu,\nand Kun Gai. Practice on long sequential user be-\nhavior modeling for click-through rate prediction. In\nProceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining,\npages 2671\u20132679, 2019.\n[9] Udit Gupta, Carole-Jean Wu, Xiaodong Wang,\nMaxim Naumov, Brandon Reagen, David Brooks,\nBradford Cottel, Kim Hazelwood, Mark Hempstead,\nBill Jia, et al. The architectural implications of face-\nbook\u2019s dnn-based personalized recommendation. In\n2020 IEEE International Symposium on High Perfor-\nmance Computer Architecture (HPCA), pages 488\u2013\n501. IEEE, 2020.\n[10] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang,\nand Xiuqiang He.\nFuxictr: An open benchmark\nfor click-through rate prediction.\narXiv preprint\narXiv:2009.05794, 2020.\n[11] Jo\u02dcao Gama, Indr\u02d9e \u02c7Zliobait\u02d9e, Albert Bifet, Mykola\nPechenizkiy, and Abdelhamid Bouchachia. A survey\non concept drift adaptation. ACM computing surveys\n(CSUR), 46(4):1\u201337, 2014.\n[12] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim\nNaumov, and Jiyan Yang. Compositional embeddings\nusing complementary partitions for memory-ef\ufb01cient\nrecommendation systems. arXiv:1909.02107, 2019.\n[13] Wenlin Chen, James Wilson, Stephen Tyree, Kilian\nWeinberger, and Yixin Chen. Compressing neural\nnetworks with the hashing trick. In International\nconference on machine learning, pages 2285\u20132294.\nPMLR, 2015.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding.\narXiv preprint arXiv:1810.04805, 2018.\n[15] Yoav Goldberg and Omer Levy. word2vec explained:\nderiving mikolov et al.\u2019s negative-sampling word-\nembedding method. arXiv preprint arXiv:1402.3722,\n2014.\n[16] Mark N. Wegman and J. Lawrence Carter. New hash\nfunctions and their use in authentication and set equal-\nity. Journal of Computer and System Sciences, 1981.\n[17] Piotr Indyk and Rajeev Motwani. Approximate near-\nest neighbors: towards removing the curse of dimen-\nsionality. In Proceedings of the thirtieth annual ACM\nsymposium on Theory of computing, pages 604\u2013613,\n1998.\n9\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nA\nLMA solves RSCMA\nUnder the GMA setup (see def 3), for any two values v1 and v2, the fraction of consistently shared memory fAL as per\nallocation AL proposed by LMA is a random variable with distribution,\nE(fAL(v1, v2)) = \u0393 = \u03c6(v1, v2) + 1 \u2212\u03c6(v1, v2)\nm\n,\nV(fAL(v1, v2)) = \u0393(1 \u2212\u0393)\nd\n,\nPr\n\u0012\n|fAL(v1, v2)\u2212\u03c6(v1, v2)| > \u03b7\u0393 + 1\u2212\u03c6(v1, v2)\nm\n\u0013\n\u22642 exp\n\u001a\u2212d\u0393\u03b72\n3\n\u001b\n,\nfor all \u03b7 > 0. Hence, LMA solves RSCMA with \u03f5 = \u03b7\u0393 + 1\u2212\u03c6(v1,v2)\nm\nand \u03b4 = 2 exp\nn\n\u2212d\u0393\u03b72\n3\no\n.\nProof sketch: Proof consists of analyzing the random variable for the fraction fAL(v1, v2) and applying Chernoff\nconcentration inequality to obtain the tail bounds.\nProof\nThe probability that a particular location is shared is exactly the probability of collision of the rehashed lsh function lr.\n(using the notation from section 4)\nPr(lr(x) == lr(y)) = \u03c6(x, y) + (1 \u2212\u03c6(x, y))\nm\n(1)\nWe can write the indicator for fraction of consistently shared memory as,\n\u02c6\nfAL =\nd\nX\ni=1\nI(l(i)\nr (x) == l(i)\nr (y))\n(2)\nfAL is a sum of independent bernoulli varaibles. It is easy to check that the expected value of consistently shared\nfraction is,\nE( \u02c6\nfAL) = \u0393 = \u03c6(x, y) + (1 \u2212\u03c6(x, y))\nm\n(3)\nV ( \u02c6\nfAL) = \u0393(1 \u2212\u0393)\nd\n(4)\nWe can apply the chernoff\u2019s bound to get tail bound. Let \u03b7 > 0 be any positive real number\nPr (|fAL(v1, v2)\u2212\u0393| > \u03b7\u0393) \u22642 exp\n\u001a\u2212d\u0393\u03b72\n3\n\u001b\n(5)\nPr\n\u0012\n|fAL(v1, v2)\u2212\u03c6(v1, v2)| > \u03b7\u0393 + 1\u2212\u03c6(v1, v2)\nm\n\u0013\n\u22642 exp\n\u001a\u2212d\u0393\u03b72\n3\n\u001b\n(6)\nB\nExistence of M with LMA for S\u2217\nUnder the GMA setup (see Def. 3), let us initialize each element of M independently from a Bernoulli(0.5, {\u22121, +1}).\nThen, the embedding table E generated via LMA on this memory, has , for every pair of values v1 and v2, the cosine\nsimilarity Cs(E[v1, :], E[v2, :]), denoted by Cs(v1, v2) is distributed as\nE(Cs(v1, v2)) = \u0393 = \u03c6(v1, v2) + 1 \u2212\u03c6(v1, v2)\nm\n,\nV(Cs(v1, v2)) = 1 \u2212\u03932\nd\n+ 2(1 \u2212\u0393)(d \u22121)\ndm2\n,\nPr\n\u0012\n|Cs(v1, v2) \u2212\u03c6(v1, v2)| \u2265\u03b7\u0393 + 1 \u2212\u03c6(v1, v2)\nm\n\u0013\n\u22641 \u2212\u03932\nd\u03b72\u03932 for any \u03b7 > 0.\n10\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nProof sketch: Proof consists of analyzing the random variable for the cosine similarity Cs(E[v1, :], E[v2, :]) and\napplying Chebyshev\u2019s concentration inequality to obtain the tail bounds.\nConsider a memory M of size m initialized randomly by a independent draws of a Bernoulli random variable from\n{\u22121, +1} with probability 0.5. Then let a be denote the value at any aribitraty memory location in M Note, for each\nk \u2208N\nE(a2k) = 1\nE(a2k+1) = 0\n(7)\nThe norm of any embedding of dimension d drawn from this M is\n\u221a\nd. Let us look at the inner product\n\\\n\u27e8Ex, Ey\u27e9= \u03a3d\ni=1{I(li(x) == li(y))M[li(x)]2 + I(li(x)! = li(y))M[li(x)]M[li(y)]}\n(8)\nLet \u0393 = \u03c6(x, y) + 1\u2212\u03c6(x,y)\nm\nE( \\\n\u27e8Ex, Ey\u27e9) = d\u0393\n(9)\nE(\n\\\nCosine(Ex, Ey)) = \u0393\n(10)\nAnalysis of Variance is a bit involved due to interdependence between each of the terms in the summation above that\ncomes about due to random collisions caused by rehashing. We analyse both the cases. The case of interdependence\njust precipitates to independent case for reasonable M.\nCase 1: Assume independence\nV( \\\n\u27e8Ex, Ey\u27e9) = d(V(I(li(x) == li(y)) + I(li(x)! = li(y))M[li(x)]M[li(y)]))\n(11)\nV( \\\n\u27e8Ex, Ey\u27e9) = d(V(I(li(x) == li(y))) + V(I(li(x)! = li(y))M[li(x)]M[li(y)])))\n(12)\nV( \\\n\u27e8Ex, Ey\u27e9) = d(\u0393(1 \u2212\u0393) + (1 \u2212\u0393))\n(13)\nV( \\\n\u27e8Ex, Ey\u27e9) = d(1 + \u0393)(1 \u2212\u0393)\n(14)\nV(Cosine(\u27e8Ex, Ey\u27e9)) = 1\nd(1 + \u0393)(1 \u2212\u0393)\n(15)\nCase 2: Don\u2019t assume independence.\n\\\nCosine(Ex, Ey) = 1\nd\n\\\n\u27e8Ex, Ey\u27e9\n(16)\n\\\ncosine(Ex, Ey) = 1\nd\u03a3d\ni=1{I(li(x) == li(y))M[li(x)]2 + I(li(x)! = li(y))M[li(x)]M[li(y)]}\n(17)\nE(\n\\\nCosine(Ex, Ey) = \u0393\n(18)\nE(\n\\\n(Cosine(Ex, Ey)\n2\n) = 1\nd2 E(\n\\\n(\u27e8Ex, Ey\u27e9)\n2\n)\n(19)\nLet exi = M[li(x)]\nE(\n\\\n(Cosine(Ex, Ey)\n2\n) = 1\nd2 E(\u03a3exieyi)(\u03a3exieyi)\n(20)\nE(\n\\\n(Cosine(Ex, Ey)\n2\n) = 1\nd2 E(\u03a3d\ni=1(exieyi)2 + \u03a3i\u0338=j(exieyi)(exjeyj)\n(21)\nE(\n\\\n(Cosine(Ex, Ey)\n2\n) = 1\nd(E((exieyi)2) + (d \u22121)E(exieyi)(exjeyj)\n(22)\nE(\n\\\n(Cosine(ex, ey)\n2\n) = 1\nd(1 + (d \u22121)EM[li(x)]M[lj(x)]M[li(y)]M[lj(x)])\n(23)\n11\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nli(x)\nli(y)\nlj(x)\nlj(y)\nprobability\na\na\nb\nb\npc\u02c62\na\nb\na\nb\n(1-pc) / m\u02c62\na\nb\nb\na\n(1 - pc) / m\u02c62\nTable 2: Table for reference on cases\nUsing table 2,\nE(\n\\\n(Cosine(ex, ey)\n2\n) = 1\nd(1 + (d \u22121)(\u03932 + 2(1 \u2212\u0393)\nm2\n))\n(24)\nV ar(\n\\\n(Cosine(Ex, Ey)) = E(\n\\\n(Cosine(Ex, Ey)\n2\n) \u2212E(\n\\\n(Cosine(Ex, Ey))2\n(25)\nV ar(\n\\\n(Cosine(ex, ey)) = 1\nd(1 + (d \u22121)(\u03932 + 2(1 \u2212\u0393)\nm2\n)) \u2212\u03932\n(26)\nCollecting terms\nV ar(\n\\\n(Cosine(ex, ey)) = 1\nd{(1 \u221221 \u2212\u0393\nm2\n\u2212\u03932) + 2d(1 \u2212\u0393\nm2 )}\n(27)\nV ar(\n\\\n(Cosine(ex, ey)) = 1 \u2212\u03932\nd\n+ 2(1 \u2212\u0393)(d \u22121)\ndm2\n\u22481\nd(1 \u2212\u03932)\n(28)\nC\nProcedures requires small data sample\nAssume that the real dataset is of size N, the sparsity of each feature value is s. i.e. the probability of a feature appearing\nin an example is s. For simplicity, let us assume that each feature value has the same sparsity. This may not be generally\ntrue. But nonetheless it helps us draw an idea of data sample needed. Let us consider the situation where the two\nfeatures have jaccard similarity J. The venn diagram below shows the distribution of samples in our case.\nFigure 8: Venn diagram for two features f1,f2\nLet the events be as follows:\n\u2022 Ai : ith sample has feature f1\n\u2022 Bi : ith sample has feature f2\nThe estimated jaccard similarity after drawing n random i.i.d samples would be,\n\u02c6J = \u03a3n\ni=1I(Ai \u2227Bi)\n\u03a3n\ni=1I(Ai \u2227Bi\n= X\nY\n(29)\n12\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nLet X and Y and C be the following\nC = (1 + J)\n2ns\n(30)\nX = C\u03a3n\ni=1I(Ai \u2229Bi)\n(31)\nY = C\u03a3n\ni=1I(Ai \u222aBi)\n(32)\nE(X) = Cn 2sJ\n1 + J =\u21d2E(X) = (1 + J)\n2ns\nn 2sJ\n1 + J = J\n(33)\nE(Y ) = Cn\n2s\n1 + J =\u21d2E(Y ) = (1 + J)\n2ns\nn\n2s\n1 + J = 1\n(34)\nV ar(X) = C2n 2sJ\n1 + J (1 \u22122sJ\n1 + J )\n(35)\nV ar(Y ) = C2n\n2s\n1 + J (1 \u2212\n2s\n1 + J )\n(36)\nLet us look at Variance of X\nV ar(X) = (1 + J)2\n4n2s2 n 2sJ\n1 + J (1 \u22122sJ\n1 + J )\n(37)\nV ar(X) =\nJ\n2ns(1 + J \u22122sJ)\n(38)\nLet us look at Variance of (Y)\nV ar(Y ) = (1 + J)2\n4n2s2 n\n2s\n1 + J (1 \u2212\n2s\n1 + J )\n(39)\nV ar(Y ) =\n1\n2ns(1 + J \u22122s)\n(40)\n(41)\nUsing Chebysev\u2019s inequality ,\nP(|Y \u22121| \u2265\u03f5) < V ar(Y )\n\u03f52\n(42)\nHence, 1 \u2212\u03f5 \u2264Y \u22641 + \u03f5 with probability 1 \u2212\u03b4 where \u03b4 = 1+J\u22122s\n2ns\u03f52\nHence we can write, with probability 1 \u2212\u03b4\nX\n1 + \u03f5 \u2264\u02c6J \u2264\nX\n1 \u2212\u03f5\n(43)\nHence, with probability 1 \u2212\u03b4\nJ\n1 + \u03f5 \u2264E( \u02c6J) \u2264\nJ\n1 \u2212\u03f5\n(44)\nX2\n(1 + \u03f5)2 \u2264\u02c6J2 \u2264\nX2\n(1 \u2212\u03f5)2\n(45)\nE(X2)\n(1 + \u03f5)2 \u2264E( \u02c6J2) \u2264E(X)2\n(1 \u2212\u03f5)2\n(46)\nV ar(X) + E(X)2\n(1 + \u03f5)2\n\u2264E( \u02c6J2) \u2264V ar(X) + E(X)2\n(1 \u2212\u03f5)2\n(47)\nJ2\n(1 + \u03f5)2 \u2264E( \u02c6J)2 \u2264\nJ2\n(1 \u2212\u03f5)2\n(48)\nV ar(X) + E(X)2\n(1 + \u03f5)2\n\u2212\nJ2\n(1 \u2212\u03f5)2 \u2264E( \u02c6J2) \u2212E( \u02c6J)2 \u2264V ar(X) + E(X)2\n(1 \u2212\u03f5)2\n\u2212\nJ2\n(1 + \u03f5)2\n(49)\n13\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Ef\ufb01cient Recommendation Systems\nV ar(X) + E(X)2 =\nJ\n2ns(1 + J \u22122sJ + 2nsJ)\n(50)\nThe above is the actual result. However for simplicity we simplify assuming \u03f5 is small.\nLet\nA = V ar(X) + E(X)2 \u2212J2\n(51)\nB = V ar(X) + E(X)2 + J2\n(52)\nA =\nJ\n2ns(1 + J \u22122sJ + 2nsJ) \u2212J2\n(53)\nB =\nJ\n2ns(1 + J \u22122sJ + 2nsJ) + J2\n(54)\nA =\nJ\n2ns(1 + J \u22122sJ)\n(55)\nB = A + 2J2\n(56)\nA \u22122\u03f5(A + 2J2) \u2264V ar( \u02c6J) \u2264A + 2\u03f5(A + 2J2)\n(57)\n(1 \u22122\u03f5)A \u22124\u03f5J2) \u2264V ar( \u02c6J) \u2264(1 + 2\u03f5)A + 4\u03f5J2)\n(58)\nAlso under small \u03f5\nJ(1 \u2212\u03f5) \u2264E( \u02c6J) \u2264J(1 + \u03f5)\n(59)\n14\n",
    "2006.05623": "Training with Multi-Layer\nEmbeddings for Model Reduction\nBenjamin Ghaemmaghami 1, Zihao Deng 1, Benjamin Cho 1, Leo Orshansky 2,\nAshish Kumar Singh 3, Mattan Erez 1, and Michael Orshansky 1\n1 Department of Electrical and Computer Engineering, University of Texas at Austin\n2 Department of Computer Science, University of Texas at Austin\n3 E2OPEN, India\nAbstract\nModern recommendation systems rely on real-valued embeddings of categorical\nfeatures. Increasing the dimension of embedding vectors improves model accuracy\nbut comes at a high cost to model size. We introduce a multi-layer embedding\ntraining (MLET) architecture that trains embeddings via a sequence of linear layers\nto derive superior embedding accuracy vs. model size trade-off.\nOur approach is fundamentally based on the ability of factorized linear layers to\nproduce superior embeddings to that of a single linear layer. We focus on the\nanalysis and implementation of a two-layer scheme. Harnessing the recent results\nin dynamics of backpropagation in linear neural networks, we explain the ability to\nget superior multi-layer embeddings via their tendency to have lower effective rank.\nWe show that substantial advantages are obtained in the regime where the width of\nthe hidden layer is much larger than that of the \ufb01nal embedding (d). Crucially, at\nconclusion of training, we convert the two-layer solution into a single-layer one: as\na result, the inference-time model size scales as d.\nWe prototype the MLET scheme within Facebook\u2019s PyTorch-based open-source\nDeep Learning Recommendation Model. We show that it allows reducing d by\n4-8X, with a corresponding improvement in memory footprint, at given model\naccuracy. The experiments are run on two publicly available click-through-rate\nprediction benchmarks (Criteo-Kaggle and Avazu). The runtime cost of MLET is\n25%, on average.\n1\nIntroduction\nRecommendation models (RMs) underlie a large number of applications and improving their perfor-\nmance is increasingly important. The click-through-rate (CTR) prediction task is a special case of\ngeneral recommendation that seeks to predict the probability of a user clicking on a speci\ufb01c item,\ne.g. an ad, given the history of the user\u2019s past reactions. The user reactions and earlier-encountered\ninstances are used in training the CTR model and are described by multiple features that capture user\ninformation (e.g., age, gender) and item information (e.g., movie title, cost) [22]. Features are either\nnumerical or categorical variables.\nA categorical variable with n possible values can be represented by an n-dimensional one-hot\nvector. However, a fundamental aspect of modern recommendation models is their reliance on\nembeddings which map categorical variables into dense representations in an abstract real-valued\nspace. Embeddings are superior for two main reasons. The \ufb01rst is that they allow a compacted\nrepresentation compared to high-dimensional sparse one-hot, or multi-hot, direct encodings of\ncategorical data. The second is that dense embedding vectors represent meaningful information that is\nexploited by RMs for improved performance: the angle (dot-product) between two embedding vectors\narXiv:2006.05623v1  [cs.LG]  10 Jun 2020\nrepresents their semantic similarity. Following a seminal innovation of Factorization Machines [24],\nmany modern RMs exploit this by using dot-products between embedding vectors to de\ufb01ne the\nstrength of feature interactions.\nState-of-the-art RMs increasingly rely on deep neural networks. Most high-performing models use a\ncombination of multi-layer perceptrons (MLPs) to process dense features, linear layers to generate\nembeddings of categorical features, and sub-networks that generate higher-order interactions. The\noutputs of the interaction sub-networks and MLPs are used as inputs into a linear (logistic) model with\na sigmoid activation to produce the CTR prediction. Broadly, the above describes the architectures of\nWide and Deep [8], Deep and Cross [28], DeepFM [13], Field-Aware Factorization Machine (FFM)\n[14], and xDeepFM [16] networks, among others. The differences between the models are largely in\nhow they handle the higher-order feature interactions. The Deep Learning Recommendation Model\n(DLRM) [21], that we use for prototyping our technique, is structurally similar to other models.\nDLRM does not include higher-order interactions, judging that their computational and memory\ncost is not justi\ufb01ed. This is supported by empirical results on public datasets that show DLRM\noutperforming models with explicit higher-order interactions, such as the Deep and Cross model [28].\nAll DNN-based RMs described above derive embeddings as part of model training through backprop-\nagation. Algorithmically, embeddings are implemented as linear layers: if a categorical feature in\none-hot encoding is a vector q \u2208Z1\u00d7n, then the embedding lookup is a vector-matrix multiplication\nqW. Here, W \u2208Rn\u00d7d is the embedding table (matrix) whose ith row represents the embedding\nof the ith category in a d-dimensional vector space. Conventionally, W is implemented as a single\nlinear layer and jointly trained with the rest of the model to minimize the loss on the CTR task.\nThough embeddings are a more ef\ufb01cient representation of features compared to one-hot categorical\nvectors, the embedding tables still impose an increasingly heavy cost in system deployments, with\ntables commonly requiring tens of gigabytes of space [9]. The reason is the large value of n: it is not\nuncommon to encounter a single categorical feature with millions of distinct values. For example, in\nthe public Avazu dataset, one categorical feature has 6.7 million values.\nThere are many techniques aiming to reduce the memory requirements of embedding tables - some\nunique to the embedding layer setting and others general. Compression-based techniques operate\non trained layers and use pruning and quantization to reduce table size [17, 27, 25]. Low-rank\napproximation via SVD is another example of post-training compression [6]. Other techniques\nperform pruning or quantization during training [2, 20]. While the above group of methods does not\ninvolve modifying the structure of the model, other methods, such as hashing and tensor factorization\n[5, 15], achieve superior quality-size behavior through a modi\ufb01ed model structure that results in\nbetter use of model parameters. Using the unique properties of RMs, in [10], a mixed-dimension\nstrategy uses statistical patterns (frequency) of accessing individual entries to embed the popular\nentries into vectors of higher dimension compared to the less popular entries.\nFigure 1: CTR model accuracy vs. the embedding dimension based on a single-layer embedding.\nThe trade-off curve is generated using DLRM on the Criteo-Kaggle dataset.\n2\nEmbedding vector dimension d is a critical factor that controls the table size as well as model\nperformance. Both empirical and theoretical evidence suggests that there exists a fundamental\ntrade-off under which reducing vector dimension d leads to the loss in model performance [19, 29].\nThe trade-off is illustrated in Figure 1 using DLRM on the Criteo-Kaggle dataset. The contribution of\nthis paper is in developing a novel way of deriving a superior model size-accuracy trade-off.\n1.1\nOur Contribution: Multi-Layer Embedding Training\nWe propose a novel way of achieving a smaller model size without accuracy degradation. The\ntechnique, which we call a multi-layer embedding training (MLET) architecture, trains embeddings\nvia a sequence of linear layers, instead of a single layer.\nThe fundamental underpinning for the superior behavior of MLET is the dynamics of training using\nbackpropagation. Harnessing recent results in the training of deep factorized linear neural networks,\nwe provide a theoretical explanation for the surprising fact that multi-layer embeddings lead to a\nsuperior size-accuracy trade-off. We show that the main reason for the superior behavior is the\nimpact of factorization on the generalization ability of the model, which is produced by the model\u2019s\nconvergence towards a less complex solution.\nWe focus on a prototype implementation that employs two linear layers. The inner dimension between\nthe two layers is k. The second layer\u2019s output dimension is equal to the target embedding dimension\nd. We \ufb01nd empirically that the effectiveness of the two-layer embedding technique depends heavily\non the ratio k/d for any given target embedding dimension d. The most bene\ufb01t occurs when k/d > 4.\nThe main cost of MLET is a k/d increase in the required memory capacity during training (compared\nto a conventional embeddings training with dimension d).\nIt would appear that MLET increases the number of model parameters signi\ufb01cantly, with the size of\nthe embedding table increased by a factor of k/d. However, a two-layered approach is essential only\nduring training. We eliminate the inference-time memory and model storage cost of MLET by a a\npost-training layer transformation that collapses the multiple linear layers into a single one. As a\nresult, for inference, only the original embedding table of size n \u00d7 d is stored.\nWe implement the proposed algorithmic framework in PyTorch using DLRM. We demonstrate\nsubstantial bene\ufb01ts of MLET in terms of model size reduction of 4-8X, at constant accuracy on two\npublic CTR datasets Avazu and Criteo-Kaggle. We \ufb01nd that the runtime cost of MLET is about 25%.\n2\nSuperior Embedding Size-Accuracy Trade-off via Multi-Layer\nEmbedding Training\n2.1\nMulti-Layer Embeddings: De\ufb01nitions\nWe now introduce the notation and details of MLET. Let the \ufb01nal embedding table be W of size\nn \u00d7 d, where n is the number of elements in the table and d is the embedding dimension.\nW \u2208Rn\u00d7d\n(1)\nWe focus on a two-layer architecture and seek to factorize the embedding table W in terms of W1\nand W2:\nW = W1W2\n(2)\nW1 \u2208Rn\u00d7k\n(3)\nW2 \u2208Rk\u00d7d\n(4)\nLet the row vector q \u2208Z1\u00d7n denote a one-hot encoding of a feature with n categories. The\nembedding lookup is represented by a vector-matrix product:\nr = qW1W2\n(5)\n3\nHere, r \u2208R1\u00d7d is the embedding of q in a d-dimensional space. W1 and W2 are trained jointly. After\ntraining there is no need to keep both W1 and W2, and we only store their product, W = W1W2.\nThis reduces a two-layer embedding into a single one for inference-time evaluation and storage.\nThe essential aspect of MLET\u2019s training of an embedding using a sequence of two linear layers are\nthe relative dimensions of W1 and W2. As de\ufb01ned above, W1 and W2 are of shape n \u00d7 k and k \u00d7 d,\nrespectively. We say that a model with a linear layer n \u00d7 d1 dominates (\u00bb) another linear model with\na linear layer n \u00d7 d2 if the validation loss on the \ufb01rst model is lower than that of the second model.\nSymbolically, (n \u00d7 d1) \u00bb (n \u00d7 d2) if Loss (n \u00d7 d1) < Loss (n \u00d7 d2).\nFor a single-layer model, the accuracy-size trade-off discussed earlier, and shown in Figure 1, can be\nrestated as follows: (n \u00d7 d1) \u00bb (n \u00d7 d2) if d1 > d2. Similarly, as a consequence of the same trade-off,\nit seems self-evident that for a two-layer linear model, the following holds: (n \u00d7 k) \u00d7 (k \u00d7 d1) \u00bb\n(n \u00d7 k) \u00d7 (k \u00d7 d2) if d1 > d2 (this is also con\ufb01rmed empirically, and can be seen in Figures 2 and 3).\nYet there are two aspects of MLET that seem quite surprising. The \ufb01rst is why a two-layer embedding\nis superior to a one-layer embedding, or compactly, why (n \u00d7 k) \u00d7 (k \u00d7 d) \u00bb (n \u00d7 d)? The second\nis why a two-layer model improves with a larger width of the hidden layer k, or compactly, why\n(n\u00d7k1)\u00d7(k1 \u00d7d) \u00bb (n\u00d7k2)\u00d7(k2 \u00d7d) if k1 > k2? In the next section we explain the \ufb01rst behavior\n- the effect of factorization per se. We currently attribute the second behavior to the general tendency\nof overparameterized linear neural networks to positively depend on the width of hidden layers. We\nplan to explore this aspect of MLET more thoroughly in our future work.\n2.2\nWhy Does Factorization Help?\nWhy should we expect to get a better embedding if we factorize the linear layer? Speci\ufb01cally, as we\ndemonstrate in Section 2.2.2, in the MLET operating regime of k \u2265d, any embedding de\ufb01ned by a\ntwo-layer model lies in the search space of a single-layer model. Therefore, if there is an optimal\nsolution found by a two-layer model, our intuition is that a single-layer model should also be able to\n\ufb01nd it, and, thus, a two-layer model should not be better than a single-layer model. Yet, empirically,\nwe \ufb01nd that two-layer models consistently outperform their single-layer counterparts.\nTo understand why factorization helps, we rely on recent results in the dynamics of training linear\nlayers using backpropagation. The main reason for the superior behavior of the multi-layer model\ntraining is the impact of factorization on the generalization ability of the model. It achieves this by\nconvergence towards a less complex solution.\n2.2.1\nDynamics of Factorized Linear Layer Network Training\nIt is a widely accepted notion in deep learning that low-rank weight matrices lead to better general-\nization and help avoid over\ufb01tting [4]. Practically, regularization on rank is a common and powerful\napproach to restrict model complexity and thus enhance generalization. A variety of ML algorithms\nuse regularization on rank to achieve better generalization, including robust principal component\nanalysis [11][26], robust matrix completion [7], subspace clustering [23][18], and others [12].\nRecent work [3] has shown that a linear layer network (LLN) with multiple layers has a strong bias\ntowards learning a low-rank weight matrix. The fundamental reason behind this is that the process of\ntraining an LLN with the gradient descent algorithm results in larger polarization of the singular\nvalues of the learned matrix in LLNs with more layers. The result is that large singular values are\nampli\ufb01ed while small ones are attenuated and tend to vanish.\nConsider an N-layer LLN with a weight matrix of each layer being Wi. Let W be the weight matrix\nthat represents the LLN in a single layer form, i.e., W = W1 \u00d7 W2... \u00d7 WN. Let \u03c3r denote the rth\nsingular value of W. Let ur and vr be the rth left and right singular vectors of W, respectively. Let\nthe loss function be L and \u2207L(W(t)) be its gradient with respect to W at time t. Given a learning\nrate \u03b7, the updates of the singular values are given by Eq. 6 (Theorem 3 in [3]):\n\u03c3r(t + 1) \u2190\u03c3r(t) \u2212\u03b7 \u00b7 N \u00b7 (\u03c3r(t))2\u22122/N \u00b7\n\n\u2207L(W(t)), ur(t)v\u22a4\nr (t)\n\u000b\n(6)\nCritically, the term (\u03c3r(t))2\u22122/N captures the dependence on the number of layers. For a single-layer\nmodel (N = 1), the term reduces to 1 for all r, making the update to \u03c3r(t) independent of its current\nvalue. However, for a multi-layer LLN, the update term grows, linearly or faster, with the current\nvalue of \u03c3r(t). For \u03c3r(t) < 1 , the term (\u03c3r(t))2\u22122/N is strictly less than 1 and gets smaller for\n4\nsmaller \u03c3r(t). Therefore, a multi-layer LLN attenuates the updates for small singular values. By the\nsame reasoning, a multi-layer LLN enhances the updates for large singular values. As N increases,\nthe gap between larger and smaller singular values increases, resulting in W having lower rank.\nWe directly observed the bias towards the low-rank weight matrices by analyzing the distribution\nof singular values of the embedding matrices in our MLET experiments. The Avazu dataset has\n21 categorical features but two of them have far more items than the rest: feature-9 and feature-10\nare jointly responsible for 99.7% of all embedding table entries. Now consider 8 singular values of\nembeddings learned using a single-layer model with d = 8 and those from the MLET model with\nk = 64 and d = 8. For feature-9, all 8 singular values of the single-layer model are larger than 0.01\nof its largest singular value. However, only 2 singular values of the embedding produced by MLET\nare larger than 0.01 of the largest one. Similarly, for feature-10, 5 singular values of the single-layer\nmodel are larger than 0.01 of its largest singular value but only 2 singular values of the MLET model\nare larger than 0.01 of the largest one.\nWe use the above tendency of factorized linear layers to produce a lower-rank W to derive superior\ncategory embeddings by replacing a single-layer embedding with a multi-layer embedding. In the\nexperiments we describe in Section 4, we \ufb01nd that using N = 2 is suf\ufb01cient and using higher N is\nnot helpful.\n2.2.2\nDimensional Constraints in Multi-Layer Embeddings\nIn MLET, the dimensions of the layers are important. For concreteness, we focus on a two-layer\nembeddings (N = 2) and explain why MLET requires k \u2265d. The theory of rank regularization\ndoes not make any assumptions about the relation between k and d. The reason for imposing the\nconstraint that k \u2265d is to ensure that the search space of a two-layer model and that of its single-layer\ncounterpart are identical. With this condition satis\ufb01ed, the tendency of a multi-layer model towards a\nlow-rank solution leads to superior generalization. When k < d, however, the search space of the\nmulti-layer model is reduced to a subset of a single-layer model\u2019s search space. This counteracts\nthe bene\ufb01ts of a lower-rank solution with no guaranteed improvement in generalization. We do not\npropose to operate in this regime.\nAgain, let the two matrices in the two-layer model be W1 \u2208Rn\u00d7k and W2 \u2208Rk\u00d7d. Let the matrix\nin the single-layer model be W \u2208Rn\u00d7d. The search space of a single-layer model is the set of\nlinear transformations de\ufb01ned by all possible matrices W: {W|W \u2208Rn\u00d7d}. The search space of a\ntwo-layer model is then the set of linear transformations de\ufb01ned by all possible products of W1W2:\n{W|W = W1W2, W1 \u2208Rn\u00d7k, W2 \u2208Rk\u00d7d}.\nFirst, we formally prove that for k \u2265d, the search space of a two-layer model is the same as that of a\nsingle-layer model. Then, we prove that for k < d, the search space of a two-layer model is reduced\nto a subset of the search space of a single-layer model.\nTheorem 2.1. The search space of a two-layer model W1W2 is the same as that of a single-layer\nmodel W when k \u2265d: {W|W \u2208Rn\u00d7d} = {W|W = W1W2, W1 \u2208Rn\u00d7k, W2 \u2208Rk\u00d7d}.\nProof. We \ufb01rst show that {W|W \u2208Rn\u00d7d} \u2286{W|W = W1W2, W1 \u2208Rn\u00d7k, W2 \u2208Rk\u00d7d} by\nshowing that for \u2200W \u2208Rn\u00d7d, \u2203W1 \u2208Rn\u00d7k and W2 \u2208Rk\u00d7d, such that W = W1W2.\nConsider the QR decomposition of W T :\nW T = QR\nQ \u2208Rd\u00d7d\nR \u2208Rd\u00d7n\n(7)\nLet Idk denote the matrix constructed by concatenating (k \u2212d) zero columns to a d \u00d7 d identity\nmatrix:\nW1 = RT Idk\nW2 = IT\ndkQT\n(8)\nIt follows that W1 \u2208Rn\u00d7k, W2 \u2208Rk\u00d7d and W = W1W2.\n5\nWe now show that {W|W = W1W2, W1 \u2208Rn\u00d7k, W2 \u2208Rk\u00d7d} \u2286{W|W \u2208Rn\u00d7d}. This follows\nfrom the fact that any matrix represented by a product of W1W2 can be represented by a single matrix\nW by simply letting W = W1W2.\nTheorem 2.2. The search space of a two-layer model W1W2 is reduced compared to that of a single-\nlayer model W when k < d: {W|W \u2208Rn\u00d7d} \u2283{W|W = W1W2, W1 \u2208Rn\u00d7k, W2 \u2208Rk\u00d7d}.\nProof. Let W be such that Rank(W) > k. Clearly, for both W1 and W2, Rank(W1) \u2264k and\nRank(W2) \u2264k. Further, Rank(W1W2) \u2264min(Rank(W1), Rank(W2)) = k. Thus, there does not\nexist W1, W2 for which W = W1W2.\n3\nExperiments\nWe evaluate the proposed algorithm on two public datasets for click-through rate tasks: Criteo-Kaggle\nand Avazu. Both datasets are composed of a mix of categorical and real-valued features (Table 1).\nBoth datasets are split into training, testing, and validation sets of 80%, 10%, and 10%, respectively.\nThe Criteo-Kaggle dataset was split based on the time of data collection: the \ufb01rst six days are used\nfor training and the seventh day is split evenly into the test and validation sets. The Avazu dataset was\nsplit randomly. The models are implemented in PyTorch. The experiments are run on two systems:\n(a) an Intel i7-9700 CPU hosting an NVIDIA RTX2080 GPU with 8GB GDDR, and (b) an Intel\ni7-8700 CPU hosting an NVIDIA Titan Xp GPU with 12GB GDDR. All experiments were run on\nthe GPUs except for the Criteo-Kaggle dataset with k \u2265128. In that case, the required memory\nexceeded 12GB and the experiments were performed on the CPU of system (b). CPU throughput is\n2 to 3 times lower compared to that of a GPU depending on the con\ufb01guration. For consistency, all\nruntime estimates are produced from experiments run on NVIDIA RTX2080.\nTable 1: Dataset Composition\nDataset\nTotal Records\nDense Features\nCategorical Features\nCriteo-Kaggle\n45,840,617\n13\n26\nAvazu\n40,400,000\n1\n21\nDLRM has several hyperparameters. For both datasets we con\ufb01gure DLRM\u2019s top MLP to have\ntwo hidden layers with 512 and 256 nodes. For the Avazu dataset, we set DLRM\u2019s bottom MLP\nto be 256 \u2192128 \u2192d. For the Criteo-Kaggle dataset, we con\ufb01gure DLRM\u2019s bottom MLP to be\n512 \u2192256 \u2192128 \u2192d. The bottom MLPs differ because their role is to handle the real-valued\nfeatures which vary between datasets. In all experiments, d is set equal to the embedding dimension\nso that vector sizes for the real-valued and categorical features match.\nFollowing prior work [21], we train the models only for a single epoch, with a universal learning rate\nof 0.2 and a batch of 128 using SGD as the optimizer. The linear factorization layers are initialized\nusing a Gaussian distribution \u223cN(0, 0.0625). The initialization is unique for each embedding table.\nFor each hyperparameter con\ufb01guration, at least \ufb01ve training runs are performed to decrease the impact\nof initialization variation and run-to-run variation due to non-deterministic GPU execution. We \ufb01nd\nthat the initial state of the embedding tables has a non-negligible impact on overall model performance\nafter training. Additionally, even with the same initial conditions, we observe run-to-run variations\nin the resulting model performance when using a GPU. We ascribe such run-to-run variation to the\ndocumented non-determinism of the CUDA implementation of some PyTorch operators, such as\nEmbeddingBag [1]. The reported data is based on the mean values of the replicated runs. We report\ntwo performance metrics: area under the ROC curve (AUC) and binary cross-entropy (LogLoss).\nRecall that d de\ufb01nes the size of the inference-time embedding vectors (and, the table) while k refers\nto the width of the hidden linear layer in MLET.\nThe experiments demonstrate the effectiveness of MLET in producing superior model size vs.\nperformance trade-offs compared to the baseline single-layer embeddings implementation using\nDLRM. Figures 2\u20133 summarize the results.\n6\nAs originally intended, the predominant system bene\ufb01t made possible by the superior model size vs.\nperformance curves is in terms of reducing the embedding table size. As Figures 2\u20133 show, at the\nsame model accuracy levels, we are able to produce models with d (and, therefore the embedding\ntable size) 4-8 times smaller compared to the baseline DLRM.\nThe bene\ufb01ts begin to be observed in MLET curves even for k = d. Increasing k for a given d leads\nto a monotonic improvement in model accuracy. For CTR systems, an improvement of 0.001 in\nLogLoss is considered substantial. The maximum LogLoss bene\ufb01t of MLET for Criteo-Kaggle is\n0.0025, and the maximum bene\ufb01t for Avazu is 0.006. This improvement in model accuracy saturates\nas k grows, e.g., for the Criteo-Kaggle dataset the curves with k = 64 and k = 128 are very similar.\nWe further observe that the relative performance improvements are largely de\ufb01ned by k/d. Recall\nthat MLET results in training memory increase of k/d compared to a single-layer training algorithm.\nSince in practice we need to operate under a certain memory budget, there is a limit to the achievable\nk/d. Speci\ufb01cally, the higher values of k/d can be achieved for smaller d, thus our technique is most\neffective at lower d.\nSo far, we analyzed either the table size reduction at a given accuracy, or accuracy improvement at a\ngiven table size, with training memory requirement being the cost (in the sense that achieving both\nbene\ufb01ts requires k > d). Interestingly, we \ufb01nd there are also k and d combinations in which both\naccuracy and size can be improved at, effectively, zero cost in terms of larger training-time memory.\nWe say that a solution has zero cost if k \u2264d. Symbolically, we can write that an MLET solution (k, d)\ndominates (\u00bb) a single-layer training solution (d) if both accuracy and table size are improved. In Fig.\n2b, we see that several points exhibit such behavior in the Avazu dataset experiments: (64,64) \u00bb 128,\n(64,32) \u00bb 128, (64,16) \u00bb 128, (32,16) \u00bb 32, and (32,8) \u00bb 32. Such behavior appears dataset-dependent\nsince we do not \ufb01nd such cost-free solutions for the Criteo-Kaggle dataset.\n(a)\n(b)\nFigure 2: Validation Area Under the Curve (AUC) for Criteo-Kaggle and Avazu datasets.\n(a)\n(b)\nFigure 3: Validation LogLoss for Criteo-Kaggle and Avazu datasets.\n7\nTable 2: DLRM and MLET training runtime on RTX2080\nMODEL\nk\nd\nms/iteration\nDLRM\n4\n4\n6.565\nDLRM\n8\n8\n6.548\nDLRM\n16\n16\n6.523\nDLRM\n32\n32\n6.530\nDLRM\n64\n64\n6.545\nMLET\n8\n4\n8.140\nMLET\n16\n4\n8.141\nMLET\n32\n4\n8.314\nMLET\n64\n4\n8.167\nMLET\n8\n8\n8.118\nMLET\n16\n8\n8.083\nMLET\n32\n8\n8.175\nMLET\n64\n8\n8.156\nMLET\n16\n16\n8.088\nMLET\n32\n16\n8.160\nMLET\n64\n16\n8.230\nMLET\n32\n32\n8.106\nMLET\n64\n32\n8.124\nMLET\n64\n64\n8.120\nThe primary system impact of MLET is on training memory with some impact on runtime. MLET\nresults in training memory increase of k/d compared to a single-layer training algorithm (with\nembedding dimension d). We observe that for most experiments the memory requirement was below\n12GB and exceeded that only for the Criteo-Kaggle dataset with k \u2265128. At inference time, MLET\nmemory consumption is equivalent to a single-layer DLRM model with embedding dimension d.\nIn our naive implementation, the runtime cost of MLET training is 25% compared to DLRM. The\nruntimes in terms of time per training iteration for various k, d pairs on NVIDIA RTX 2080 are\nsummarized in Table 2.\n4\nConclusion\nIn this paper, we introduced a multi-layer embedding training architecture that trains embeddings\nvia a sequence of linear layers to derive a superior embedding accuracy vs. model size trade-off.\nWe provide an explanation for obtaining superior embeddings based on the theory of dynamics of\nbackpropagation in linear layer neural networks. We prototyped the MLET scheme within Facebook\u2019s\nPyTorch-based open-source Deep Learning Recommendation Model and demonstrated that it allows\nreducing memory footprint by 4-8X without model accuracy degradation.\n5\nAcknowledgements\nWe gratefully acknowledge the generous support of Facebook Research under the \"AI System Hard-\nware/Software Co-Design\" program. We thank Maxim Naumov, Dheevatsa Mudigere, Constantine\nCaramanis, and Sujay Sanghavi for many helpful discussions.\nReferences\n[1] Reproducibility. PyTorch Documentation v1.4.0 (2020).\n[2] ALVAREZ, J. M., AND SALZMANN, M. Compression-aware training of deep networks. In\nAdvances in Neural Information Processing Systems (2017), pp. 856\u2013867.\n[3] ARORA, S., COHEN, N., HU, W., AND LUO, Y. Implicit regularization in deep matrix\nfactorization. In NeurIPS (2019).\n8\n[4] ARORA, S., GE, R., NEYSHABUR, B., AND ZHANG, Y. Stronger generalization bounds for\ndeep nets via a compression approach. ArXiv abs/1802.05296 (2018).\n[5] ATTENBERG, J., WEINBERGER, K., DASGUPTA, A., SMOLA, A., AND ZINKEVICH, M.\nCollaborative email-spam \ufb01ltering with the hashing trick. CEAS.\n[6] BHAVANA, P., KUMAR, V., AND PADMANABHAN, V.\nBlock based singular value de-\ncomposition approach to matrix factorization for recommender systems.\narXiv preprint\narXiv:1907.07410 (2019).\n[7] CHEN, Y., XU, H., CARAMANIS, C., AND SANGHAVI, S. Matrix completion with column ma-\nnipulation: Near-optimal sample-robustness-rank tradeoffs. IEEE Transactions on Information\nTheory 62 (2011), 503\u2013526.\n[8] CHENG, H., KOC, L., HARMSEN, J., SHAKED, T., CHANDRA, T., ARADHYE, H., AN-\nDERSON, G., CORRADO, G., CHAI, W., ISPIR, M., ANIL, R., HAQUE, Z., HONG, L.,\nJAIN, V., LIU, X., AND SHAH, H. Wide & deep learning for recommender systems. CoRR\nabs/1606.07792 (2016).\n[9] GINART, A., NAUMOV, M., MUDIGERE, D., YANG, J., AND ZOU, J. Mixed dimension em-\nbeddings with application to memory-ef\ufb01cient recommendation systems. ArXiv abs/1909.11810\n(2019).\n[10] GINART, A., NAUMOV, M., MUDIGERE, D., YANG, J., AND ZOU, J. Mixed dimension\nembeddings with application to memory-ef\ufb01cient recommendation systems, 2019.\n[11] GU, S., XIE, Q., MENG, D., ZUO, W., FENG, X., AND ZHANG, L. Weighted nuclear norm\nminimization and its applications to low level vision. International Journal of Computer Vision\n121 (2016), 183\u2013208.\n[12] GU, S., ZHANG, L., ZUO, W., AND FENG, X. Weighted nuclear norm minimization with\napplication to image denoising. 2014 IEEE Conference on Computer Vision and Pattern\nRecognition (2014), 2862\u20132869.\n[13] GUO, H., TANG, R., YE, Y., LI, Z., AND HE, X. Deepfm: A factorization-machine based\nneural network for ctr prediction. In IJCAI (2017).\n[14] JUAN, Y.-C., ZHUANG, Y., CHIN, W.-S., AND LIN, C.-J. Field-aware factorization machines\nfor ctr prediction. In RecSys \u201916 (2016).\n[15] KHRULKOV, V., HRINCHUK, O., MIRVAKHABOVA, L., AND OSELEDETS, I. Tensorized\nembedding layers for ef\ufb01cient model compression. arXiv preprint arXiv:1901.10787 (2019).\n[16] LIAN, J., ZHOU, X., ZHANG, F., CHEN, Z., XIE, X., AND SUN, G. xdeepfm: Combining\nexplicit and implicit feature interactions for recommender systems. Proceedings of the 24th\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining (2018).\n[17] LING, S., SONG, Y., AND ROTH, D. Word embeddings with limited memory. In Proceedings\nof the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers) (Berlin, Germany, Aug. 2016), Association for Computational Linguistics, pp. 387\u2013392.\n[18] LIU, G., LIN, Z., AND YU, Y. Robust subspace segmentation by low-rank representation. In\nICML (2010).\n[19] NAUMOV, M. On the dimensionality of embeddings for sparse features and data. arXiv preprint\narXiv:1901.02103 (2019).\n[20] NAUMOV, M., DIRIL, U., PARK, J., RAY, B., JABLONSKI, J., AND TULLOCH, A. On periodic\nfunctions as regularizers for quantization of neural networks. arXiv preprint arXiv:1811.09862\n(2018).\n[21] NAUMOV, M., MUDIGERE, D., SHI, H.-J. M., HUANG, J., SUNDARAMAN, N., PARK, J.,\nWANG, X., GUPTA, U., WU, C.-J., AZZOLINI, A. G., DZHULGAKOV, D., MALLEVICH,\nA., CHERNIAVSKII, I., LU, Y., KRISHNAMOORTHI, R., YU, A., KONDRATENKO, V. Y.,\nPEREIRA, S., CHEN, X., CHEN, W., RAO, V., JIA, B., XIONG, L., AND SMELYANSKIY, M.\nDeep learning recommendation model for personalization and recommendation systems. ArXiv\nabs/1906.00091 (2019).\n[22] OUYANG, W., ZHANG, X., REN, S., LI, L., LIU, Z., AND DU, Y. Click-through rate\nprediction with the user memory network. ArXiv abs/1907.04667 (2019).\n9\n[23] PENG, C., KANG, Z., LI, H., AND CHENG, Q. Subspace clustering using log-determinant\nrank approximation. In KDD \u201915 (2015).\n[24] RENDLE, S. Factorization machines. 2010 IEEE International Conference on Data Mining\n(2010), 995\u20131000.\n[25] SUN, F., GUO, J., LAN, Y., XU, J., AND CHENG, X. Sparse word embeddings using l1\nregularized online learning. In Proceedings of the Twenty-Fifth International Joint Conference\non Arti\ufb01cial Intelligence (2016), AAAI Press, pp. 2915\u20132921.\n[26] SUN, Q., XIANG, S., AND YE, J. Robust principal component analysis via capped norms. In\nKDD \u201913 (2013).\n[27] TISSIER, J., GRAVIER, C., AND HABRARD, A. Near-lossless binarization of word embeddings.\nProceedings of the AAAI Conference on Arti\ufb01cial Intelligence 33 (Jul 2019), 7104\u20137111.\n[28] WANG, R., FU, B., FU, G., AND WANG, M. Deep & cross network for ad click predictions.\nIn ADKDD\u201917 (2017).\n[29] YIN, Z., AND SHEN, Y. On the dimensionality of word embedding. ArXiv abs/1812.04224\n(2018).\n10\n",
    "1911.02079": "Post-Training 4-bit Quantization on Embedding\nTables\nHui Guan1, Andrey Malevich2, Jiyan Yang2, Jongsoo Park2, Hector Yuen2\n1North Carolina State University\n2Facebook, Inc\nhguan2@ncsu.edu, {amalevich, chocjy, jongsoo, hyz}@fb.com\nAbstract\nContinuous representations have been widely adopted in recommender systems\nwhere a large number of entities are represented using embedding vectors. As the\ncardinality of the entities increases, the embedding components can easily contain\nmillions of parameters and become the bottleneck in both storage and inference\ndue to large memory consumption. This work focuses on post-training 4-bit quanti-\nzation on the continuous embeddings. We propose row-wise uniform quantization\nwith greedy search and codebook-based quantization that consistently outperforms\nstate-of-the-art quantization approaches on reducing accuracy degradation. We\ndeploy our uniform quantization technique on a production model in Facebook and\ndemonstrate that it can reduce the model size to only 13.89% of the single-precision\nversion while the model quality stays neutral.\n1\nIntroduction\nThe success of word embeddings in Natural Language Processing (NLP) [23, 20] has promoted the\nwide adoption of continuous representations in recommender systems in recent years. Embedding-\nbased approaches have achieved state-of-the-art performance in recommendation and ranking tasks\nand have been successfully applied in real-world applications [21, 22, 5, 16, 12]. In these recommen-\ndation models, a large number of entities such as page ids and user ids are encoded using embedding\ntables whose row vectors correspond to entities. As embedding tables scale with the number of entities\nand embedding dimensions, they can easily contain billions of parameters and usually contribute to\n99.99% of the size of the models. For example, a single-precision embedding table with 50,000,000\nnumber of ids and 64 embedding dimensions costs 12GB memory and a recommendation model could\ncontain up to hundreds of embedding tables. Furthermore, due to memory-bandwidth limitations,\nembedding table lookups are one of the most time-consuming operations. Their proportion will\nincrease due to the acceleration of other parts (e.g. FC) from faster increase of compute throughput\nthan memory bandwidth, posing a great challenge to get real-time predictions [22, 21].\nQuantization is one of the effective approaches to reduce model size. By quantizing \ufb02oating-point\nvalues in embedding tables to low-precision numbers that use less number of bits, a large recom-\nmendation model can be reduced to a model with much smaller model size and memory bandwidth\nconsumption during inference. Prior work on quantization has been focusing on quantization-aware\ntraining from scratch or a pre-trained \ufb02oating-point model [7, 28, 26, 11, 18, 8, 15, 10]. Although\nthese techniques have shown promising results, they are not always applicable in many practical sce-\nnarios where the training dataset might be no longer available during model deployment [27, 3, 6, 19].\nIn these cases, post-training quantization is a more desirable approach. Post-training quantization\nis simple to use and convenient for rapid deployment. Recent studies have shown post-training\nquantization using 8-bit precision can achieve accuracy close to that of single-precision models in a\nwide variety of DNN architectures [19, 14]. Post-training quantization using lower bit width (e.g.\n4-bit), however, usually incurs signi\ufb01cant accuracy drop [6].\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1911.02079v1  [cs.LG]  5 Nov 2019\nSeveral state-of-the-art post-training quantization techniques that rely on the clipping have been\nproposed to mitigate accuracy degradation. Shin et al. [24] and Sung et al. [25] approximated the\ninputs as a histogram and adopt a clipping threshold that minimizes the \u21132 norm of the quantization\nerror. Migacz et al. [19] proposed an iterative approach to search for the clipping threshold based on\nKullback-Leibler Divergence measure for quantizing activations. Later, Banner et al. [3] proposed\nACIQ, an analytic solution that computes the optimal clip threshold by assuming the input values are\nsampled from a Gaussian or Laplacian distribution. Although these approaches are demonstrated to\nreduce the accuracy drops to some extent, the problem of post-training 4-bit quantization without\naccuracy drop is still unsolved yet. Empirically, we also observe that the above-mentioned approaches\ncan result in signi\ufb01cant accuracy drops when applied to embedding table quantization.\nIn this paper, we explore a variety of post-training 4-bit quantization methods on embedding tables\nand propose novel quantization approaches that can reduce model size while incurring negligible\naccuracy degradation. Quantization on embedding tables is usually applied to row vectors (row-wise\nquantization) to reduce quantization error. Throughout the paper, quantization is applied to row\nvectors unless noted differently. We notice that the prior post-training quantization approaches approx-\nimate the inputs to quantize using either a histogram or some distributions. While these assumptions\nare bene\ufb01cial to derive ef\ufb01cient algorithms for \ufb01nding the optimal clipping thresholds for weights\nand activations of convolutional neural networks (CNNs), they are not suitable for embedding tables\nbecause their row vectors contain too few values to be well-characterized using either histograms or\ndistributions. Inspired by these understandings, we design quantization algorithms that directly target\nat minimizing the mean square error after quantization. Speci\ufb01cally,\n\u2022 Our exploration reveals that state-of-the-art post-training 4-bit quantization approaches are\nno better than the approach that uses the range the input without clipping when the input\ncontains only tens or hundreds of values, as in the case of embedding table quantization.\n\u2022 We propose two simple yet effective approaches to improve 4-bit quantization on embedding\ntables: 1) row-wise uniform quantization with greedy search that \ufb01nds the best clipping\nthresholds from a gradually discovered set of local optima; 2) codebook-based quantization\nthat maps inputs to indices of non-uniformly distributed values using k-means clustering.\n\u2022 Moreover, for 4-bit quantized embedding tables using uniform quantization, we can achieve\ndequantization performance similar to the Caffe2 8-bit dequantization operators (e.g., Sparse-\nLengthSum1) that are already heavily optimized.\nWe empirically demonstrate the effectiveness of our proposed quantization approaches on DNN-based\nrecommendation models [26, 21] and also a production model in Facebook. The results show that the\nproposed approaches consistently outperform state-of-the-art post-training quantization approaches\nin reducing quantization error and accuracy degradation. Row-wise uniform quantization with greedy\nsearch can reduce the model size to 13.3%-25.0% of the baseline single-precision models with\nnegligible accuracy loss. Codebook-based quantization can reduce the model size to 18.5%-37.5% of\nthe single-precision model with no accuracy loss. We deploy our uniform quantization technique on\na production model in Facebook and demonstrate that it can reduce the model size to only 13.89% of\nthe single-precision version while the model quality stays neutral.\n2\nPrior Quantization Methods and Their Limitations\nLet x be a value clipped to the range [xmin, xmax]. Quantization using n bits maps x to an integer in\nthe range [0, 2n \u22121], where each integer corresponds to a quantized value. If the quantized values are\na set of discrete, evenly-spaced grid points, the methods are called uniform quantization. Otherwise,\nthey are called non-uniform quantization. This section reviews several state-of-the-art post-training\nuniform quantization methods and explains their limitations on embedding table quantization.\nLet xint and xfloat be the quantized and dequantized values respectively. Uniform quantization\nproceeds as follows:\nxint = round\n\u0012\nx \u2212xmin\nxmax \u2212xmin\n\u2217(2n \u22121)\n\u0013\n= round\n\u0012x \u2212bias\nscale\n\u0013\n,\n(1)\n1https://caffe2.ai/docs/operators-catalogue.html#sparselengthssum\n2\nwhere scale = xmax\u2212xmin\n2n\u22121\nand bias = xmin. The de-quantization operation is: xfloat = scale \u2217\nxint + bias. The quantization is symmetric if xmax = \u2212xmin. Otherwise, it is asymmetric. To ease\nthe description below, we de\ufb01ne a quantization function2 as: xfloat = Q(x, xmin, xmax).\n4\n6\n8\n10\n12\n14\nlog(d)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nnormalized l2 loss\nASYM\nHIST-APPRX\nHIST-BRUTE\nGSS\nACIQ\nGREEDY\nGREEDY(opt)\nKMEANS\nTABLE\nFigure 1: The normalized \u21132 loss of 4-bit quantization\nwith different embedding dimensions on a FP32 embed-\nding table with 10 row vectors. The values in the embed-\nding table are randomly sampled from a normal distri-\nbution, which is in favor of GSS and especially ACIQ.\nTABLE applies range-based uniform quantization on\nthe entire table while the other methods are on row vec-\ntors. HIST-APPRX and HIST-BRUTE use b = 200,\nGREEDY uses b = 200, r = 0.16; GREEDY (opt) uses\nb = 1000, r = 0.5.\nWithout loss of generality, let X \u2208RN be an\ninput vector to quantize. Tensors with higher\ndimensions can be \ufb02attened to a vector. Be-\ncause each value is scaled by xmax \u2212xmin, the\nnaive quantization using xmax = max(X) and\nxmin = min(X) is sensitive to outliers, i.e.,\nvalues with large magnitude in the input X, and\ncould cause large accuracy drops. We refer to\nthis method range-based asymmetric quanti-\nzation (ASYM).\nState-of-the-art post-training quantization tech-\nniques rely on the clipping to mitigate accuracy\ndegradation. They differ in their way to mini-\nmize the mean squared error (MSE) of the orig-\ninal values and the quantized values:\nf(xmin, xmax) = \u2225X \u2212Q(X, xmin, xmax)\u22252\n2.\n(2)\nHistogram-based\nQuantization\n(HIST)\nThis method chooses the clipping thresholds\nwhich minimize the MSE between the his-\ntogram of \ufb02oating-point inputs and that of\nthe quantized versions [24]. Let xi and h(xi),\nwhere i = 1, \u00b7 \u00b7 \u00b7 , b, be the bin value and the frequency of the i-th bin in the inputs\u2019 histogram. The op-\ntimization objective is de\ufb01ned as: fhist(xmin, xmax) = 1\nb\nPb\ni=1 h(xi) \u2217(xi \u2212Q(xi, xmin, xmax))2.\nWe used the approximate algorithm (HIST-APPRX) implemented in Caffe2 [1] that scales linearly\nwith the number of bins to solve the above optimization problem. We also implemented a brute force\napproach (HIST-BRUTE) to \ufb01nd better solutions. Its time complexity is O(b3) (See Appendix A).\nACIQ\nAnalytical Clipping for Integer Quantization (ACIQ) [3] derives the clipping thresholds\nanalytically from the distribution of the tensor. It assumes that the values in the tensor are sampled\nfrom a Gaussian or a Laplacian distribution. After determining the distribution to use, it uses an\napproximate closed-form solution for the clip thresholds which minimizes MSE in Eq. 2. For example,\nif the tensor is closer to a Laplacian distribution, the clipping thresholds for 4-bit quantization are\ncalculated using the formula: xmin = E(X)\u2212\u03b1, xmax = E(X)+\u03b1, where \u03b1 = 5.03\u2217E(|X\u2212E(X)).\nWe used the open-source code from the authors3.\nQuantization with Golden Section Search (GSS)\nInstead of approximating the \ufb02oating-point\ninputs using a histogram or assuming it follows a certain distribution, this approach \ufb01nds a range\nlimit xthr that minimizes MSE using golden section search (GSS) [13] for symmetric quantization.\nThe objective function is simpli\ufb01ed as: fsym(xthr) = 1\nN \u2225X \u2212Q(X, \u2212xthr, xthr))\u22252\n2. The method\nis applied to compress word embeddings in [17].\nTheir Limitations\nQuantization on embedding tables is commonly applied to row vectors to\nreduce the quantization error (See ASYM v.s. TABLE in Figure 1). The embedding dimension in\nrecommendation models is usually 8 to 200 [21]. The above clipping-based approaches are better than\nthe range-based asymmetric quantization (ASYM) method when quantizing weights and activations\nof CNNs to 4-bit. However, we empirically observed that they are no better than ASYM when\nthe input X to quantize has a small dimension (i.e., a small number of values), as in the case of\n2An alternative uniform quantization uses: xint = round(x/scale) \u2212zero_point. This method is better\nwhen the inputs to quantize have lots of zeros, e.g., ReLU activations. We found that the mapping in Eq. 1\nprovides better accuracy for embedding table quantization.\n3https://github.com/submission2019/cnn-quantization\n3\nembedding table quantization. Approximating row vectors in an embedding table using histograms\nor distributions could give large quantization error.\nFigure 1 shows the normalized \u21132 loss of different quantization methods with various embedding\ndimensions. Normalized \u21132 loss is calculated as \u2225X \u2212Q(X, xmin, xmax)\u22252/\u2225X\u22252. It measures\nrelative quantization errors. Overall, when the embedding dimension is larger than 1024, GSS, ACIQ,\nHIST-APPRX, and HIST-BRUTE can achieve a smaller loss compared with ASYM. However, when\nthe embedding dimension is small (e.g. 64), the advantage of GSS, ACIQ, and HIST-APPRX over\nASYM is gone. GSS is even much worse than ASYM. Although HIST-BRUTE is still better than\nASYM, it is very time-consuming (millions of times slower than ASYM) and too expensive to be\napplied in real-world recommendation applications that require continuous learning and thus periodic\nquantization for model serving (See Appendix A).\n3\nProposed Quantization Approaches\nThis section elaborates the proposed uniform quantization with greedy search and codebook-based\nquantization with k-means clustering.\nAlgorithm 1 greedy search\nInput: X\n// a vector to quantize.\nInput: b\n// default: 200\nInput: r\n// default: 0.16\nOutput: xmin, xmax // range used for quantization\n1: xmin = cur_min = min(X)\n2: xmax = cur_max = max(X)\n3: loss = compute_loss(X, Q(X, xmin, xmax))\n4: stepsize = (xmax - xmin)/b\n5: min_steps = b * (1 - r) * stepsize\n6: while cur_min + min_steps < cur_max do\n7:\nloss_l = compute_loss(X, Q(X, cur_min + step-\nsize, cur_max))\n8:\nloss_r = compute_loss(X,Q(X, cur_min, cur_max\n- stepsize))\n9:\nif loss_l < loss_r then\n10:\ncur_min = cur_min + stepsize\n11:\nif loss_l < loss then\n12:\nloss, xmin = loss_1, cur_min\n13:\nelse\n14:\ncur_max = cur_max - stepsize\n15:\nif loss_r < loss then\n16:\nloss, xmax = loss_r, cur_max\n17: return xmin, xmax\nUniform Quantization with Greedy Search\n(GREEDY)\nTo overcome the limitations of\nthe prior uniform quantization approaches, we\npropose a greedy search algorithm (see Algo-\nrithm 1) to \ufb01nd the optimal clipping thresholds.\nThe algorithm is inspired by the 1-D golden\nsection search (GSS) and directly targets at min-\nimizing the MSE objective function in Eq. 2.\nAlthough 2-D GSS was proposed recently, it is\nnot applicable in general as it is too consum-\ning [4]. The basic idea of greedy search is to\n\ufb01nd as many local optima as possible and select\nthe best one as the clipping thresholds. The al-\ngorithm takes the input vector X and two hyper-\nparameters b and r that balance the optimality\nof its solution and its time complexity.\nThe algorithm initializes xmin and xmax with\nthe range of the input X (lines 1-2). It then\ngradually increases xmin or decreases xmax\nby one stepsize to reduce the range and \ufb01nd\na smaller loss calculated as Eq. 2 (lines 7-16).\nThe algorithm stops when the current range is\n1 \u2212r percentage of the range of X (lines 5-6).\nThe larger the b and r are, the better the found\nsolution will be but the higher the time cost is\n(O(b \u00d7 r) time complexity). The default value\nof b and r are set as 200 and 0.16 respectively.\nCodebook-based Quantization\nCodebook-based quantization is a type of non-uniform quantiza-\ntion that maps each input value to the index of a value in the codebook. Quantizing a value to 4 bits\nmeans the number of values in a codebook cannot be larger than 16. We consider the following two\ncodebook-based quantization variants: Quantization with Rowwise Clustering (KMEANS) and\nQuantization with Two-Tier Clustering (KMEANS-CLS). KMEANS algorithm applies k-means\nclustering to produce a 16-value codebook for each row vector, and then maps each value in the row\nvector to the index of the codebook based on its cluster assignment. Although the algorithm has less\nmodel size reduction than uniform quantization due to the storage overhead of codebooks, it has the\npotential to achieve a lower MSE and avoid accuracy degradation.\nTo achieve a larger compression rate, KMEANS-CLS applies k-means clustering in a more coarse-\ngrained way. The algorithm \ufb01rst groups similar row vectors in an embedding table into blocks (called\ntier-1 clustering) and then generates a 16-value codebook for each block (called tier-2 clustering).\n4\nTable 1: Computational throughput in billion sums per second for SparseLengthsSum operators.\nThe performance is measured using a single core of Intel Xeon Gold 6138 CPU @ 2.0 GHz with\nturbo-mode off.\nData type\nCache non-resident case\nCache resident case\nd=64\nd=128\nd=256\nd=512\nd=64\nd=128\nd=256\nd=512\nFP32\n1.939\n1.908\n1.997\n2.063\n2.804\n4.165\n4.209\n4.127\nINT8\n1.246\n1.511\n2.726\n3.076\n2.242\n2.510\n3.748\n3.450\nINT4\n1.608\n2.047\n2.532\n5.581\n2.093\n2.878\n6.454\n6.893\nBoth steps use k-means clustering. Let K be the number of clusters in tier-1 clustering. After 4-bit\nquantization using KMEANS-CLS, the required number of bytes to store a N \u00d7 d embedding table\nis Nd/2 + N log2 K/8 + 64K, where log2 K/8 is the number of bytes used to store tier-1 cluster\nassignment.\n4\nEf\ufb01cient 4-bit Embedding Operation Implementation\nA challenge for quantizing embedding tables in less than 8-bit is the overhead of dequantization when\nreading the tables. This is because, in most commonly used processors, 8-bit is the smallest granularity\nin which instructions can operate with, and less than 8-bit granularity requires bit manipulations\nlike or, shift, and so on. Nevertheless, we found that we can sustain good enough dequantization\nthroughput with careful use of vector instructions available in recent CPUs (e.g., AVX512 in Intel\nSkylake CPUs) as shown in Table 1. We measure computational throughput of SparseLengthsSum\noperator (the most time-consuming operator reading embedding tables in our recommendation\nmodels [21]) in FP32, INT8, and INT4, both in cache non-resident and cache resident cases. In\ncache non-resident cases, we \ufb02ush the last level cache between benchmark runs, which is more\nrepresentative of running big recommendation models with many huge embedding tables. The cache\nresident cases are to see upper bound computational throughputs (a worst case for 4-bit embedding\ntables). We can see that in most cases the speed of 4-bit SparseLengthsSum is on par or faster than its\nhighly-optimized 8-bit or FP32 counterparts running in production.\n5\nExperiments\nIn this section, we present the experimental results of the proposed approaches. We use the Terabyte\nCriteo data [2]. It is a click prediction dataset that has a size of 1.3TB and contains more than 4.3\nbillion records. The dataset is a commonly used benchmark dataset for ranking applications.\nThe ranking problem is a binary classi\ufb01cation problem. The models we used are DNN models [21].\nFor categorical features, following the same procedure as in [26], we transform them into dense\nvectors using embedding tables. The number of rows in embedding tables corresponds to the number\nof categorical features with a maximum of 50 million. The number of columns corresponds to the\nembedding dimension. We choose a variety of embedding dimensions: 8, 16, 32, 64, and 128, that are\ncommonly used in ranking models. The dense embeddings of the categorical features, concatenated\nwith the dense vector formed by dense features, are taken as the input to a neural network with\n2 fully-connected (FC) layers. The FC layers have a width of 512. The models are trained using\nAdagrad [9] with a batch size of 100. The initial learning rate is set to 0.015 for embedding tables\nand 0.005 for the rest of the parameters. All the parameters are trained using single-precision (FP32).\nAll the embedding tables are quantized to use 4 bits after model training is \ufb01nished.\nWe compare the proposed approaches (GREEDY, KMEANS, KMEANS-CLS) with other uniform\nquantization approaches including SYM, GSS, ASYM, HIST-APPROX, HIST-BRUTE, ACIQ.\nBecause k-means is sensitive to initialization, we initialize cluster centers using uniform quantization\nresults from ASYM. The default hyperparameter settings (b = 200, r = 0.16) are used for greedy\nsearch. HIST-APPRX uses b = 200 as it gives the best performance after a grid-based hyper-\nparameter tuning. For KMEANS-CLS, we choose the K such that it achieves the same compression\nrate as the uniform quantization approaches. If a method is appended with \u201c(FP16)\u201d, it means that\nthe scales and biases in uniform quantization and the codebooks in codebook-based quantization\nare stored using FP16 instead of FP32. Besides the baseline where embedding tables are not\nquantized (FP32), we include another baseline that uses range-based uniform quantization to quantize\n5\nTable 2: Normalized l2 loss with different quantization methods and embedding dimensions.\nMethods\nDescription\nd=8\nd=16\nd=32\nd=64\nd=128\nASYM-8BITS\nAsymmetric, xmin = min(X), xmax = max(X)\n0.00260\n0.00329\n0.00376\n0.00387\n0.00400\nSYM\nSymmetric, xmin = \u2212xmax, xmax = max(|X|)\n0.05564\n0.06296\n0.06785\n0.06836\n0.06928\nGSS\nSymmetric with Golden Section Search\n0.05269\n0.05965\n0.06328\n0.06400\n0.06423\nASYM\nAsymmetric, xmin = min(X), xmax = max(X)\n0.04451\n0.05479\n0.06387\n0.06608\n0.06781\nHIST-APPRX\nAsymmetric with histogram-based approximation [1]\n0.04452\n0.05512\n0.06409\n0.06589\n0.06768\nHIST-BRUTE\nAsymmetric with histogram-based brute force algorithm\n0.04156\n0.05082\n0.05881\n0.06083\n0.06272\nACIQ\nAnalytical Clipping for Integer Quantization [3]\n0.04451\n0.05479\n0.06387\n0.06742\n0.07665\nGREEDY\nAsymmetric with greedy search (ours)\n0.03889\n0.04878\n0.05744\n0.05991\n0.06221\nGREEDY (FP16)\nAsymmetric with greedy search (ours)\n0.03889\n0.04879\n0.05744\n0.05991\n0.06221\nKMEANS-CLS (FP16)\nTwo-tier k-means clustering with uniform init (ours)\n0.03948\n0.05349\n0.06826\n0.07369\n0.07287\nKMEANS (FP16)\nRowwise kmeans clustering with uniform init (ours)\n0\n0\n0.03670\n0.05160\n0.05781\n\"-8BITS\": 8-bit quantization; otherwise, 4-bit quantization. \"(FP16)\": scales and biases or values of a codebook in FP16; otherwise, FP32.\nTable 3: Model log loss and size with different quantization methods and embedding dimensions.\nMethods\nd=8\nd=16\nd=32\nd=64\nd=128\nloss\nsize\nloss\nsize\nloss\nsize\nloss\nsize\nloss\nsize\nFP32 (no quantization)\n0.12522\n8.07GB\n0.12489\n16.14GB\n0.12468\n32.27GB\n0.12451\n64.54GB\n0.12454\n129.09GB\nASYM-8BITS\n0.12522\n49.98%\n0.12489\n37.49%\n0.12469\n31.25%\n0.12451\n28.12%\n0.12454\n26.56%\nSYM\n0.12528\n37.49%\n0.12507\n24.99%\n0.13266\n18.75%\n0.13107\n15.62%\n0.12470\n14.06%\nGSS\n0.12527\n0.12504\n0.13199\n0.12843\n0.12459\nASYM\n0.12526\n0.12491\n0.12496\n0.12494\n0.12455\nHIST-APPRX\n0.12525\n0.12492\n0.12497\n0.12498\n0.12455\nHIST-BRUTE\n0.12525\n0.12490\n0.12490\n0.12489\n0.12454\nACIQ\n0.12526\n0.12491\n0.12804\n0.12514\n0.12455\nGREEDY\n0.12525\n0.12490\n0.12489\n0.12485\n0.12454\nGREEDY (FP16)\n0.12525\n24.99%\n0.12490\n18.74%\n0.12489\n15.62%\n0.12485\n14.06%\n0.12454\n13.28%\nKMEANS (FP16)\n-\n-\n-\n-\n0.12469\n37.50%\n0.12451\n25.00%\n0.12454\n18.75%\nall embedding tables to 8 bits (ASYM-8BITS). We evaluate the performance of the quantization\napproaches using three evaluation metrics: Normalized \u21132 loss, model log loss, and model size.\nTable 2 lists the normalized \u21132 loss results on an embedding table from models with different\nembedding dimensions. Overall, our proposed approach GREEDY consistently gives the smallest loss\namong all 4-bit uniform quantization approaches. Using FP16 for scales and biases further reduces\nthe embedding table size without affecting the loss. KMEANS achieves the smallest normalized \u21132\nloss for the use of codebook. Even though KMEANS-CLS variants can achieve the same compression\nrate as the uniform quantization approaches, they suffer from larger losses, indicating the importance\nof row-wise quantization for embedding tables.\nTable 3 lists the model log loss and model size for the models after 4-bit quantization. Overall,\nGREEDY consistently gives the smallest model log loss compared with other uniform quantization\napproaches while reducing the models to 13.3%-25.0% of the single-precision model size. The\nproposed KMEANS approach can even get the same model loss as the original single-precision\nmodel while reducing the models to 18.8%-37.5% of the single-precision model size.\nWe deployed GREEDY on one of the ranking applications at Facebook. The application uses a DNN\nmodel trained on billions of records. Being able to reduce the model size using post-training 4-bit\nquantization while preserving model accuracy is a challenging task. Our experimental results show\nthat the 4-bit uniform quantization with greedy search can reduce the model size to only 13.89% of\nthe single-precision version while the model quality stays neutral. This demonstrates the practicality\nof our approach in real applications.\n6\nConclusions and Future Work\nWe proposed row-wise uniform quantization with greedy search and non-uniform quantization with\nk-means clustering to improve 4-bit post-training quantization on embedding tables. We empirically\nshowed that the proposed approaches consistently outperform state-of-the-art quantization methods\non reducing the quantization error and the model accuracy degradation. The model size reduction\nresulting from 4-bit quantization makes it possible to use even larger embedding tables for potentially\nbetter model accuracy. In the future, we want to explore how much accuracy gain can be achieved by\nincreasing model size while applying 4-bit quantization to meet a certain space budget.\n6\nReferences\n[1] caffe2 histogram-based norm minimization. https://caffe2.ai/doxygen-c/html/norm_\n_minimization_8cc_source.html. Accessed: 2019-08-03.\n[2] Criteo\nreleases\nindustry\u2019s\nlargest-ever\ndataset\nfor\nmachine\nlearning\nto\nacademic\ncommunity.\nhttps://www.criteo.com/news/press-releases/2015/07/\ncriteo-releases-industrys-largest-ever-dataset/. Accessed: 2019-08-03.\n[3] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Aciq: Analytical clipping for\ninteger quantization of neural networks. arXiv preprint arXiv:1810.05723, 2018.\n[4] Yen-Ching Chang. N-dimension golden section search: Its variants and limitations. In 2009 2nd\nInternational Conference on Biomedical Engineering and Informatics, pages 1\u20136. IEEE, 2009.\n[5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,\nGlen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for\nrecommender systems. In Proceedings of the 1st workshop on deep learning for recommender\nsystems, pages 7\u201310. ACM, 2016.\n[6] Yoni Choukroun, Eli Kravchik, and Pavel Kisilev. Low-bit quantization of neural networks for\nef\ufb01cient inference. arXiv preprint arXiv:1902.06822, 2019.\n[7] Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with\nweights and activations constrained to+ 1 or- 1. arxiv 2016. arXiv preprint arXiv:1602.02830.\n[8] Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger,\nKunle Olukotun, and Christopher R\u00e9. High-accuracy low-precision training. arXiv preprint\narXiv:1803.03383, 2018.\n[9] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.\n[10] Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, and Evgeny Terentev. Fast\nadjustable threshold for uniform neural network quantization. arXiv preprint arXiv:1812.07872,\n2018.\n[11] Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu Zhou, and Yuheng Zou.\nEffective quantization methods for recurrent neural networks. arXiv preprint arXiv:1611.10176,\n2016.\n[12] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural\ncollaborative \ufb01ltering. In Proceedings of the 26th international conference on world wide web,\npages 173\u2013182. International World Wide Web Conferences Steering Committee, 2017.\n[13] Jack Kiefer. Sequential minimax search for a maximum. Proceedings of the American mathe-\nmatical society, 4(3):502\u2013506, 1953.\n[14] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for ef\ufb01cient inference:\nA whitepaper. arXiv preprint arXiv:1806.08342, 2018.\n[15] Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training\nquantized nets: A deeper understanding. In Advances in Neural Information Processing Systems,\npages 5811\u20135821, 2017.\n[16] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong\nSun. xdeepfm: Combining explicit and implicit feature interactions for recommender systems.\nIn Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining, pages 1754\u20131763. ACM, 2018.\n[17] Avner May, Jian Zhang, Tri Dao, and Christopher R\u00e9. On the downstream performance of\ncompressed word embeddings. arXiv preprint arXiv:1909.01264, 2019.\n7\n[18] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. arXiv preprint arXiv:1710.03740, 2017.\n[19] Szymon Migacz. 8-bit inference with tensorrt. In GPU technology conference, volume 2, page 7,\n2017.\n[20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-\nsentations of words and phrases and their compositionality. In Advances in neural information\nprocessing systems, pages 3111\u20133119, 2013.\n[21] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sun-\ndaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini,\net al. Deep learning recommendation model for personalization and recommendation systems.\narXiv preprint arXiv:1906.00091, 2019.\n[22] Jongsoo Park, Maxim Naumov, Protonu Basu, Summer Deng, Aravind Kalaiah, Daya Khudia,\nJames Law, Parth Malani, Andrey Malevich, Satish Nadathur, et al. Deep learning inference in\nfacebook data centers: Characterization, performance optimizations and hardware implications.\narXiv preprint arXiv:1811.09886, 2018.\n[23] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for\nword representation. In Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 1532\u20131543, 2014.\n[24] Sungho Shin, Kyuyeon Hwang, and Wonyong Sung. Fixed-point performance analysis of\nrecurrent neural networks. In 2016 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 976\u2013980. IEEE, 2016.\n[25] Wonyong Sung, Sungho Shin, and Kyuyeon Hwang. Resiliency of deep neural networks under\nquantization. arXiv preprint arXiv:1511.06488, 2015.\n[26] Jian Zhang, Jiyan Yang, and Hector Yuen. Training with low-precision embedding tables. In\nSystems for Machine Learning Workshop at NeurIPS, volume 2018, 2018.\n[27] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural net-\nwork quantization without retraining using outlier channel splitting. In International Conference\non Machine Learning, pages 7543\u20137552, 2019.\n[28] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net:\nTraining low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint\narXiv:1606.06160, 2016.\n8\nA\nHIST-BRUTE\nIn this section, we present the algorithm and the complexity analysis of the brute force histogram-\nbased quantization approach (HIST-BRUTE). Its pseudo-code is shown in Algorithm 2. For 4-bit\nquantization, the algorithm uses a histogram with 16 number of bins dst_nbins to approximate the\nhistogram of the inputs with b number of bins. Lines 1-9 are for initialization. The algorithm selects\ndifferent numbers of consecutive bins from the inputs\u2019 histogram and approximates these selected\nbins using 16 target bins (lines 10-36). The selected bins determine the clipping thresholds (lines\n37-42). HIST-BRUTE has a time complexity of O(b3).\nAlgorithm 2 HIST-BRUTE\nInput: X // a vector to quantize.\nInput: b // number of bins used to generate histogram, default: 200\nOutput: xmin, xmax // range used for quantization\n1: // Initialize\n2: xmin = min(X)\n3: xmax = max(X)\n4: histogram = get_histogram(X, b)\n5: dst_nbins = 16\n6: bin_width = (xmax - xmin)/b\n7: norm_min = \u221e\n8: best_start_bin = -1\n9: best_nbins_selected = 1\n10: for nbins_selected = 1 to b do\n11:\nstart_bin_begin = 0\n12:\nstart_bin_end = b - nbins_selected + 1\n13:\ndst_bin_width = bin_width * nbins_selected / (dst_nbins - 1)\n14:\nfor start_bin = start_bin_begin to start_bin_end do\n15:\nnorm = 0\n16:\n// Go over each histogram bin and accumulate errors.\n17:\nfor src_bin = 0 to b do\n18:\nsrc_bin_begin = (src_bin - start_bin) * bin_width\n19:\nsrc_bin_end = src_bin_begin + bin_width\n20:\n// Determine which dst_bins the beginning and end of src_bin belong to\n21:\ndst_bin_of_begin = min(dst_nbins - 1, max(0, \ufb02oor((src_bin_begin + 0.5 * dst_bin_width) /\ndst_bin_width)))\n22:\ndst_bin_of_end = min(dst_nbins - 1, max(0, \ufb02oor((src_bin_end + 0.5 * dst_bin_width) /\ndst_bin_width)))\n23:\ndst_bin_of_begin_center = dst_bin_of_begin * dst_bin_width\n24:\ndensity = histogram[src_bin] / bin_width\n25:\ndelta_begin = src_bin_begin - dst_bin_of_begin_center\n26:\nif dst_bin_of_begin == dst_bin_of_end then\n27:\ndelta_end = src_bin_end - dst_bin_of_begin_center\n28:\nnorm += get_l2_norm(delta_begin, delta_end, density)\n29:\nelse\n30:\ndelta_end = dst_bin_width / 2\n31:\nnorm += get_l2_norm(delta_begin, delta_end, density)\n32:\nnorm += (dst_bin_of_end - dst_bin_of_begin - 1) * get_l2_norm( -dst_bin_width / 2,\ndst_bin_width / 2, density)\n33:\ndst_bin_of_end_center = dst_bin_of_end * dst_bin_width\n34:\ndelta_begin = -dst_bin_width / 2\n35:\ndelta_end = src_bin_end - dst_bin_of_end_center\n36:\nnorm += get_l2_norm(delta_begin, delta_end, density)\n37:\nif norm < norm_min then\n38:\nnorm_min = norm\n39:\nbest_start_bin = start_bin\n40:\nbest_nbins_selected = nbins_selected\n41: xmin = xmin + bin_width * best_start_bin\n42: xmax = xmax + bin_width * (best_start_bin + best_nbins_selected)\n43: return xmin, xmax\n9\n4\n6\n8\n10\n12\n14\nlog(d)\n\u22121\n0\n1\n2\n3\n4\n5\ntime (ms) in log10\nASYM\nHIST-APPRX\nHIST-BRUTE\nGSS\nACIQ\nGREEDY\nGREEDY(opt)\nFigure 2: Average time per row spent on 4-bit quantization. Time is shown in log10 scale.\nFigure 2 shows the average time in milliseconds to quantize a row vector with different dimensions\nusing 4 bits. To make a fair comparison, we implemented all the quantization algorithms in python.\nThe results show that HIST-BRUTE is millions of times slower than ASYM. All the other clipping-\nbased approaches take less than 100ms to quantize a row vector when d is less than 2048. The times\nare measured on a computer with Ubuntu 16.04, 3.00GHz Intel Xeon CPU E5-1607 processor, and\n8GB memory.\nB\nHistograms After 4-bit Quantization\nWe show the histograms of a vector after 4-bit quantization using different approaches in Figure 3.\nThe vector is of dimension 64 and its values are randomly sampled from a Gaussian distribution. The\nresults echo the observations in Figure 1 that GREEDY and KMEANS have the smallest quantization\nerror than state-of-the-art quantization approaches (HIST-APPRX, ACIQ, GSS).\n10\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\n8\n10\nfrequency\nnormalized l2 loss: 0.1009\nFP32:[-2.3015, 2.1003]\nSYM:[-2.3015, 1.9947]\n(a) SYM\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\n8\n10\nfrequency\nnormalized l2 loss: 0.0909\nFP32:[-2.3015, 2.1003]\nGSS:[-1.9702, 1.9702]\n(b) GSS\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\n8\n10\nfrequency\nnormalized l2 loss: 0.0879\nFP32:[-2.3015, 2.1003]\nASYM:[-2.3015, 2.1003]\n(c) ASYM\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\n8\n10\nfrequency\nnormalized l2 loss: 0.0879\nFP32:[-2.3015, 2.1003]\nHIST-APPRX:[-2.3015, 2.1003]\n(d) HIST-APPRX\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\n8\n10\nfrequency\nnormalized l2 loss: 0.0733\nFP32:[-2.3015, 2.1003]\nHIST-BRUTE:[-2.3015, 2.0122]\n(e) HIST-BRUTE\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\n8\nfrequency\nnormalized l2 loss: 0.1037\nFP32:[-2.3015, 2.1003]\nACIQ:[-2.0835, 2.1214]\n(f) ACIQ\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\n8\n10\nfrequency\nnormalized l2 loss: 0.0733\nFP32:[-2.3015, 2.1003]\nGREEDY:[-2.3015, 2.0122]\n(g) GREEDY\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\n8\n10\nfrequency\nnormalized l2 loss: 0.0522\nFP32:[-2.3015, 2.1003]\nKMEANS:[-2.1887, 2.1003]\n(h) KMEANS\nFigure 3: Histograms of a vector (d=64) before and after 4-bit quantization with different techniques.\nHIST-APPRX and HIST-BRUTE use b = 200, GREEDY uses b = 200, r = 0.16.\n11\n",
    "2010.11305": "MIXED-PRECISION EMBEDDING USING A CACHE\nJie (Amy) Yang * 1 Jianyu Huang * 1 Jongsoo Park 1 Ping Tak Peter Tang 1 Andrew Tulloch 1\nABSTRACT\nIn recommendation systems, practitioners observed that increase in the number of embedding tables and their\nsizes often leads to signi\ufb01cant improvement in model performances. Given this and the business importance of\nthese models to major internet companies, embedding tables for personalization tasks have grown to terabyte scale\nand continue to grow at a signi\ufb01cant rate. Meanwhile, these large-scale models are often trained with GPUs where\nhigh-performance memory is a scarce resource, thus motivating numerous work on embedding table compression\nduring training. We propose a novel change to embedding tables using a cache memory architecture, where\nthe majority of rows in an embedding is trained in low precision, and the most frequently or recently accessed\nrows cached and trained in full precision. The proposed architectural change works in conjunction with standard\nprecision reduction and computer arithmetic techniques such as quantization and stochastic rounding. For an\nopen source deep learning recommendation model (DLRM) running with Criteo-Kaggle dataset, we achieve\n3\u00d7 memory reduction with INT8 precision embedding tables and full-precision cache whose size are 5% of the\nembedding tables, while maintaining accuracy. For an industrial scale model and dataset, we achieve even higher\n>7\u00d7 memory reduction with INT4 precision and cache size 1% of embedding tables, while maintaining accuracy,\nand 16% end-to-end training speedup by reducing GPU-to-host data transfers.\n1\nINTRODUCTION\nMachine learning and deep learning in particular has been\ntremendously successful in tackling tasks that were tradition-\nally considered to require human intelligence or intuition.\nDeep learning typically accomplishes this feat with massive\ncomputations in the Euclidean space on matrices of real\nnumbers. While inputs to many deep learning tasks are\nnaturally represented as matrices of continuous numerical\nvalues such as image classi\ufb01cation and detection, in the\ncases where input data is discrete in nature such as categori-\ncal features and vocabularies, embedding is at present the\nde facto technique that maps a discrete set into the continu-\nous Euclidean space. In essence, a good embedding maps\ndiscrete entries into Rd in such a way that preserves natural\nrelationship via the basic operations in Euclidean space.\nNaturally, embedding table is an important component of\nrecommendation systems that are important to internet com-\npanies (Cheng et al., 2016; Wang et al., 2017; Hazelwood\net al., 2018; Naumov et al., 2019), which rely heavily on\ncategorical features to deliver personalized contents based\non user-item interactions. Such models customarily contain\nhundreds of embedding tables representing different users,\n*Equal contribution\n1Facebook Inc., Menlo Park, California,\nUSA. Correspondence to: Jie (Amy) Yang <amyyang@fb.com>.\nProceedings of the 4 th MLSys Conference, Austin, TX, USA,\n2020. Copyright 2021 by the author(s).\ncharacteristics of commercial items, news articles\u2019 topics,\nand are commonly tens of gigabytes in size in commercial\nML models (Hazelwood et al., 2018; Park et al., 2018).\nMoreover, industry generally believes that further growing\nthe embedding table size will improve the models\u2019 predic-\ntive performance. However, such scaling of embeddings\nposes a challenge in scaling the training system\u2019s memory\nand computation capacity accordingly. In GPU training\nsystems with highly parallel execution but limited high-\nbandwidth memory capacity, the cost of training can be\nsigni\ufb01cantly increased as embeddings trend towards billions\nscale. Due to industry\u2019s interest in recommendation models,\naccommodating the intense memory requirement of em-\nbeddings is essential in scaling high-performance training\nsystems.\nFigure 1 captures the structure of a representative Deep\nLearning Recommendation Model (DLRM) (Naumov et al.,\n2019) developed for personalization tasks. Dense and sparse\nfeatures are computed via multi-layer perceptron (MLP)\nand embedding lookups, then joined in sparse-dense in-\nteraction and top MLP to compute the \ufb01nal click-through\nrate prediction. Large embeddings are partitioned across\nmultiple GPUs following model-parallelism with no repli-\ncation across device. This paper focuses on training large\nembeddings with reduced memory footprint while maintain-\ning accuracy. Training acceleration via dense computation\noptimization is not the main subject of this study.\narXiv:2010.11305v2  [cs.LG]  23 Oct 2020\nSubmission and Formatting Instructions for MLSys 2020\nFigure 1. Deep Learning Recommendation Model (DLRM) (Nau-\nmov et al., 2019)\nIn this work, we propose to train embeddings in low pre-\ncision with the addition of a small full-precision cache for\nthe most recently or frequently accessed rows. Depend-\ning on the speci\ufb01c cache replacement policy in play, some\nembedding rows may behave as if they were trained in full\nprecision most or even all of the time. Previous work (Zhang\net al., 2018) shows that we can train embeddings in FP16\nwith stochastic rounding while maintaining neutral accuracy.\nWe show that with a small high-precision cache, we can\ntrain embeddings with INT8 and INT4 precision, further\nnarrowing the bitwidths and thus the memory consumption\nat little to no cost of accuracy. While caching is commonly\nadopted in GPU training systems to utilize CPU host mem-\nory (Zhao et al., 2020), we reuse the same architecture with\nmixed-precision to achieve the purpose of reduced memory\nfootprint for embeddings and maintain a neutral training\naccuracy.\nSpeci\ufb01cally, this paper makes the following contributions:\n\u2022 We propose mixed-precision embedding training with\na full-precision cache, and show its effectiveness and\ngenerality in reducing embeddings memory (3\u00d7 mem-\nory reduction on an open source DLRM model and 7\u00d7\nmemory reduction on an industrial scale model) usage\nduring training and maintaining model accuracy.\n\u2022 We compare different cache replacement policies: least\nrecently used (LRU), least frequently used (LFU), and\nthe effect of cache associativity on model accuracy and\ncache hit rate.\n\u2022 We explore the effect of varying cache sizes on model\naccuracy with embeddings trained in low precision.\nWhile model accuracy improves with large cache sizes,\nthere is diminishing return in accuracy recovery with\nincreasingly large cache size.\n\u2022 We study the effect of rounding modes on model accu-\nracy with and without cache, and show that stochastic\nrounding consistently outperforms round-to-nearest in\nlow-precision embedding training by getting rid of sys-\ntematic rounding bias.\n\u2022 We achieve 16% end-to-end training speed improve-\nment on an industrial scale recommendation system\nusing our optimized full-precision cache implementa-\ntion. With 95% cache hit rate, the embedding lookup\nthroughput is improved by 25.8\u00d7 over that of uni\ufb01ed\nmemory.\nThe rest of paper is organized as follows. Section 2 reviews\nprior work on embedding table compression techniques.\nSection 3 explains our approach and different cache replace-\nment policies. Section 4 discusses the experiments and\nresults with CPU emulation and open source DLRM model.\nThe results support our focus on an associative cache GPU\nimplementation described in Section 5. The GPU imple-\nmentation enables us to experiment with large-scale data\nset that would otherwise be prohibitive on CPU. Section 6\nrecaps our result and discusses future work.\n2\nRELATED WORKS\nA sample of works showing embedding in action are\n(Mikolov et al., 2013; Pennington et al., 2014; Liu et al.,\n2015; Peters et al., 2018; Liu et al., 2019) for natural lan-\nguage processing, (Vasileva et al., 2018; Barz & Denzler,\n2019) for computer vision, and other areas such as knowl-\nedge representation and biology (Wu et al., 2018a; Chiu\net al., 2016).\nWe follow the conceptual classi\ufb01cation termed in (Ginart\net al., 2019) of algorithmic and architectural compression of\nembedding tables. Some compression algorithms apply post\nprocessing methods on the embedding tables such as low-\nrank SVD and other factorizations (Bhavana et al., 2019;\nAndrews, 2015), sparsi\ufb01cation (Sattigeri & Thiagarajan;\nSun et al., 2016) and quantization (Guan et al., 2019). Other\ncompression algorithms aim at arriving at easily compress-\nible tables through training. These include quantization or\ncompression-aware training (Alvarez & Salzmann, 2017;\nPark et al., 2018; Naumov et al., 2018; Elthakeb et al., 2019)\nand gradual pruning (Frankle & Carbin, 2018). These tech-\nniques reduce the memory requirement at inference time\nbut use the uncompressed embedding tables during training.\nAlternatively, compressed architectures use alternative repre-\nsentation in the ML model architecture of the embedding ta-\nbles other than the standard 2D-array of FP32 \ufb02oating-point\nnumbers. Hence memory saving is realized even during\ntraining. Along this line, representing the tables in low-\nprecision data type is a natural strategy. Works along this\nline mainly need to maintain the models\u2019 accuracy despite\nthe models\u2019 parameters have reduced precision. Notable\nSubmission and Formatting Instructions for MLSys 2020\nworks using this approach include (Gupta et al., 2015; Chen\net al., 2017; De Sa et al., 2018; Wu et al., 2018b; Zhang\net al., 2018; Kalamkar et al., 2019). We note that some of\nthe just referenced works try to reduce the memory footprint\nof weight parameters so as to improve compute performance\nand not that of embedding table size. Some other works\nexplore more structural changes to the embedding table\nsuch as tensor train representation (Khrulkov et al., 2019),\nmixed-dimension factorization (Ginart et al., 2019) or vector\nquantization (Fan et al., 2020).\nInstead of using algebraic factorization, we use memory hi-\nerarchy and varying precision to enable compression. Like\nthe other compression architecture work, the compression\nbene\ufb01ts training as well as inference. The sequel of this\npaper explains our design and reports the results on exten-\nsive experiments with our CPU and GPU implementations\nrunning different models on multiple data sets with different\ncache size/precision/policy combinations.\n3\nMIXED-PRECISION EMBEDDING TABLE\nWITH A CACHE\nA common embedding table T consists of N rows of d-\ndimensional vectors of FP32 numbers. Each row corre-\nsponds to some categorical features, for example a particular\nuser, and any production grade ML model such as a recom-\nmendation system can consist of hundreds of such tables.\nExpanding the capacity of embedding tables \u2013 by increasing\nthe total number of tables or dimensions of individual ones \u2013\ncan increase model performance, motivating active research\nin embedding table compression that does not sacri\ufb01ce the\ntables\u2019 representation capacities.\nTo explain our compression method of mixed-precision with\na cache, it suf\ufb01ces to consider the work\ufb02ow involving a\nsingle row of one table T during a training iteration. During\na forward pass, a particular index-i row x of T is needed\nand obtained by a fetch operator x \u2190fetch(T, i). This\nrow will participate in the model evaluation as input to\nsome layers and eventually contribute to the loss function\nL. During the corresponding backward pass, the gradient\nof L with respect to x, \u2202L\n\u2202x is computed and x updated via\nx \u2190x \u2212\u03b7 \u2202L\n\u2202x for example with a simple SGD algorithm\nwith learning rate \u03b7. This update to the embedding table is\ndenoted as update(T, i, x).\nOur method has two main components. The \ufb01rst is the use of\na reduced-precision embedding table similar to (Zhang et al.,\n2018). Table T is in a precision lower than FP32. During\na forward pass, the fetch operator x \u2190fetch(T, i) up-\nconverts the row in its low-precision representation to FP32.\nComputations are carried out in FP32 including the com-\nputation x \u2190x \u2212\u03b7 \u2202L\n\u2202x . The update step update(T, i, x)\nuses either a round-to-nearest or stochastic rounding (Zhang\net al., 2018) approach to down convert x into the precision\nof T\u2019s entries. (Zhang et al., 2018) applies this approach to\nan FP16 embedding table T and uses stochastic rounding to\nstore the updated row back into T.\nOur second component augments the low-precision embed-\nding table T with a cache C of FP32 entries. C only has n\nrows, which is a fraction of T\u2019s row dimension N. Similar\nto cache memory architecture, an embedding row may be\nresiding both in the cache C and the table T, albeit with\ndifferent precision. We use and preserve the high precision\nversion whenever possible.\nCombining these two components, the method is encap-\nsulated by the fetch and update operators as x \u2190\nfetch(T, C, i) and update(T, C, i, x).\nThe operator\nfetch returns the higher-precision version if the row i re-\nsides in cache, or return the up converted result of the lower-\nprecision version in T. In updating the table and cache with\nthe updated row x, the update operator checks that if this\nrow is cache resident, simply replaces the row in cache with\nthe updated x. Otherwise, one of two scenarios happen that\nis determined by the cache policy in effect. Either x does\nnot have priority to evict the current cache content whose\nspace it con\ufb02icts with, or that it does. In the former, x is\nplaced in the cache verbatim while the evicted row is placed\nin the table T after being down converted. In the latter, one\nsimply down converts x and places it in T.\nOur design allows four knobs as follows.\n\u2022 Precision: The embedding table T contains entries in\nprecision lower than FP32. We allow the IEEE 16-bit\n\ufb02oating-point values FP16 as well as quantization into\nINT8, INT4, as well as INT2 precision. For integer\nquantization, each row of the embedding values share\none pair of quantization parameters (s, b), scale and\nbias, represented in FP32. We use row-wise uniform\nmin-max quantization in mapping real values to un-\nsigned quantization domain:\nxquantize = x \u2212b\ns\nwhere b=min(x) and s=(max(x)-min(x))/(2N \u22121). The\nquantization parameters are chosen so that the quan-\ntized embedding values will be in the corresponding\nrange of the unsigned integer datatype: [0, 3], [0, 15]\nand [0, 255] for INT2, INT4 and INT8 quantization,\nrespectively.\n\u2022 Rounding: When a FP32 value has to be placed\ninto a low-precision embedding table, a precision\ndown conversion occurs. Let x be the FP32 value\nin question. There is a pair of neighboring numbers\n(x\u2212, x+) in the quantized low-precision domain such\nthat x\u2212\u2264x \u2264x+ (that is, they differ by one \u201cunit in\nSubmission and Formatting Instructions for MLSys 2020\nlast place\u201d in FP16, or are consecutive integers in the\ncase of quantization). We allow two rounding options:\nround to nearest and stochastic rounding. If round to\nnearest is used, then the number in {x\u2212, x+} that is\nclosest to x is returned. In case of a tie, the one with\neven parity (least signi\ufb01cant bit equals 0) is chosen. If\nstochastic rounding is used, then one of x\u2212and x+ is\nchosen randomly with a Bernoulli distribution so that\nthe expected value (average) is in fact x.\n\u2022 Cache Structure: Let an uniform-dimension embed-\nding table T has N rows with indices from 0 to N \u22121,\nand each row has dimension [1, d]. The cache C has n\nsets each of which can hold \u03b1 FP32 embedding rows.\nThe cache size is thus 4\u03b1nd bytes. We restrict our-\nselves to \u03b1 \u22651 being an integral power of 2 and cache\nsizes chosen such that \u03b1, n are integers. A hash func-\ntion\nh : {0, 1, . . . , N \u22121} \u2192{0, 1, . . . , n \u22121}\nis chosen and row i of T is mapped to the set h(i) of\nthe cache. When \u03b1 = 1, we have a direct mapped\ncache; otherwise, we have a set associative cache.\n\u2022 Cache Replacement Policy: When we attempt to\nplace a non-cache resident row of index i into the cache\nand the set h(i) is already fully occupied, the replace-\nment policy dictates the appropriate action. We main-\ntain a priority value of each row which can be either\naccess frequency or timestamp of last access, depend-\ning on the replacement policy. We also keep track of\nthe lowest priority value of the current cache residents\nfor each set s, 0 \u2264s < n. When the FP32 Row-i is to\nbe stored, our policy puts the row in the full-precision\ncache if and only if its priority value is higher than the\nlowest priority value of the residents. In this case, the\nlowest priority row in the cache will be evicted \u2013 down\nconverted by a rounding method of choice and placed\nback into the table T. Otherwise, Row-i bypasses the\ncache, and after it is updated in FP32, is put into T\nwith down conversion. We have two speci\ufb01c policies\nspeci\ufb01ed by their respective priority value calculations.\n1. Least Frequently Used (LFU). Each embedding\nrow carries with it an access count as the priority\nvalue. Access count is a popularity measure; LFU\npolicy naturally let the more popular rows main-\ntain higher precision based on the assumption that\nthe more frequently used rows pull heavier weight\non model accuracy. One drawback of this policy\nis that access count requires extra memory per\nrow, whose size grows linearly with the number\nof rows in embeddings.\n2. Least Recently Used (LRU). Each cached row\ncarries the last access timestamp as the priority\nvalue. This policy lets those rows that are of-\nten accessed within a time window to maintain\nhigher precision in that duration. Another advan-\ntage over LFU is that this priority value needs\nnot be explicitly maintained at all in the case of a\ndirect mapped cache, as the cache resident entry\nis always evicted in case of a con\ufb02ict.\nWe will report on our experiments and results in the next\nsections.\n4\nCPU EXPERIMENTS WITH DLRM AND\nKAGGLE DATASET\nWe use a CPU emulation to explore our algorithm design\nspace with an open source model and dataset. Section 5\nwill discuss an implementation in a GPU training system\nand performance evaluation with an industrial-scale internal\nmodel and dataset. The main emulation aspect here is that\nwe do not keep two physical spaces for the embedding table\nand the cache, but rather use a single FP32 array to emulate\na single low-precision table with FP32 cache. We use the\nsimple technique of fake quantization, detailed below, to\nmaintain faithful behavior of low-precision numerics and\nexperimented with all combinations of the four precisions\nand two rounding methods (discussed in section 3) on each\nof the following cache settings: direct-mapped cache with\nLFU and LRU policies and set associative LFU cache with\nassociativity \u03b1 = 2k, 1 \u2264k \u22645.\n4.1\nLow-Precision Emulation\nWe implemented a custom sparse AdaGrad optimizer that\nuses one high-precision tensor to store the embedding\nweights, and keep a list of row indices of the current cache\nresidents. All embedding gradient updates are applied in\nfull precision. When Row-i in table T is supposed to be\nstored in low precision, we apply fake quantization (quanti-\nzation followed by dequantization) to row T[i, :] to provide\na numerically faithful emulation: In the case of FP16, fake\nquantization of a FP32 value x is simply the operation\nFP16 to FP32 (FP32 to FP16(x)) .\nWith the above conversion, while the result physically re-\nmains in FP32, its precision is that of FP16. Fake inte-\nger quantization of an FP32 value x similarly returns an\nFP32 object but whose precision is the same as if integer\nquantization was performed. Algorithm 1 shows the fake\nquantization details.\n4.2\nCache Implementations\nNumerical faithfulness is paramount in our study; thus we\nshow the exact numerical steps on our single-array emula-\ntion of an embedding table and its cache. Here we have a\nSubmission and Formatting Instructions for MLSys 2020\nAlgorithm 1 Fake Row-wise Integer IntN Quantization\nParameters: Uniform Min-Max\nInput:\nembedding row r, embedding dimension d,\nbitwidth N\nCompute b \u2190min(r), s \u2190(max(r) \u2212b)/(2N \u22121)\nfor i = 0, 1, ..., d \u22121 do\nq \u2190round to int((r[i]\u2212b)/s) (nearest or stochastic)\nr[i] \u2190q \u00d7 s + b\nend for\nsingle table T where some rows correspond to cache resi-\ndents in FP32 precision and the others non-cache residents\nin the corresponding low precision.\nAt any given iteration, let IU be the set of embedding row\nindices to be updated by a scaled gradient, U. Let PV [k]\nbe the priority value for the embedding row k, either for\nLRU or LFU. Note that in the case of \u03b1 = 1 (direct-mapped\ncache) for LRU, PV is not required as all accessed rows will\nbe the most recent by default and no priority comparison is\nnecessary. Tag indices IC are maintained to keep the row\nids of cache residents for each of the \u03b1 rows residing in the\nset h(i), with h being the hash function. Algorithm 2 gives\nthe numerical details of our cache implementation.\nAlgorithm 2 Update \u03b1-Associative Cache\nInput: embedding T, tag indices IC of current cached\nrows, indices to update IU, hash function h, gradient\nupdate U, priority value PV (non existent for \u03b1 = 1,\ndirect-mapped LRU)\nfor i in IU do\nUpdate PV [i] according to LRU or LFU\ns \u2190h(i)\nT[i, :] \u2190T[i, :] + U[i, :] # apply gradient updates\nif i /\u2208IC[s] then\nj \u2190argmin{PV [k]|k \u2208IC[s]}\nif PV [i] \u2264PV [j] (False if \u03b1 = 1 and LRU) then\n# bypass\nT[i, :] \u2190fake quantize(T[i, :])\nelse\n# replace\nT[j, :] \u2190fake quantize(T[j, :])\nreplace j by i in the set IC[s]\nend if\nend if\nend for\n4.3\nExperiments and Results\nWe\nuse\nDeep\nLearning\nRecommendation\nModel\n(DLRM) (Naumov et al., 2019) with Criteo-Kaggle\n7D Ads Display Challenge dataset1 (Criteo AI Lab, 2014)\nfor model accuracy evaluations with different low-precision\nand cache implementations.\nThe accuracy metric is\nbased on the relative change to the test accuracy of the\nresulting low-precision models using the full FP32 model\nas reference. The precise de\ufb01nition of our metric is\nAccuracy Drop % = AccFP32 \u2212Acclowprec\nAccFP32\n\u00b7 100\n(1)\nIn particular, if the low-precision model in fact has higher\ntest accuracy, our metric becomes negative. Our goal is to\nkeep this number below roughly 0.02%.\nOur benchmark model has 26 embedding tables with the\nfollowing sizes shown in Table 1. We con\ufb01gure all 26\nembedding tables to use row dimension 128 (total 4.3B\nparameters), and only apply low-precision with or without\ncaching technique to tables with more than 1K rows for all\nexperiments. Embedding tables with less than 1K rows are\ntrained in FP32.\nTable 1. DLRM benchmark model embedding sizes\n4\n4\n11\n16\n18\n24\n28\n105\n306\n584\n634\n1,461\n2,173\n3,195\n5,653\n5,684\n12,518\n14,993\n93,146\n142,572\n286,181\n2,202,608\n5,461,306\n7,046,547\n8,351,593\n10,131,227\n4.3.1\nAccuracy Recovery with High-precision Cache\nWe \ufb01rst evaluate accuracy loss of training with low-precision\nembeddings without a high-precision cache. Table 2 shows\nthe relative test accuracy drop of training embeddings in\nvarious low precisions with nearest and stochastic rounding.\nWe observe that neutral accuracy can be achieved with FP16\nwith stochastic rounding without high-precision caching;\nbut accuracy decreases signi\ufb01cantly with progressively nar-\nrower bitwidth. The accuracy boost from stochastic round-\ning is consistent with previous work on low-precision em-\nbedding training (Zhang et al., 2018), but not nearly enough\nfor embedding tables represented by 8 or fewer bits. This\nloss of accuracy was part of the motivation for our high-\nprecision cache.\nTable 2. Test Accuracy Drop without Cache in %\nFP16\nINT8\nINT4\nINT2\nNEAREST\n0.047\n0.549\n1.080\n1.454\nSTOCHASTIC\n-0.010\n0.077\n0.591\n1.037\n1http://labs.criteo.com/2014/02/kaggle-display-advertising-\nchallenge-dataset/\nSubmission and Formatting Instructions for MLSys 2020\nWe experiment with direct-mapped high-precision LFU,\nLRU cache, and set associative LFU cache with varying\nsizes: 5%, 10%, 30%, and 50% (of the original FP32 table)\non top of low-precision tables with nearest/stochastic round-\ning. 32-Way LFU gives the best accuracy results and as\nshown in Table 3: we achieve neutral accuracy with INT8,\nINT4, and INT2 embeddings (with stochastic rounding) at\ncache sizes of 5%, 30%, and 50%, respectively.\nTable 3. High-precision 32-Way LFU cache with varying cache\nsizes recover accuracy of low-precision embedding tables\nCACHE SIZE\n% OF\nTEST ACCURACY DROP IN %\nROUNDING: NEAREST/STOCHASTIC\nTABLE\nINT8\nINT4\nINT2\n5%\n0.074/-0.014\n0.251/0.110\n0.376/0.275\n10%\n0.033/-0.013\n0.157/0.073\n0.257/0.196\n30%\n0.000/-0.021\n0.043/-0.009\n0.086/0.077\n50%\n-0.005/-0.008\n0.018/-0.004\n0.042/0.025\nTables 3, 4, and 5 show that the presence of a high-precision\ncache with as low as 5% capacity of the original table size,\nregardless of cache replacement policy, recovers accuracy\nsigni\ufb01cantly for various lower precision levels with either\nnearest or stochastic rounding. With 5% high-precision\ncache, we recover 70-80% of accuracy drop from low-\nprecision embeddings for 8 bits and lower.\nTable 4. High-precision LRU cache with varying cache sizes re-\ncover accuracy of low-precision embedding tables\nCACHE\nSIZE\n% OF\nTEST ACCURACY DROP IN %\nCACHE: LRU, DIRECT MAPPED\nROUNDING: NEAREST/STOCHASTIC\nTABLE\nFP16\nINT8\nINT4\nINT2\n5%\n0.005/-0.005\n0.180/0.010\n0.520/0.259\n0.602/0.872\n10%\n0.003/0.004\n0.144/0.008\n0.430/0.212\n0.761/0.536\n30%\n0.004/-0.008\n0.093/0.006\n0.321/0.164\n0.582/0.425\n50%\n0.006/-0.016\n0.043/-0.004\n0.246/0.143\n0.462/0.352\nTable 5. High-precision LFU cache with varying cache sizes re-\ncover accuracy of low-precision embedding tables\nCACHE\nSIZE\n% OF\nTEST ACCURACY DROP IN %\nCACHE: LFU, DIRECT MAPPED\nROUNDING: NEAREST/STOCHASTIC\nTABLE\nFP16\nINT8\nINT4\nINT2\n5%\n-0.006/-0.001\n0.087/0.014\n0.333/0.147\n0.533/0.423\n10%\n-0.011/-0.003\n0.063/0.008\n0.270/0.138\n0.430/0.375\n30%\n-0.004/-0.003\n0.044/0.009\n0.175/0.090\n0.280/0.243\n50%\n-0.018/-0.008\n0.014/0.008\n0.136/0.056\n0.217/0.193\nAlthough larger cache sizes recover more accuracy in Tables\n4 and 5, we observe a diminishing return of value in their\ncontinued increase. Plotting in Figure 2 the 16 columns of\ndata in Tables 4 and 5 demonstrate the trend.\n4.3.2\nReplacement policy\nThe previous section shows that the presence of a high-\nprecision cache recovers signi\ufb01cantly the accuracy lost due\nCache Size\nAccuracy Drop\n-0.5\n0.0\n0.5\n1.0\n1.5\n0%\n10%\n20%\n30%\n40%\nFP16\nINT8\nINT4\nINT2\nCache Size\nAccuracy Drop\n-0.2\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n0%\n10%\n20%\n30%\n40%\nFP16\nINT8\nINT4\nINT2\n(a) LRU nearest rounding\n(b) LRU stochastic rounding\nCache Size\nAccuracy Drop\n-0.\n0.0\n0.5\n1.0\n1.5\n0%\n10%\n20%\n30%\n40%\nFP16\nINT8\nINT4\nINT2\nCache Size\nAccuracy Drop\n-0.2\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n0%\n10%\n20%\n30%\n40%\nFP16\nINT8\nINT4\nINT2\n(c) LFU nearest rounding\n(d) LFU stochastic rounding\nFigure 2. Diminishing return of accuracy recovered vs. cache sizes\nFigure 3. CDF of sorted row access counts vs. fraction of rows\nto embeddings trained exclusively in low precisions. When\nan FP32 cache is present, rows updated while in cache are\ncomputed in full FP32 precision. Thus any cache policy that\npromotes more updates to cache residents improve accuracy.\nThis motivates us to examine the comparative number of\nupdates each row receives during training.\nWe collected access counts for each row, and plotted cu-\nmulative distribution of row counters sorted in descending\norder. Figure 3 shows four of the representative distribution\nplots out of 15 embeddings with more than 1K rows. For\nCriteo-Kaggle dataset, all large embedding tables share the\nproperty that a small fraction (< 20%) of rows are respon-\nsible for a large fraction (> 80%) of total accesses; some\nembedding tables show an even greater skew.\nWe now consider the relative merits of different cache re-\nSubmission and Formatting Instructions for MLSys 2020\nTable 6. Cache hit rates for varying replacement policies and sizes\n(relative to embedding tables)\nCACHE SIZE\nLRU\nLFU\n32-WAY LFU\n5%\n68.22%\n75.65%\n77.66%\n10%\n75.18%\n81.21%\n83.63%\n30%\n85.66%\n89.07%\n92.45%\n50%\n90.12%\n92.43%\n96.13%\nplacement policies in light of the inequalities in access fre-\nquency. If the access pattern that produces these access\ncount is non-stationary (Agarwal et al., 1989) and highly\nphased, a direct-mapped LRU cache can be effective in pro-\nmoting cache hit rate. An advantage is that direct-mapped\nLRU is the easiest to implement among all the choices in\nAlgorithm 2. On the other hand, if the access pattern is sta-\ntionary, then the relative priority rankings among the rows\nbased on access counts will more or less stabilize after some\ninitial stages of training. Consequently, an LFU policy in\ngeneral promotes the highly accessed rows be cache resi-\ndent, increasing the number of cache hit. Along this line,\nan associative cache, with LFU policy in our case, is well\nknown to improve cache hit rate further.\nTable 6 reports the cache hit rate statistics of various cache\ncon\ufb01gurations. 32-way LFU provides noticeable increase in\nhit rate than direct-mapped LFU, while direct-mapped LFU\nhas higher hit rates than direct-mapped LRU. These statistics\nare consistent with the hypothesis that the current model\u2019s\nembedding access pattern is somewhat stationary on the\ndata set in question, making LFU and in particular with set-\nassociative highly effective. The resulting model accuracy is\nwell correlated positively with hit rate, as shown in Figures\n4 and 5. Figure 4 shows that LFU outperforms LRU when\nboth are direct mapped; Figure 5 shows the superiority of\nset-associative LFU. That said, Figure 6 shows that accuracy\ngain from increased set associativity plateaus after a while,\nsuccumbing to the law of diminishing return.\n4.3.3\nResults Summary\nWe summarize our CPU experiments with DLRM and\nCriteo-Kaggle dataset.\nMost importantly, the use of a\nlow-precision embedding table in conjunction with a high-\nprecision cache is effective for maintaining accuracy as we\nhave demonstrated. Stochastic rounding helps maintain\ntraining accuracy on low-precision embedding tables. We\nalso see that set associative cache architecture is prefer-\nable to direct mapped, and that the LFU replacement policy\nserves this model/dataset better than LRU.\nConcentrating then on a 32-way associative LFU cache with\nstochastic rounding, Figure 7 plots the best accuracy ob-\ntained at each memory compression factor with or without\nCache Size\nAccuracy Drop\n0.0\n0.2\n0.4\n0.6\n5%\n10%\n30%\n50%\nDM LFU\nDM LRU\nCache Size\nAccuracy Drop\n0.0\n0.1\n0.2\n0.3\n5%\n10%\n30%\n50%\nDM LFU\nDM LRU\n(a) INT4 nearest rounding\n(b) INT4 stochastic rounding\nCache Size\nAccuracy Drop\n0.0\n0.2\n0.5\n0.7\n1.0\n5%\n10%\n30%\n50%\nDM LFU\nDM LRU\nCache Size\nAccuracy Drop\n0.0\n0.2\n0.4\n0.6\n0.8\n5%\n10%\n30%\n50%\nDM LFU\nDM LRU\n(c) INT2 nearest rounding\n(d) INT2 stochastic rounding\nFigure 4. Direct-mapped LFU vs. LRU\nCache Size\nAccuracy Drop\n0.0\n0.2\n0.4\n0.6\n5%\n10%\n30%\n50%\n32-way LFU\nLFU\nLRU\nCache Size\nAccuracy Drop\n-0.1\n0.0\n0.1\n0.2\n0.3\n5%\n10%\n30%\n50%\n32-way LFU\nLFU\nLRU\n(a) INT4 nearest rounding\n(b) INT4 stochastic rounding\nCache Size\nAccuracy Drop\n0.0\n0.2\n0.5\n0.7\n1.0\n5%\n10%\n30%\n50%\n32-way LFU\nLFU\nLRU\nCache Size\nAccuracy Drop\n0.0\n0.2\n0.4\n0.6\n0.8\n5%\n10%\n30%\n50%\n32-way LFU\nLFU\nLRU\n(c) INT2 nearest rounding\n(d) INT2 stochastic rounding\nFigure 5. 32-Way Associative LFU vs. Direct-mapped LFU vs.\nLRU\na high-precision cache. Memory compression factor with\nnon-cached low-precision embedding is calculated from\nbitwidths, taking into quantization parameter storage over-\nhead, i.e., compression for FP16 and INT8 is 0.5 and 0.265,\nrespectively. Compression factor with cached implemen-\ntation is calculated from bitwidths and cache sizes, taking\ninto account quantization parameter storage overhead and\ncaching overhead such as access counters and tags: i.e., for\nINT8 embedding with 10% high-precision LFU cache, its\nmemory compression factor is 0.374, or a 2.7\u00d7 memory\ncompression. Please refer to Appendix A for more details\nfor computing the memory compression factor.\nModel accuracy degrades with increasingly large compres-\nsion factors introduced by low-precision embeddings regard-\nless of the presence of a high-precision cache. However with\nSubmission and Formatting Instructions for MLSys 2020\nAssociativity\nAccuracy Drop\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n5\n10\n15\n20\n25\n30\nINT4 Nearest\nINT4 Stochastic\nINT2 Nearest\nINT2 Stochastic\nAssociativity\nAccuracy Drop\n0.0\n0.1\n0.2\n0.3\n0.4\n5\n10\n15\n20\n25\n30\nINT4 Nearest\nINT4 Stochastic\nINT2 Nearest\nINT2 Stochastic\n(a) 5% cache\n(b) 10% cache\nAssociativity\nAccuracy Drop\n-0.0\n0.00\n0.05\n0.10\n0.15\n0.20\n5\n10\n15\n20\n25\n30\nINT4 Nearest\nINT4 Stochastic\nINT2 Nearest\nINT2 Stochastic\nAssociativity\nAccuracy Drop\n-0.0\n0.00\n0.02\n0.04\n0.06\n0.08\n5\n10\n15\n20\n25\n30\nINT4 Nearest\nINT4 Stochastic\nINT2 Nearest\nINT2 Stochastic\n(c) 30% cache\n(d) 50% cache\nFigure 6. Diminishing return with associativity for set associative\nLFU cache\nMemory Compression\nAccuracy Drop\n-0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.30\n0.35\n0.40\n0.45\n0.50\nw/ Cache\nw/o Cache\nFigure 7. Accuracy vs. memory compression factor with and with-\nout high-precision cache on Criteo-Kaggle dataset. Please refer to\nFigure 10 in Appendix A for the complete \ufb01gure.\nthe addition of a high-precision cache, we effectively enable\n\ufb01ne-grained tradeoffs between accuracy and memory foot-\nprint compression via varying cache sizes and precisions. A\nsweet spot here is a 3\u00d7 memory compression (INT8 and 5%\ncache) while maintaining neutral accuracy. For clarity to\ndemonstrate the accuracy improvement near 0.02% that we\ntypically consider neutral, Figure 7 plots parts of our results,\nalthough the trend demonstrated holds true for lower preci-\nsion than plotted in the \ufb01gure. Please refer to Appendix A\nfor memory compression \ufb01gure with complete results that\nshow a general trend extending into lower precision.\n5\nGPU EXPERIMENTS WITH AN\nINDUSTRY-SCALE MODEL AND DATASET\nA GPU implementation enables us to validate our approach\non larger datasets. We match the features of the CPU ex-\nperimentation: supporting real FP16 and emulations of\nINT8/4/2 with rounding-to-nearest and stochastic round-\ning modes.\n5.1\nImplementations\nWe made the following changes to fully utilize the GPU\narchitecture.\n\u2022 32-way only: We implemented 32-way set-associative\ncache for: 1) our CPU results showed accuracy im-\nprovements over direct-mapped cache and 2) it matches\nwell with the GPU\u2019s warp size of 32, hence achieving\ntraining speed similar to that of the direct-mapped case.\n\u2022 De-duplicate gradients: Unlike the Kaggle dataset\nused for CPU evaluation with one-hot embedding ac-\ncesses, the large dataset used for GPU evaluation has\nmulti-hot embedding accesses. Therefore, there is a\nhigher chance that multiple examples in a mini-batch\naccess the same rows during sparse optimizer, leading\nto concurrent update races especially in GPUs with\nhighly parallel execution that typically requires a large\nbatch size. We avoid such races by sorting row indices\nin each mini-batch and merging gradient updates for\nthe same rows into one update. Suppose one input in\nthe mini-batch accesses rows 1 and 2 with gradient g1,\nand another input accesses rows 2 and 3 with gradi-\nent g2. We sort by the rows, de-duplicate gradients\n(e.g., g1 + g2 for row 2), and then apply the dedup\u2019ed\ngradients.\n5.2\nExperiments and Results\nTo validate the feasibility of a high-precision cache on a\nlarger model with larger dataset, we evaluate an industrial\nscale DLRM-like recommendation model for one ranking\napplication at [institution name removed for double-blind re-\nview] on an internal training dataset. The size of the model\nand dataset are more than 10\u00d7 bigger than the DLRM model\nand Kaggle dataset evaluated in the previous section, respec-\ntively. Model accuracy is measured by Normalized-Entropy\n(NE) (He et al., 2014) metric. Lower NE corresponds to\nhigher prediction accuracy for a recommendation model.\nSimilar to the DLRM evaluation on CPUs, we adopt rela-\ntive accuracy drop in Equation 1, with the goal to keep the\nthreshold below 0.02%.\n5.2.1\nAccuracy evaluation\nTable 7 shows the accuracy drop without cache. Similar\nto the CPU experiments, FP16 can achieve neutral accu-\nracy. However, accuracy drops more steeply with narrower\nbit-width of INT8/4/2, and doesn\u2019t even converge without\nstochastic rounding.\nThen, we apply the 32-way high-precision LFU and LRU\nSubmission and Formatting Instructions for MLSys 2020\nTable 7. Accuracy Drop without Cache in %. Note that \u201dN/A\u201d\ndenotes the training cannot converge, resulting not a number (NaN)\naccuracy.\nFP16\nINT8\nINT4\nINT2\nNEAREST\n0.001\nN/A\nN/A\nN/A\nSTOCHASTIC\n-0.023\n4.254\n5.200\n4.673\nTable 8. High-precision LRU cache with varying cache sizes re-\ncover accuracy of low-precision embedding tables.\nCACHE\nSIZE\n% OF\nACCURACY DROP IN %\nCACHE: 32-WAY LRU ON GPUS\nROUNDING: NEAREST/STOCHASTIC\nTABLE\nFP16\nINT8\nINT4\nINT2\n0.1%\n-0.010/-0.024\n0.006/0.022\n0.053/0.010\n0.329/0.135\n1%\n0.004/-0.027\n-0.007/0.004\n0.002/-0.001\n0.134/0.095\n10%\n-0.001/-0.030\n-0.011/0.014\n-0.011/-0.021\n0.024/0.028\nTable 9. High-precision LFU cache with varying cache sizes re-\ncover accuracy of low-precision embedding tables\nCACHE\nSIZE\n% OF\nACCURACY DROP IN %\nCACHE: 32-WAY LFU ON GPUS\nROUNDING: NEAREST/STOCHASTIC\nTABLE\nFP16\nINT8\nINT4\nINT2\n0.1%\n0.002/-0.029\n0.041/0.129\n0.112/0.133\n0.526/0.278\n1%\n0.011/-0.022\n0.038/0.081\n0.065/0.081\n0.389/0.189\n10%\n0.003/-0.017\n-0.003/0.011\n0.002/0.009\n0.057/0.042\ncache with different sizes: 0.1%, 1%, and 10% of the origi-\nnal table size, for large embedding tables that collectively\naccount for more than 97% of the total model size. Tables 8\nand 9 demonstrate that on the large training dataset, both\nLFU and LRU can achieve better accuracy for various pre-\ncisions with nearest/stochastic rounding. Unlike the results\nwith DLRM and Kaggle dataset, LRU performs generally\nbetter than LFU. This will be explained in the next section.\n5.2.2\nCache hit rate analysis\nTo understand the data reuse pattern of the large training\ndataset, we collected the cache hit rate statistics for different\ncache sizes and replacement policies (Table 10). Contrary to\nthe Criteo-Kaggle 7D Ads display challenge dataset, when\nTable 10. Cache hit rates for varying replacement policies and sizes\n(relative to embedding tables). Caching is applied to two sets of\ntables. The \ufb01rst set is 5% of embedding tables accounting for\n97.1% of total model size.\nCACHE\n5% OF TABLES\n55% OF TABLES\nSIZE\n97.1% OF SIZE\n99.9% OF SIZE\nIN %\nLRU\nLFU\nLRU\nLFU\n0.1%\n10.52%\n2.41%\n33.50%\n74.97%\n1%\n34.72%\n13.41%\n94.06%\n96.11%\n10%\n59.86%\n57.31%\n99.96%\n99.95%\nMemory Compression\nAccuracy Drop\n-0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.1\n0.2\n0.3\n0.4\n0.5\nw/ Cache\nw/o Cache\nINT8\nFigure 8. Accuracy vs. memory compression factor with and with-\nout high-precision cache for large training dataset.\nwe apply caching to 5% of tables that account for 97% of\ntotal model size, LRU performs consistently better than LFU\nfor the internal training dataset, especially for smaller cache\nsizes. However, if we apply caching to more tables, LFU\nperforms better than LRU. One reason for this difference is\nthe \u201cdata shift\u201d pattern in the internal dataset for a few large\ntables, where some entity ids are popular for a continued\nperiod of time exhibiting temporal locality but later don\u2019t\nappear as frequently.\n5.2.3\nMemory Reduction\nSimilar to the analysis in Section 4.3.3, we calculate the\nmemory compression factor for both non-cached and cached\nimplementations of low-precision embeddings on GPUs,\ntaking into account quantization parameter storage over-\nhead and cache-speci\ufb01c access counters as well as tags. For\nexample, the memory compression factor for INT8 embed-\nding with 1% high-precision LRU cache is 0.259. Note that\nLFU takes a bit more storage than LRU implementations\nsince LFU needs to maintain the extra counters to record\nthe frequency.\nFigure 8 shows the correspondences between accuracy and\nmemory reduction with and without cache. For clarity to\ndemonstrate accuracy differences for the settings that give\nclose to neutral accuracy, we only plot parts of our results\nin Figure 8. The trend demonstrated holds true for lower\nprecision that are not shown in the \ufb01gure as well.\nFor example, with the accuracy drop threshold 0.02%, we\ncan use INT4 stochastic rounding embedding with 1% high-\nprecision cache with 86.6% memory savings on GPUs (more\nthan 7\u00d7 memory reduction), which signi\ufb01cantly mitigates\nthe limited on-device high-bandwidth memory (HBM) ca-\npacity of GPUs.\nSubmission and Formatting Instructions for MLSys 2020\nCache Hit Rate\nEffectective BW (GB/s)\n0\n200\n400\n600\n800\n20.00%\n40.00%\n60.00%\n80.00%\n100.00%\nFP32 Device\nFP16 Nearest\nFP16 Stochastic\nFP32 UVM\n(a) LRU, forward. FP16 Near./Stoc. overlap with each other.\nCache Hit Rate\nEffectective BW (GB/s)\n0\n200\n400\n600\n20.00%\n40.00%\n60.00%\n80.00%\n100.00%\nFP32 Device\nFP16 Nearest\nFP16 Stochastic\nFP32 UVM\n(b) LRU, backward\nFigure 9. Effective bandwidth for FP32 (for embedding table on\ndevice and on uni\ufb01ed memory) vs. FP16 with high-precision LRU\ncache. The effective bandwidth of LFU is similar.\n5.2.4\nSpeed improvement\nWhen embedding tables are too big to \ufb01t on-device HBM\nmemory of GPUs, one should use mechanisms like uni\ufb01ed\nmemory (UVM, (NVIDIA, 2020). However, UVM operates\nin a granularity considerably bigger than embedding table\nrows with non-contiguous access pattern, signi\ufb01cantly wast-\ning PCIe bandwidth. By using low-precision embedding\ntables with high-precision cache, we can either (1) reduce\nthe size of cache plus embedding table small enough to \ufb01t\nthe both entirely within device memory, or (2) still put the\nlow-precision embedding table in the uni\ufb01ed memory but\nreducing PCIe traf\ufb01c.\nWe want to make sure that our GPU cache implementation\nis fast enough to outperform UVM when cache hit rate is\nlow, and saturates a large fraction of peak device memory\nbandwidth when hit rate is high. This is demonstrated by\nFigure 9.\nFigure 9 shows micro-benchmark results evaluated on a\nNvidia V100 GPU, where a large low-precision embedding\ntable is allocated on the uni\ufb01ed memory (UVM) and a high-\nprecision cache is allocated on the device memory. We\nplot the effective bandwidth (in GB/s) in correspondence to\ndifferent cache hit rate for LRU cache during the forward\nand backward passes . The top and bottom lines show the\nachieved bandwidth of the forward/backward path on the\ndevice memory and on the uni\ufb01ed memory, which corre-\nspond to the upper and lower bound of the bandwidth. Note\nthat the bandwidth of the device memory is bounded by\nthe HBM peak bandwidth (900 GB/s), while the bandwidth\nof the uni\ufb01ed memory is bounded by the PCIe bandwidth.\nWhen the hit rate is high, we get close to the device band-\nwidth. For lower hit rate, the LRU/LFU implementation\nis still reasonably faster than the bandwidth achieved by\nuni\ufb01ed memory.\nWe also measure an end-to-end training speedup of 16% us-\ning FP16 LRU cache (cache size is 1% of embedding tables)\nover the baseline where UVM is used for large embedding\ntables. This is an example that our technique not only saves\nmemory but can also improve the training speed.\n6\nCONCLUSIONS AND FUTURE WORK\nIn this paper we presented low-precision embedding table\nwith a high-precision cache as an effective memory saving\ntechnique for training large-scale recommendation models,\nparticularly well suited for systems with high compute band-\nwidth but limited capacity memory like GPUs. To the best of\nour knowledge, this is the \ufb01rst time mixed-precision training\nwith a high-precision cache is applied on an industrial-scale\nrecommendation system. Our best results include reducing\nmemory by over 7\u00d7 for a large embedding trained in INT4\nprecision while maintaining neutral accuracy.\nAs part of future work, we will further experiment with het-\nerogeneous precision and replacement policy assignment for\ndifferent embeddings. We will potentially assign different\nprecision and cache policy for each embedding based on\na light-weight pro\ufb01le in the \ufb01rst few training iterations for\nthe best accuracy. This can be useful to handle dataset with\ndifferent characteristics as we have seen from the Kaggle\nand our internal dataset. We will further explore different\nhash functions and replacement heuristics, and evaluate their\nimpact on model accuracy and performance. Our technique\ncan be applied to other models with embeddings such as\nlanguage models. Once proven to work for a wider range of\nmodels, it can be interesting to add hardware support by aug-\nmenting low-precision conversion to the existing hardware\ncache mechanisms.\nSubmission and Formatting Instructions for MLSys 2020\nREFERENCES\nAgarwal, A. A., Hennessy, J. L., and Horowitz, M. H. An\nanalytical cache model. ACM Transactions on Computer\nSystems, 1989.\nAlvarez, J. M. and Salzmann, M. Compression-aware train-\ning of deep networks. In Proc. Advances in Neural Infor-\nmation Processing Systems, pp. 856\u2013867, 2017.\nAndrews, M.\nCompressing word embeddings.\nCoRR,\nabs/1511.06397, 2015.\nBarz, B. and Denzler, J. Hierarchy-based image embeddings\nfor semantic image retrieval. In Proc. IEEE Winter Conf.\nApplications of Computer Vision, pp. 638\u2013647. IEEE,\n2019.\nBhavana, P., Kumar, V., and Padmanabhan, V. Block based\nsingular value decomposition approach to matrix factor-\nization for recommender systems. Pattern Recognition\nLetters, abs/1907.07410, 2019.\nChen, X., Hu, X., Zhou, H., and Xu, N. Fxpnet: Train-\ning a deep convolutional neural network in \ufb01xed-point\nrepresentation. In IJCNN, 2017.\nCheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra,\nT., Aradhye, H., Anderson, G., Corrado, G., Chai, W.,\nIspir, M., Anil, R., Haque, Z., Hong, L., Jain, V., Liu, X.,\nand Shah, H. Wide and deep learning for recommender\nsystems. In RecSys, 2016.\nChiu, B., Crichton, G., Korhonen, A., and Pyysalo, S. How\nto train good word embeddings for biomedical nlp. In\nProc. 15th Workshop on Biomedical Natural Language\nProcessing, pp. 166\u2013174, 2016.\nCriteo AI Lab. Display Advertising Challenge.\nhttp://labs.criteo.com/2014/02/\nkaggle-display-advertising-challenge\\\n-dataset/, 2014. Accessed: 2020-10-08.\nDe Sa, C., Leszczynski, M., Zhang, J., Marzoev, A.,\nAberger, C. R., Olukotun, K., and R\u00b4e, C. High-accuracy\nlow-precision training. arXiv preprint arXiv:1803.03383,\n2018.\nElthakeb, A. T., Pilligundla, P., and Esmaeilzadeh, H.\nSinReQ: Generalized sinusoidal regularization for au-\ntomatic low-bitwidth deep quantized training. CoRR,\nabs/1905.01416, 2019.\nFan, A., Stock, P., Graham, B., Grave, E., Gribonval, R.,\nJ\u00b4egou, H., and Joulin, A. Training with quantization\nnoise for extreme model compression. arXiv preprint\narXiv:2004.07320, 2020.\nFrankle, J. and Carbin, M.\nThe lottery ticket hypothe-\nsis: Finding sparse, trainable neural networks. CoRR,\nabs/1803.03635, 2018.\nGinart, A., Naumov, M., Mudigere, D., Yang, J. Y., and\nZou, J. Mixed dimension embeddings with application\nto memory-ef\ufb01cient recommendation systems.\narXiv\npreprint arXiv:1909.11810, 2019.\nGuan, H., Malevich, A., Yang, J., Park, J., and Yuen, H. Post-\ntraining 4-bit quantization on embedding tables. arXiv\npreprint arXiv:1911.02079, 2019.\nGupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan,\nP. Deep learning with limited numerical precision. In\nICML, 2015.\nHazelwood, K., Bird, S., Brooks, D., Chintala, S., Diril, U.,\nDzhulgakov, D., Fawzy, M., Jia, B., Jia, Y., Kalro, A.,\nLaw, J., Lee, K., Lu, J., Noordhuis, P., Smelyanskiy, M.,\nXiong, L., and Wang, X. Applied machine learning at\nFacebook: A datacenter infrastructure perspective. In\nProc. IEEE Int. Symposium on High Performance Com-\nputer Architecture, pp. 620\u2013629, 2018.\nHe, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., Shi, Y.,\nAtallah, A., Herbrich, R., Bowers, S., et al. Practical\nlessons from predicting clicks on ads at facebook. In\nProceedings of the Eighth International Workshop on\nData Mining for Online Advertising, pp. 1\u20139, 2014.\nKalamkar, D., Mudigere, D., Mellempudi, N., Das, D.,\nBanerjee, K., Avancha, S., Vooturi, D. T., Jammala-\nmadaka, N., Huang, J., Yuen, H., Yang, J., Park, J.,\nHeinecke, A., Georganas, E., Srinivasan, S., Kundu, A.,\nSmelyanskiy, M., Kaul, B., and Dubey, P. A study of\nBFLOAT16 for deep learning training. arXiv preprint\narXiv:1905.12322, 2019.\nKhrulkov, V., Hrinchuk, O., Mirvakhabova, L., and Os-\neledets, I. V. Tensorized embedding layers for ef\ufb01cient\nmodel compression. CoRR, abs/1901.10787, 2019.\nLiu, Y., Liu, Z., Chua, T.-S., and Sun, M. Topical word em-\nbeddings. In Proc. 29th AAAI Conf. Arti\ufb01cial Intelligence,\n2015.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. Distributed representations of words and phrases\nand their compositionality. In Advances in Neural Infor-\nmation Processing Systems, pp. 3111\u20133119, 2013.\nSubmission and Formatting Instructions for MLSys 2020\nNaumov, M., Diril, U., Park, J., Ray, B., Jablonski, J., and\nTulloch, A. On periodic functions as regularizers for\nquantization of neural networks. CoRR, abs/1811.09862,\n2018.\nNaumov, M., Mudigere, D., Shi, H. M., Huang, J., Sun-\ndaraman, N., Park, J., Wang, X., Gupta, U., Wu, C.,\nAzzolini, A. G., Dzhulgakov, D., Mallevich, A., Cher-\nniavskii, I., Lu, Y., Krishnamoorthi, R., Yu, A., Kon-\ndratenko, V., Pereira, S., Chen, X., Chen, W., Rao, V.,\nJia, B., Xiong, L., and Smelyanskiy, M. Deep learning\nrecommendation model for personalization and recom-\nmendation systems. CoRR, abs/1906.00091, 2019. URL\nhttps://arxiv.org/abs/1906.00091.\nNVIDIA. Uni\ufb01ed memory in CUDA 6.\nhttps://developer.nvidia.com/blog/\nunified-memory-in-cuda-6/, 2020. Accessed:\n2020-10-08.\nPark, J., Naumov, M., Basu, P., Deng, S., Kalaiah, A., Khu-\ndia, D. S., Law, J., Malani, P., Malevich, A., Satish, N.,\nPino, J., Schatz, M., Sidorov, A., Sivakumar, V., Tulloch,\nA., Wang, X., Wu, Y., Yuen, H., Diril, U., Dzhulgakov,\nD., Hazelwood, K. M., Jia, B., Jia, Y., Qiao, L., Rao, V.,\nRotem, N., Yoo, S., and Smelyanskiy, M. Deep learn-\ning inference in Facebook data centers: Characterization,\nperformance optimizations and hardware implications.\nCoRR, abs/1811.09886, 2018.\nPennington, J., Socher, R., and Manning, C. Glove: Global\nvectors for word representation. In Proceedings of the\n2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 2014.\nPeters, M., Neumann, M., Zettlemoyer, L., and Yih, W.-t.\nDissecting contextual word embeddings: Architecture\nand representation. In Proc. 2018 Conf. Empirical Meth-\nods in NLP, pp. 1499\u20131509, 2018.\nSattigeri, P. and Thiagarajan, J. J. Sparsifying word rep-\nresentations for deep unordered sentence modeling. In\nProc. 1st Workshop on Representation Learning for NLP,\npp. 206\u2013214.\nSun, F., Guo, J., Lan, Y., Xu, J., and Cheng, X. Sparse word\nembeddings using l1 regularized online learning. In Proc.\n25th Int. Joint Conf. Arti\ufb01cial Intelligence, 2016.\nVasileva, M. I., Plummer, B. A., Dusad, K., Rajpal, S.,\nKumar, R., and Forsyth, D. Learning type-aware embed-\ndings for fashion compatibility. In Proc. European Conf.\nComputer Vision, pp. 390\u2013405, 2018.\nWang, R., Fu, B., Fu, G., and Wang, M. Deep & cross\nnetwork for ad click predictions. In ADKDD@KDD,\n2017.\nWu, S., Hsiao, L., Cheng, X., Hancock, B., Rekatsinas, T.,\nLevis, P., and R\u00b4e, C. Fonduer: Knowledge base construc-\ntion from richly formatted data. In Proceedings of the\n2018 International Conference on Management of Data,\n2018a.\nWu, S., Li, G., Chen, F., and Shi, L.\nTraining and in-\nference with integers in deep neural networks. CoRR,\nabs/1802.04680, 2018b.\nZhang, J., Yang, J., and Yuen, H.\nTraining with low-\nprecision embedding tables. In Systems for Machine\nLearning Workshop at NeurIPS 2018, 2018.\nZhao, W., Xie, D., Jia, R., Qian, Y., Ding, R., Sun, M., and\nLi, P. Distributed hierarchical gpu parameter server for\nmassive scale deep learning ads systems. arXiv preprint\narXiv:2003.05622, 2020.\nSubmission and Formatting Instructions for MLSys 2020\nA\nAPPENDIX: MEMORY COMPRESSION\nFACTOR\nMemory compression factor is used to characterize the com-\npressed model size after applying the low-precision embed-\nding table with a high-precision cache. We need to consider\nthe following dominant components:\n\u2022 Low-precision table size, MTlowprec;\n\u2022 Quantization parameter storage size, MQ;\n\u2022 High-precision cache size, MC;\n\u2022 Cache tags size, Mtags;\n\u2022 Access counters (if using LFU), Mcnt;\n\u2022 Original FP32 embedding table size, MTF P 32\nSpeci\ufb01cally, the \u201cmemory compression factor\u201d is calculated\nas\n(MTlowprec + MC + Mtags + Mcnt)/MTF P 32\n(2)\nwhere:\n\u2022 Cache tags are INT32 numbers, one per each row in\ncache;\n\u2022 Access counters are INT32 numbers, one per each row\nof the embedding.\nAssume the number of embedding rows is E, the embedding\ndimension is D, and the low-precision bitwidth is bitwidth,\nthen the low-precision table size is calculated as:\nMTlowprec = E \u00d7 (bitwidth \u00d7 D + 64),\n(3)\nwhere 64 is the per-row overhead of quantization parameters\n(in FP32): 32 bits for the per-row scale and 32 bits for the\nper-row bias.\nTable 11 shows memory compression factors calculated for\na sample of precision and cache size combinations using a\nLFU cache, and the embedding dimension D = 128 (used in\nCPU experiments). As the number of rows per embedding is\ndifferent for each table, we compute cache and quantization\noverhead as the number of bits per embedding row.\nFigure 10 plots the complete accuracy vs. memory com-\npression trends for CPU experiments with DLRM-Kaggle\ndataset using compression factors computed by Table 11.\nSubmission and Formatting Instructions for MLSys 2020\nTable 11. Calculation of memory compression factor\nBITWIDTH\nCACHE SIZE\nBITS PER ROW\nBITS PER ROW\nACCESS COUNTER\nCACHE TAGS\nCOMPRESSION\nRATIO\n(FP32)\n(QUANTIZED)\nBITS PER ROW\nBITS PER ROW\nFACTOR\n8\n0\n128 \u00d7 32\n8 \u00d7 128 + 64\n0\n0\n0.26563\n4\n0\n128 \u00d7 32\n4 \u00d7 128 + 64\n0\n0\n0.14063\n2\n0\n128 \u00d7 32\n2 \u00d7 128 + 64\n0\n0\n0.07813\n4\n0.3\n128 \u00d7 32\n4 \u00d7 128 + 64\n32\n32 \u00d7 0.3\n0.45078\n8\n0.1\n128 \u00d7 32\n8 \u00d7 128 + 64\n32\n32 \u00d7 0.1\n0.37422\n8\n0.05\n128 \u00d7 32\n8 \u00d7 128 + 64\n32\n32 \u00d7 0.05\n0.32383\n4\n0.1\n128 \u00d7 32\n4 \u00d7 128 + 64\n32\n32 \u00d7 0.1\n0.24922\n4\n0.05\n128 \u00d7 32\n4 \u00d7 128 + 64\n32\n32 \u00d7 0.05\n0.19883\n2\n0.1\n128 \u00d7 32\n2 \u00d7 128 + 64\n32\n32 \u00d7 0.1\n0.18672\n2\n0.05\n128 \u00d7 32\n2 \u00d7 128 + 64\n32\n32 \u00d7 0.05\n0.13633\nMemory Compression\nAccuracy Drop\n-0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n0.1\n0.2\n0.3\n0.4\n0.5\nw/ Cache\nw/o Cache\nFigure 10. Accuracy vs. memory compression factor with and without high-precision cache on Criteo-Kaggle dataset.\n",
    "2006.14827": "Memory-efficient Embedding for Recommendations\nXiangyu Zhao, Haochen Liu,\nHui Liu, Jiliang Tang\nMichigan State University\n{zhaoxi35,liuhaoc1,liuhui7,tangjili}@msu.edu\nWeiwei Guo, Jun Shi, Sida Wang,\nHuiji Gao, Bo Long\nLinkedin Corporation\n{wguo,jshi,sidwang,hgao,blong}@linkedin.com\nABSTRACT\nPractical large-scale recommender systems usually contain thou-\nsands of feature fields from users, items, contextual information,\nand their interactions. Most of them empirically allocate a unified\ndimension to all feature fields, which is memory inefficient. Thus it\nis highly desired to assign different embedding dimensions to differ-\nent feature fields according to their importance and predictability.\nDue to the large amounts of feature fields and the nuanced rela-\ntionship between embedding dimensions with feature distributions\nand neural network architectures, manually allocating embedding\ndimensions in practical recommender systems can be very difficult.\nTo this end, we propose an AutoML based framework (AutoDim)\nin this paper, which can automatically select dimensions for dif-\nferent feature fields in a data-driven fashion. Specifically, we first\nproposed an end-to-end differentiable framework that can calculate\nthe weights over various dimensions in a soft and continuous man-\nner for feature fields, and an AutoML based optimization algorithm;\nthen we derive a hard and discrete embedding component archi-\ntecture according to the maximal weights and retrain the whole\nrecommender framework. We conduct extensive experiments on\nbenchmark datasets to validate the effectiveness of the AutoDim\nframework.\nACM Reference Format:\nXiangyu Zhao, Haochen Liu,, Hui Liu, Jiliang Tang and Weiwei Guo, Jun\nShi, Sida Wang,, Huiji Gao, Bo Long. 2020. Memory-efficient Embedding for\nRecommendations. In Proceedings of ACM Conference (Conference\u201917). ACM,\nNew York, NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nWith the explosive growth of the world-wide web, huge amounts of\ndata have been generated, which results in the increasingly severe\ninformation overload problem, potentially overwhelming users [5].\nRecommender systems can mitigate the information overload prob-\nlem through suggesting personalized items that best match users\u2019\npreferences [1, 2, 22, 26, 33, 34]. Recent years have witnessed the\nincreased development and popularity of deep learning based rec-\nommender systems (DLRSs) [27, 41, 44], which outperform tradi-\ntional recommendation techniques, such as collaborative filtering\nand learning-to-rank, because of their strong capability of feature\nrepresentation and deep inference [45].\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nConference\u201917, July 2017, Washington, DC, USA\n\u00a9 2020 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n0\n0\n1\nField 1\nField m\nField M\n0\n1\n0\n1\n0\n0\nUser\nItem\nContext\nInteraction\nMLP\nComponent\nOutput\nLayer\nEmbedding\nComponent\nInput\nFeatures\nFeature\nFields\nFigure 1: The Typically DLRS architecture.\nReal-world recommender systems typically involve a massive\namount of categorical feature fields from users (e.g. occupation and\nuserID), items (e.g. category and itemID), contextual information\n(e.g. time and location), and their interactions (e.g. user\u2019s purchase\nhistory of items). DLRSs first map these categorical features into\nreal-valued dense vectors via an embedding-component [28, 30, 49],\ni.e., the embedding-lookup process, which leads to huge amounts\nof embedding parameters. For instance, the YouTube recommender\nsystem consists of 1 million of unique videoIDs, and assign each\nvideoID with a specific 256-dimensional embedding vector; in other\nwords, the videoID feature field alone occupies 256 million parame-\nters [10]. Then, the DLRSs nonlinearly transform the input embed-\ndings from all feature fields and generate the outputs (predictions)\nvia the MLP-component (Multi-Layer Perceptron), which usually\ninvolves only several fully connected layers in practice. Therefore,\ncompared to the MLP-component, the embedding-component dom-\ninates the number of parameters in practical recommender systems,\nwhich naturally plays a tremendously impactful role in the recom-\nmendations.\nThe majority of existing recommender systems assign fixed and\nunified embedding dimension for all feature fields, such as the\nfamous Wide&Deep model [8], which may lead to memory in-\nefficiency. First, the embedding dimension often determines the\ncapacity to encode information. Thus, allocating the same dimen-\nsion to all feature fields may lose the information of high predictive\narXiv:2006.14827v2  [cs.IR]  21 Oct 2020\n(a) Dimensionality Search\n(b) Parameter Re-training\nEmbedding\nLookup\nDeriving\nDiscrete\nArchitectures\n0\n0\n1\nField 1\nField m\nField M\n0\n1\n0\n1\n0\n0\nWeights\nTransforms\n0\n0\n1\nField 1\nField m\nField M\n0\n1\n0\n1\n0\n0\n0.7\n0.3\n0.2\n0.8\n0.6\n0.4\nEmbedding\nLookup\nTransforms\nFigure 2: Overview of the proposed framework.\nfeatures while wasting memory on non-predictive features. There-\nfore, we should assign large dimension to the high informative and\npredictive features, for instance, the \u201clocation\u201d feature in location-\nbased recommender systems [1]. Second, different feature fields\nhave different cardinality (i.e. the number of unique values). For\nexample, the gender feature has only two (i.e. male and female),\nwhile the itemID feature usually involves millions of unique val-\nues. Intuitively, we should allocate larger dimensions to the feature\nfields with more unique feature values to encode their complex\nrelationships with other features, and assign smaller dimensions to\nfeature fields with smaller cardinality to avoid the overfitting prob-\nlem due to the over-parameterization [11, 18, 20, 47]. According to\nthe above reasons, it is highly desired to assign different embedding\ndimensions to different feature fields in a memory-efficient manner.\nIn this paper, we aim to enable different embedding dimen-\nsions for different feature fields for recommendations. We face\ntremendous challenges. First, the relationship among embedding\ndimensions, feature distributions and neural network architectures\nis highly intricate, which makes it hard to manually assign em-\nbedding dimensions to each feature field [11]. Second, real-world\nrecommender systems often involve hundreds and thousands of\nfeature fields. It is difficult, if possible, to artificially select different\ndimensions for all feature fields, due to the expensive computa-\ntion cost from the incredibly huge (\ud835\udc41\ud835\udc40, with \ud835\udc41the number of\ncandidate dimensions for each feature field to select, and \ud835\udc40the\nnumber of feature fields) search space. Our attempt to address these\nchallenges results in an end-to-end differentiable AutoML based\nframework (AutoDim), which can efficiently allocate embedding\ndimensions to different feature fields in an automated and data-\ndriven manner. Our experiments on benchmark datasets demon-\nstrate the effectiveness of the proposed framework. We summarize\nour major contributions as: (i) we propose an end-to-end AutoML\nbased framework AutoDim, which can automatically select various\nembedding dimensions to different feature fields; (ii) we develop\ntwo embedding lookup methods and two embedding transforma-\ntion approaches, and compare the impact of their combinations on\nthe embedding dimension allocation decision; and (iii) we demon-\nstrate the effectiveness of the proposed framework on real-world\nbenchmark datasets.\nThe rest of this paper is organized as follows. In Section 2, we in-\ntroduce details about how to assign various embedding dimensions\nfor different feature fields in an automated and data-driven fashion,\nand propose an AutoML based optimization algorithm. Section 3\ncarries out experiments based on real-world datasets and presents\nexperimental results. Section 4 briefly reviews related work. Finally,\nsection 5 concludes this work and discusses our future work.\n2\nFRAMEWORK\nIn order to achieve the automated allocation of different embed-\nding dimensions to different feature fields, we propose an AutoML\nbased framework, which effectively addresses the challenges we\ndiscussed in Section 1. In this section, we will first introduce the\noverview of the whole framework; then we will propose an end-to-\nend differentiable model with two embedding-lookup methods and\ntwo embedding dimension search methods, which can compute the\nweights of different dimensions for feature fields in a soft and con-\ntinuous fashion, and we will provide an AutoML based optimization\nalgorithm; finally, we will derive a discrete embedding architecture\nupon the maximal weights, and retrain the whole DLRS framework.\n2.1\nOverview\nOur goal is to assign different feature fields various embedding di-\nmensions in an automated and data-driven manner, so as to enhance\nthe memory efficiency and the performance of the recommender\nsystem. We illustrate the overall framework in Figure 2, which\nconsists of two major stages:\n2.1.1\nDimensionality search stage. It aims to find the optimal\nembedding dimension for each feature field. To be more specific, we\nfirst assign a set of candidate embeddings with different dimensions\nto a specific categorical feature via an embedding-lookup step; then,\nwe unify the dimensions of these candidate embeddings through a\ntransformation step, which is because of the fixed input dimension\nof the first MLP layer; next, we obtain the formal embedding for this\ncategorical feature by computing the weighted sum of all its trans-\nformed candidate embeddings, and feed it into the MLP-component.\nThe DLRS parameters including the embeddings and MLP layers are\nlearned upon the training set, while the architectural weights over\nthe candidate embeddings are optimized upon the validation set,\nwhich prevents the framework selecting the embedding dimensions\nthat overfit the training set [24, 29].\nIn practice, before the alternative training of DLRS parameters\nand architectural weights, we initially assign equivalent architec-\ntural weights on all candidate embeddings (e.g., [0.5, 0.5] for the\nexample in Figure 2), fix these architectural weights and pre-train\nthe DLRS including all candidate embeddings. The pre-training\nenables a fair competition between candidate embeddings when\nwe start to update architectural weights.\n2.1.2\nParameter re-training stage. According to the architec-\ntural weights learned in dimensionality search stage, we select the\nembedding dimension for each feature field, and re-train the param-\neters of DLRS parameters (i.e., MLPs and selected embeddings) on\nthe training dataset in an end-to-end fashion. It is noteworthy that:\n(i) re-training stage is necessary, since in dimensionality search\nstage, the model performance is also influenced by the suboptimal\nembedding dimensions, which are not desired in practical recom-\nmender system; and (ii) new embeddings are still unified into the\nsame dimension, since most existing deep recommender models\n(such as FM [32], DeepFM [13], NFM [14]) capture the interactions\nbetween two feature fields via a interaction operation (e.g., inner\nproduct) over their embedding vectors. These interaction opera-\ntions constrain the embedding vectors to have same dimensions.\nNote that numerical features will be converted into categorical\nfeatures through bucketing, and we omit this process in the follow-\ning sections for simplicity. Next, we will introduce the details of\neach stage.\n2.2\nDimensionality Search\nAs discussed in Section 1, different feature fields have different\ncardinalities and various contributions to the final prediction. In-\nspired by this phenomenon, it is highly desired to enable various\nembedding dimensions for different feature fields. However, due to\na large amount of feature fields and the complex relationship be-\ntween embedding dimensions with feature distributions and neural\nnetwork architectures, it is difficult to manually select embedding\ndimensions via conventional dimension reduction methods. An\nintuitive solution to tackle this challenge is to assign several embed-\nding spaces with various dimensions to each feature field, and then\nthe DLRS automatically selects the optimal embedding dimension\nfor each feature field.\n(a) Separate Embeddings\n0\n1\n0\n(b) Weight-sharing Embeddings\n0\n1\n0\nFigure 3: The embedding lookup methods.\n2.2.1\nEmbedding Lookup Tricks. Suppose for each user-item\ninteraction instance, we have \ud835\udc40input features (\ud835\udc651, \u00b7 \u00b7 \u00b7 ,\ud835\udc65\ud835\udc40), and\neach feature \ud835\udc65\ud835\udc5abelongs to a specific feature field, such as gender\nand age, etc. For the \ud835\udc5a\ud835\udc61\u210efeature field, we assign \ud835\udc41candidate em-\nbedding spaces {X1\ud835\udc5a, \u00b7 \u00b7 \u00b7 , X\ud835\udc41\ud835\udc5a}. The dimension of an embedding in\neach space is \ud835\udc511, \u00b7 \u00b7 \u00b7 ,\ud835\udc51\ud835\udc41, where \ud835\udc511 < \u00b7 \u00b7 \u00b7 < \ud835\udc51\ud835\udc41; and the cardinality\nof these embedding spaces are the number of unique feature values\nin this feature field. Correspondingly, we define {x1\ud835\udc5a, \u00b7 \u00b7 \u00b7 , x\ud835\udc41\ud835\udc5a} as\nthe set of candidate embeddings for a given feature \ud835\udc65\ud835\udc5afrom all\nembedding spaces, as shown in Figure 3 (a). Note that we assign the\nsame candidate dimension to all feature fields for simplicity, but it\nis straightforward to introduce different candidate sets. Therefore,\nthe total space assigned to the feature \ud835\udc65\ud835\udc5ais \u00cd\ud835\udc41\n\ud835\udc5b=1 \ud835\udc51\ud835\udc5b. However, in\nreal-world recommender systems with thousands of feature fields,\ntwo challenges lie in this design include (i) this design needs huge\nspace to store all candidate embeddings, and (ii) the training effi-\nciency is reduced since a large number of parameters need to be\nlearned.\nTo address these challenges, we propose an alternative solution\nfor large-scale recommendations, named weight-sharing embed-\nding architecture. As illustrated in Figure 3 (b), we only allocate a\n\ud835\udc51\ud835\udc41-dimensional embedding to a given feature \ud835\udc65\ud835\udc5a, referred as to\nx\u2032\ud835\udc5a, then the \ud835\udc5b\ud835\udc61\u210ecandidate embedding x\ud835\udc5b\ud835\udc5acorresponds to the first\n\ud835\udc51\ud835\udc5bdigits of x\u2032\ud835\udc5a. The advantages associated with weight-sharing\nembedding method are two-fold, i.e., (i) it is able to reduce the stor-\nage space and increase the training efficiency, as well as (ii) since\nthe relatively front digits of x\u2032\ud835\udc5ahave more chances to be retrieved\nand then be trained (e.g. the \u201cred part\u201d of x\u2032\ud835\udc5ais leveraged by all\nEmbedding\nLookup\nLinear\nTransform\nBatchNorm\nWeighted\nSum\n0\n1\n0\nFigure 4: Method 1 - Linear Transformation.\ncandidates in Figure 3 (b)), we intuitively wish they can capture\nmore essential information of the feature \ud835\udc65\ud835\udc5a.\n2.2.2\nUnifying Various Dimensions. Since the input dimension\nof the first MLP layer in existing DLRSs is often fixed, it is difficult\nfor them to handle various candidate dimensions. Thus we need to\nunify the embeddings {x1\ud835\udc5a, \u00b7 \u00b7 \u00b7 , x\ud835\udc41\ud835\udc5a} into same dimension, and we\ndevelop two following methods:\nMethod 1: Linear Transformation. Figure 4 (a) illustrates the\nlinear transformation method to handle the various embedding\ndimensions (the difference of two embedding lookup methods is\nomitted here). We introduce \ud835\udc41fully-connected layers, which trans-\nform embedding vectors {x1\ud835\udc5a, \u00b7 \u00b7 \u00b7 , x\ud835\udc41\ud835\udc5a} into the same dimension\n\ud835\udc51\ud835\udc41:\nex\ud835\udc5b\ud835\udc5a\u2190W\u22a4\ud835\udc5bx\ud835\udc5b\ud835\udc5a+ b\ud835\udc5b\n\u2200\ud835\udc5b\u2208[1, \ud835\udc41]\n(1)\nwhere W\ud835\udc5b\u2208R\ud835\udc51\ud835\udc5b\u00d7\ud835\udc51\ud835\udc41is weight matrice and b\ud835\udc5b\u2208R\ud835\udc51\ud835\udc41is bias vector.\nFor each field, all candidate embeddings with the same dimension\nshare the same weight matrice and bias vector, which can reduce\nthe amount of model parameters. With the linear transformations,\nwe map the original embedding vectors {x1\ud835\udc5a, \u00b7 \u00b7 \u00b7 , x\ud835\udc41\ud835\udc5a} into the\nsame dimensional space, i.e., {ex1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,ex\ud835\udc41\ud835\udc5a} \u2208R\ud835\udc51\ud835\udc41. In practice, we\ncan observe that the magnitude of the transformed embeddings\n{ex1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,ex\ud835\udc41\ud835\udc5a} varies significantly, which makes them become in-\ncomparable. To tackle this challenge, we conduct BatchNorm [16]\non the transformed embeddings {ex1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,ex\ud835\udc41\ud835\udc5a} as:\nbx\ud835\udc5b\ud835\udc5a\u2190\nex\ud835\udc5b\n\ud835\udc5a\u2212\ud835\udf07\ud835\udc5b\nB\n\u221a\ufe03\n(\ud835\udf0e\ud835\udc5b\nB)2+\ud835\udf16\n\u2200\ud835\udc5b\u2208[1, \ud835\udc41]\n(2)\nwhere \ud835\udf07\ud835\udc5b\nB is the mini-batch mean and (\ud835\udf0e\ud835\udc5b\nB)2 is the mini-batch vari-\nance for \u2200\ud835\udc5b\u2208[1, \ud835\udc41]. \ud835\udf16is a small constant added to the mini-batch\nvariance for numerical stability when (\ud835\udf0e\ud835\udc5b\nB)2 is very small. After\nBatchNorm, the linearly transformed embeddings {ex1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,ex\ud835\udc41\ud835\udc5a} be-\ncome to magnitude-comparable embedding vectors {bx1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,bx\ud835\udc41\ud835\udc5a}\nwith the same dimension \ud835\udc51\ud835\udc41.\nMethod 2: Zero Padding. Inspired by zero-padding techniques\nfrom the computer version community, which pads the input vol-\nume with zeros around the border, we address the problem of vari-\nous embedding dimensions by padding shorter embedding vectors\nto the same length as the longest embedding dimension \ud835\udc51\ud835\udc41with\nzeros, which is illustrated in Figure 5. For the embedding vectors\n{x1\n\ud835\udc56, \u00b7 \u00b7 \u00b7 , x\ud835\udc41\n\ud835\udc56} with different dimensions, we first execute Batch-\nNorm process, which forces the original embeddings {x1\n\ud835\udc56, \u00b7 \u00b7 \u00b7 , x\ud835\udc41\n\ud835\udc56}\ninto becoming magnitude-comparable embeddings:\nex\ud835\udc5b\ud835\udc5a\u2190\nx\ud835\udc5b\n\ud835\udc5a\u2212\ud835\udf07\ud835\udc5b\nB\n\u221a\ufe03\n(\ud835\udf0e\ud835\udc5b\nB)2+\ud835\udf16\n\u2200\ud835\udc5b\u2208[1, \ud835\udc41]\n(3)\nwhere \ud835\udf07\ud835\udc5b\nB, (\ud835\udf0e\ud835\udc5b\nB)2 are the mini-batch mean and variance. \ud835\udf16is the\nconstant for numerical stability. The transformed {ex1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,ex\ud835\udc41\ud835\udc5a} are\nmagnitude-comparable embeddings. Then we pad the {ex1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,ex\ud835\udc41\u22121\n\ud835\udc5a\n}\nto the same length \ud835\udc51\ud835\udc41by zeros:\nbx\ud835\udc5b\ud835\udc5a\u2190\ud835\udc5d\ud835\udc4e\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54(ex\ud835\udc5b\ud835\udc5a, \ud835\udc51\ud835\udc41\u2212\ud835\udc51\ud835\udc5b)\n\u2200\ud835\udc5b\u2208[1, \ud835\udc41]\n(4)\nwhere the second term of each padding formula is the number of\nzeros to be padded with the embedding vector of the first term.\nThen the embeddings {bx1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,bx\ud835\udc41\ud835\udc5a} share the same dimension\n\ud835\udc51\ud835\udc41. Compared with the linear transformation (method 1), the zero-\npadding method reduces lots of linear-transformation computations\nand corresponding parameters. The possible drawback is that the\nfinal embeddings {bx1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,bx\ud835\udc41\ud835\udc5a} becomes spatially unbalanced since\nthe tail parts of some final embeddings are zeros. Next, we will\nintroduce embedding dimension selection process.\n2.2.3\nDimension Selection. In this paper, we aim to select the\noptimal embedding dimension for each feature field in an automated\nand data-driven manner. This is a hard (categorical) selection on the\ncandidate embedding spaces, which will make the whole framework\nnot end-to-end differentiable. To tackle this challenge, in this work,\nwe approximate the hard selection over different dimensions via\nintroducing the Gumbel-softmax operation [17], which simulates\nthe non-differentiable sampling from a categorical distribution by\na differentiable sampling from the Gumbel-softmax distribution.\nTo be specific, suppose weights {\ud835\udefc1\ud835\udc5a, \u00b7 \u00b7 \u00b7 , \ud835\udefc\ud835\udc41\n\ud835\udc5a} are the class prob-\nabilities over different dimensions. Then a hard selection \ud835\udc67can be\ndrawn via the the gumbel-max trick [12] as:\n\ud835\udc67= one_hot\n\u0012\narg max\n\ud835\udc5b\u2208[1,\ud835\udc41]\n\u0002\nlog\ud835\udefc\ud835\udc5b\n\ud835\udc5a+ \ud835\udc54\ud835\udc5b\n\u0003\u0013\n\ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc54\ud835\udc5b= \u2212log (\u2212log (\ud835\udc62\ud835\udc5b))\n\ud835\udc62\ud835\udc5b\u223c\ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc5a(0, 1)\n(5)\nEmbedding\nLookup\nBatchNorm\nWeighted\nSum\n0\n1\n0\nZero\nPadding\n0\n0\nzeros\nzeros\nFigure 5: Method 2 - Zero Padding Transformation.\nThe gumbel noises\ud835\udc54\ud835\udc56, \u00b7 \u00b7 \u00b7 ,\ud835\udc54\ud835\udc41are i.i.d samples, which perturb log\ud835\udefc\ud835\udc5b\ud835\udc5a\nterms and make the arg max operation that is equivalent to draw-\ning a sample by \ud835\udefc1\ud835\udc5a, \u00b7 \u00b7 \u00b7 , \ud835\udefc\ud835\udc41\n\ud835\udc5aweights. However, this trick is non-\ndifferentiable due to the arg max operation. To deal with this prob-\nlem, we use the softmax function as a continuous, differentiable\napproximation to arg max operation, i.e., straight-through gumbel-\nsoftmax [17]:\n\ud835\udc5d\ud835\udc5b\n\ud835\udc5a=\nexp\n\u0010 log(\ud835\udefc\ud835\udc5b\n\ud835\udc5a)+\ud835\udc54\ud835\udc5b\n\ud835\udf0f\n\u0011\n\u00cd\ud835\udc41\n\ud835\udc56=1 exp\n\u0010 log(\ud835\udefc\ud835\udc56\ud835\udc5a)+\ud835\udc54\ud835\udc56\n\ud835\udf0f\n\u0011\n(6)\nwhere \ud835\udf0fis the temperature parameter, which controls the smooth-\nness of the output of gumbel-softmax operation. When\ud835\udf0fapproaches\nzero, the output of the gumbel-softmax becomes closer to a one-hot\nvector. Then \ud835\udc5d\ud835\udc5b\ud835\udc5ais the probability of selecting the \ud835\udc5b\ud835\udc61\u210ecandidate\nembedding dimension for the feature \ud835\udc65\ud835\udc5a, and its embedding x\ud835\udc5a\ncan be formulated as the weighted sum of {bx1\ud835\udc5a, \u00b7 \u00b7 \u00b7 ,bx\ud835\udc41\ud835\udc5a}:\nx\ud835\udc5a= \u00cd\ud835\udc41\n\ud835\udc5b=1 \ud835\udc5d\ud835\udc5b\ud835\udc5a\u00b7 bx\ud835\udc5b\ud835\udc5a\n\u2200\ud835\udc5a\u2208[1, \ud835\udc40]\n(7)\nWe illustrate the weighted sum operations in Figure 4 and 5. With\ngumbel-softmax operation, the dimensionality search process is\nend-to-end differentiable. The discrete embedding dimension selec-\ntion conducted based on the weights {\ud835\udefc\ud835\udc5b\ud835\udc5a} will be detailed in the\nfollowing subsections.\nThen, we concatenate the embeddings h0 = [x1, \u00b7 \u00b7 \u00b7 , x\ud835\udc40] and\nfeed h0 input into \ud835\udc3fmultilayer perceptron layers:\nh\ud835\udc59= \ud835\udf0e\n\u0010\nW\u22a4\n\ud835\udc59h\ud835\udc59\u22121 + b\ud835\udc59\n\u0011\n\u2200\ud835\udc59\u2208[1, \ud835\udc3f]\n(8)\nwhere W\ud835\udc59and b\ud835\udc59are the weight matrix and the bias vector for the\n\ud835\udc59\ud835\udc61\u210eMLP layer. \ud835\udf0e(\u00b7) is the activation function such as ReLU and\nTanh. Finally, the output layer that is subsequent to the last MLP\nlayer, produces the prediction of the current user-item interaction\ninstance as:\n\u02c6\ud835\udc66= \ud835\udf0e\u0000W\u22a4\n\ud835\udc5ch\ud835\udc3f+ b\ud835\udc5c\n\u0001\n(9)\nwhere W\ud835\udc5cand b\ud835\udc5care the weight matrix and bias vector for the\noutput layer. Activation function \ud835\udf0e(\u00b7) is selected based on different\nrecommendation tasks, such as Sigmoid function for regression [8],\nand Softmax for multi-class classification [38]. Correspondingly,\nthe objective function L( \u02c6\ud835\udc66,\ud835\udc66) between prediction \u02c6\ud835\udc66and ground\ntruth label \ud835\udc66also varies based on different recommendation tasks.\nIn this work, we leverage negative log-likelihood function:\nL( \u02c6\ud835\udc66,\ud835\udc66) = \u2212\ud835\udc66log \u02c6\ud835\udc66\u2212(1 \u2212\ud835\udc66) log(1 \u2212\u02c6\ud835\udc66)\n(10)\nwhere \ud835\udc66is the ground truth (1 for like or click, 0 for dislike or non-\nclick). By minimizing the objective function L( \u02c6\ud835\udc66,\ud835\udc66), the dimension-\nality search framework updates the parameters of all embeddings,\nhidden layers, and weights {\ud835\udefc\ud835\udc5b\ud835\udc5a} through back-propagation. The\nhigh-level idea of the dimensionality search is illustrated in Figure 2\n(a), where we omit some details of embedding-lookup, transforma-\ntions and gumbel-softmax for the sake of simplicity.\n2.3\nOptimization\nIn this subsection, we will detail the optimization method of the pro-\nposed AutoDim framework. In AutoDim, we formulate the selection\nover different embedding dimensions as an architectural optimiza-\ntion problem and make it end-to-end differentiable by leveraging\nthe Gumbel-softmax technique. The parameters to be optimized\nin AutoDim are two-fold, i.e., (i) W: the parameters of the DLRS,\nincluding the embedding-component and the MLP-component; (ii)\n\ud835\udf36: the weights {\ud835\udefc\ud835\udc5b\ud835\udc5a} on different embedding spaces ({\ud835\udc5d\ud835\udc5b\ud835\udc5a} are cal-\nculated based on {\ud835\udefc\ud835\udc5b\ud835\udc5a} as in Equation (6)). DLRS parameters W and\narchitectural weights \ud835\udf36can not be optimized simultaneously on\ntraining dataset as conventional supervised attention mechanism\nsince the optimization of them are highly dependent on each other.\nIn other words, simultaneously optimization on training dataset\nmay result in model overfitting on the examples from training\ndataset.\nInspired by the differentiable architecture search (DARTS) tech-\nniques [24], W and \ud835\udf36are alternately optimized through gradient\ndescent. Specifically, we alternately update W by optimizing the\nloss L\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bon the training data and update \ud835\udf36by optimizing the\nloss L\ud835\udc63\ud835\udc4e\ud835\udc59on the validation data:\nmin\n\ud835\udf36\nL\ud835\udc63\ud835\udc4e\ud835\udc59\n\u0000W\u2217(\ud835\udf36), \ud835\udf36\u0001\n\ud835\udc60.\ud835\udc61. W\u2217(\ud835\udf36) = arg min\nW L\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b(W, \ud835\udf36\u2217)\n(11)\nthis optimization forms a bilevel optimization problem [29], where\narchitectural weights \ud835\udf36and DLRS parameters W are identified as\nthe upper-level variable and lower-level variable. Since the inner\noptimization of W is computationally expensive, directly optimizing\nAlgorithm 1 DARTS based Optimization for AutoDim.\nInput: the features (\ud835\udc651, \u00b7 \u00b7 \u00b7 ,\ud835\udc65\ud835\udc40) of user-item interactions and the\ncorresponding ground-truth labels \ud835\udc66\nOutput: the well-learned DLRS parameters W\u2217; the well-learned\nweights on various embedding spaces \ud835\udf36\u2217\n1: while not converged do\n2:\nSample a mini-batch of user-item interactions from\nvalidation data\n3:\nUpdate \ud835\udf36by descending \u2207\ud835\udf36L\ud835\udc63\ud835\udc4e\ud835\udc59\n\u0000W\u2217(\ud835\udf36), \ud835\udf36\u0001 with the\napproximation in Eq.(12)\n4:\nCollect a mini-batch of training data\n5:\nGenerate predictions \u02c6\ud835\udc66via DLRS with current W and\narchitectural weights \ud835\udf36\n6:\nUpdate W by descending \u2207WL\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b(W, \ud835\udf36)\n7: end while\n\ud835\udf36via Eq.(11) is intractable. To address this challenge, we take\nadvantage of the approximation scheme of DARTS:\narg min\nW L\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b(W, \ud835\udf36\u2217) \u2248W \u2212\ud835\udf09\u2207WL\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b(W, \ud835\udf36)\n(12)\nwhere \ud835\udf09is the learning rate. In the approximation scheme, when\nupdating \ud835\udf36via Eq.(12), we estimate W\u2217(\ud835\udf36) by descending the gra-\ndient \u2207WL\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b(W, \ud835\udf36) for only one step, rather than to optimize\nW(\ud835\udf36) thoroughly to obtain W\u2217(\ud835\udf36) = arg minW L\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b(W, \ud835\udf36\u2217).\nIn practice, it usually leverages the first-order approximation by\nsetting \ud835\udf09= 0, which can further enhance the computation efficiency.\nThe DARTS based optimization algorithm for AutoDim is de-\ntailed in Algorithm 1. Specifically, in each iteration, we first sample\na batch of user-item interaction data from the validation set (line 2);\nnext, we update the architectural weights \ud835\udf36upon it (line 3); after-\nward, the DLRS make the predictions \u02c6\ud835\udc66on the batch of training data\nwith current DLRS parameters W and architectural weights \ud835\udf36(line\n5); eventually, we update the DLRS parameters W by descending\n\u2207WL\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b(W, \ud835\udf36) (line 6).\n2.3.1\nA pre-train trick. In practice, in order to enable a fair com-\npetition between the candidate embeddings, for each feature field,\nwe first allocate the equivalent architectural weights initially on all\nits candidate embeddings, e.g., [0.5, 0.5] if there are two candidate\nembedding dimensions. Then, we fix these initialized architectural\nweights \ud835\udf36and pre-train the DLRS parameters W including all can-\ndidate embeddings. This process ensures a fair competition between\ncandidate embeddings when we begin to update \ud835\udf36.\n2.4\nParameter Re-Training\nSince the suboptimal embedding dimensions in dimensionality\nsearch stage also influence the model training, a re-training stage is\ndesired to training the model with only optimal dimensions, which\ncan eliminate these suboptimal influences. In this subsection, we\nwill introduce how to select optimal embedding dimension for each\nfeature field and the details of re-training the recommender system\nwith the selected embedding dimensions.\n2.4.1\nDeriving Discrete Dimensions. During re-training, the\ngumbel-softmax operation is no longer used, which means that the\noptimal embedding space (dimension) are selected for each feature\nAlgorithm 2 The Optimization of DLRS Re-training Process.\nInput: the features (\ud835\udc651, \u00b7 \u00b7 \u00b7 ,\ud835\udc65\ud835\udc40) of user-item interactions and the\ncorresponding ground-truth labels \ud835\udc66\nOutput: the well-learned DLRS parameters W\u2217\n1: while not converged do\n2:\nSample a mini-batch of training data\n3:\nGenerate predictions \u02c6\ud835\udc66via DLRS with current W\n4:\nUpdate W by descending \u2207WL\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b(W)\n5: end while\nfield as the one corresponding to the largest weight, based on the\nwell-learned \ud835\udf36. It is formally defined as:\nX\ud835\udc5a= X\ud835\udc58\ud835\udc5a,\n\ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc58= arg max\ud835\udc5b\u2208[1,\ud835\udc41] \ud835\udefc\ud835\udc5b\ud835\udc5a\n\u2200\ud835\udc5a\u2208[1, \ud835\udc40]\n(13)\nFigure 2 (a) illustrates the architecture of AutoDim framework with\na toy example about the optimal dimension selections based on two\ncandidate dimensions, where the largest weights corresponding\nto the 1\ud835\udc60\ud835\udc61, \ud835\udc5a\ud835\udc61\u210eand \ud835\udc40\ud835\udc61\u210efeature fields are 0.7, 0.8 and 0.6, then the\nembedding space X1\n1, X2\ud835\udc5aand X1\n\ud835\udc40are selected for these feature\nfields. The dimension of an embedding vector in these embedding\nspaces is \ud835\udc511, \ud835\udc512 and \ud835\udc511, respectively.\n2.4.2\nModel Re-training. As shown in Figure 2 (b), given the\nselected embedding spaces, we can obtain unique embedding vec-\ntors (x1, \u00b7 \u00b7 \u00b7 , x\ud835\udc40) for features (\ud835\udc651, \u00b7 \u00b7 \u00b7 ,\ud835\udc65\ud835\udc40). Then we concatenate\nthese embeddings and feeds them into hidden layers. Next, the\nprediction \u02c6\ud835\udc66is generated by the output layer. Finally, all the pa-\nrameters of the DLRS, including embeddings and MLPs, will be up-\ndated via minimizing the supervised loss function L( \u02c6\ud835\udc66,\ud835\udc66) through\nback-propagation. The model re-training algorithm is detailed in\nAlgorithm 2. The re-training process is based on the same training\ndata as Algorithm 1.\nNote that the majority of existing deep recommender algorithms\n(such as FM [32], DeepFM [13], FFM [28], AFM [42], xDeepFM [21])\ncapture the interactions between feature fields via interaction oper-\nations, such as inner product and Hadamard product. These inter-\naction operations require the embedding vectors from all fields to\nhave the same dimensions. Therefore, the embeddings selected in\nSection 2.4.1 are still mapped into the same dimension as in Section\n2.2.2. In the re-training stage, the BatchNorm operation is no longer\nin use, since there are no comparisons between candidate embed-\ndings in each field. Unifying embeddings into the same dimension\ndoes not increase model parameters and computations too much: (i)\nlinear transformation: all embeddings from one feature field share\nthe same weight matrice and bias vector, and (ii) zero-padding: no\nextra trainable parameters are introduced.\n3\nEXPERIMENTS\nIn this section, we first introduce experimental settings. Then we\nconduct extensive experiments to evaluate the effectiveness of the\nproposed AutoDim framework. We mainly seek answers to the\nfollowing research questions: RQ1: How does AutoDim perform\ncompared with other embedding dimension search methods? RQ2:\nHow do the important components, i.e., 2 embedding lookup meth-\nods and 2 transformation methods, influence the performance?\nTable 1: Statistics of the datasets.\nData\nCriteo\nAvazu\n# Interactions\n45,840,617\n40,428,968\n# Feature Fields\n39\n22\n# Sparse Features\n1,086,810\n2,018,012\nRQ3: How efficient is AutoDim as compared with other methods?\nRQ4: What is the impact of important parameters on the results?\nRQ5: What is the transferability and stability of AutoDim? RQ6:\nCan AutoDim assign large embedding dimensions to really impor-\ntant feature fields?\n3.1\nDatasets\nWe evaluate our model on two benchmark datasets: (i) Criteo1:\nThis is a benchmark industry dataset to evaluate ad click-through\nrate prediction models. It consists of 45 million users\u2019 click records\non displayed ads over one month. For each data example, it contains\n13 numerical feature fields and 26 categorical feature fields. We nor-\nmalize numerical features by transforming a value \ud835\udc63\u2192\n\u0004\nlog(\ud835\udc63)2\u0005\nif \ud835\udc63> 2 as proposed by the Criteo Competition winner 2, and then\nconvert it into categorical features through bucketing. All \ud835\udc40= 39\nfeature fields are anonymous. (ii) Avazu3: Avazu dataset was pro-\nvided for the CTR prediction challenge on Kaggle, which contains\n11 days\u2019 user clicking behaviors that whether a displayed mobile ad\nimpression is clicked or not. There are \ud835\udc40= 22 categorical feature\nfields including user/ad features and device attributes. Parts of the\nfields are anonymous. Some key statistics of the datasets are shown\nin Table 1. For each dataset, we use 90% user-item interactions as\nthe training/validation set (8:1), and the rest 10% as the test set.\n3.2\nImplement Details\nNext, we detail the AutoDim architectures. For the DLRS, (i) em-\nbedding component: existing work usually set the embedding di-\nmension as 10 or 16, while recent research found that a larger\nembedding size leads to better performance [50], so we set the\nmaximal embedding dimension as 32 within our GPU memory con-\nstraints. For each feature field, we select from \ud835\udc41= 5 candidate\nembedding dimensions {2, 8, 16, 24, 32}. (ii) MLP component: we\nhave two hidden layers with the size |\u210e0| \u00d7128 and 128\u00d7128, where\n|\u210e0| is the input size of first hidden layer, |\u210e0| = 32 \u00d7 \ud835\udc40with \ud835\udc40the\nnumber of feature fields for different datasets, and we use batch\nnormalization, dropout (\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52= 0.2) and ReLU activation for both\nhidden layers. The output layer is 128 \u00d7 1 with Sigmoid activation.\nFor architectural weights \ud835\udf36: \ud835\udefc1\ud835\udc5a, \u00b7 \u00b7 \u00b7 , \ud835\udefc\ud835\udc41\n\ud835\udc5aof the \ud835\udc5a\ud835\udc61\u210efeature field\nare produced by a Softmax activation upon a trainable vector of\nlength \ud835\udc41. We use an annealing temperature \ud835\udf0f= max(0.01, 1 \u2212\n0.00005 \u00b7 \ud835\udc61) for Gumbel-softmax, where \ud835\udc61is the training step.\nThe learning rate for updating DLRS and weights are 0.001 and\n0.001, and the batch-size is set as 2000. For the parameters of the\nproposed AutoDim framework, we select them via cross-validation.\nCorrespondingly, we also do parameter-tuning for baselines for\n1https://www.kaggle.com/c/criteo-display-ad-challenge/\n2https://www.csie.ntu.edu.tw/ r01922136/kaggle-2014-criteo.pdf\n3https://www.kaggle.com/c/avazu-ctr-prediction/\na fair comparison. We will discuss more details about parameter\nselection for the proposed framework in the following subsections.\nOur implementation is based on a public Pytorch for recommen-\ndation library4, which involves 16 state-of-the-art recommendation\nalgorithms. Our model is implemented as 3 separate classes/functions,\nso it is easily to be apply our AutoDim model to these recommenda-\ntion algorithms. Due to the limited space, we show the performances\nof applying AutoDim on FM [32], W&D [8] and DeepFM [13].\n3.3\nEvaluation Metrics\nThe performance is evaluated by AUC, Logloss and Params, where\na higher AUC or a lower Logloss indicates a better recommendation\nperformance. A lower Params means a fewer embedding parameters.\nArea Under the ROC Curve (AUC) measures the probability that\na positive instance will be ranked higher than a randomly chosen\nnegative one; we introduce Logoss since all methods aim to optimize\nthe logloss in Equation (10), thus it is natural to utilize Logloss as a\nstraightforward metric. It is noteworthy that a slightly higher AUC\nor lower Logloss at 0.001-level is regarded as significant for the\nCTR prediction task [8, 13]. For an embedding dimension search\nmodel, the Params metric is the optimal number of embedding\nparameters selected by this model for the recommender system.\nWe omit the number of MLP parameters, which only occupy a\nsmall part of the total model parameters, e.g., \u223c0.5% in W&D and\nDeepFM on Criteo dataset. FM model has no MLP component.\n3.4\nOverall Performance (RQ1)\nWe compare the proposed framework with following embedding\ndimension search methods: (i) FDE (Full Dimension Embedding):\nIn this baseline, we assign the same embedding dimensions to all\nfeature fields. For each feature field, the embedding dimension is set\nas the maximal size from the candidate set, i.e., 32. (ii) MDE (Mixed\nDimension Embedding) [11]: This is a heuristic method that assigns\nhighly-frequent feature values with larger embedding dimensions,\nvice versa. We enumerate its 16 groups of suggested hyperparame-\nters settings and report the best one. (iii) DPQ (Differentiable Prod-\nuct Quantization) [7]: This baseline introduces differentiable quanti-\nzation techniques from network compression community to compact\nembeddings. (iv) NIS (Neural Input Search) [19]: This baseline ap-\nplies reinforcement learning to learn to allocate larger embedding\nsizes to active feature values, and smaller sizes to inactive ones. (v)\nMGQE (Multi-granular quantized embeddings) [20]: This baseline\nis based on DPQ, and further cuts down the embeddings space by us-\ning fewer centroids for non-frequent feature values. (vi) AutoEmb\n(Automated Embedding Dimensionality Search) [47]: This baseline\nis based on DARTS [24], and assigns embedding dimensions accord-\ning to the frequencies of feature values. (vii) RaS (Random Search):\nRandom search is strong baseline in neural network search [24].\nWe apply the same candidate embedding dimensions, randomly\nallocate dimensions to feature fields in each experiment time, and\nreport the best performance. (viii) AutoDim-s: This baseline shares\nthe same architecture with AutoDim, while we update the DLRS\nparameters and architectural weights simultaneously on the same\ntraining batch in an end-to-end backpropagation fashion.\n4https://github.com/rixwew/pytorch-fm\nTable 2: Performance comparison of different embedding search methods\nDataset\nModel\nMetrics\nSearch Methods\nFDE\nMDE\nDPQ\nNIS\nMGQE\nAutoEmb\nRaS\nAutoDim-s\nAutoDim\nCriteo\nFM\nAUC\n0.8020\n0.8027\n0.8035\n0.8042\n0.8046\n0.8049\n0.8056\n0.8063\n0.8078*\nLogloss\n0.4487\n0.4481\n0.4472\n0.4467\n0.4462\n0.4460\n0.4457\n0.4452\n0.4438*\nParams (M)\n34.778\n15.520\n20.078\n13.636\n12.564\n13.399\n16.236\n31.039\n11.632*\nCriteo\nW&D\nAUC\n0.8045\n0.8051\n0.8058\n0.8067\n0.8070\n0.8072\n0.8076\n0.8081\n0.8098*\nLogloss\n0.4468\n0.4464\n0.4457\n0.4452\n0.4446\n0.4445\n0.4443\n0.4439\n0.4419*\nParams (M)\n34.778\n18.562\n22.628\n14.728\n15.741\n15.987\n18.233\n30.330\n12.455*\nCriteo\nDeepFM\nAUC\n0.8056\n0.8060\n0.8067\n0.8076\n0.8080\n0.8082\n0.8085\n0.8089\n0.8101*\nLogloss\n0.4457\n0.4456\n0.4449\n0.4442\n0.4439\n0.4438\n0.4436\n0.4432\n0.4416*\nParams (M)\n34.778\n17.272\n25.737\n12.955\n13.059\n13.437\n17.816\n31.770\n11.457*\nAvazu\nFM\nAUC\n0.7799\n0.7802\n0.7809\n0.7818\n0.7823\n0.7825\n0.7827\n0.7831\n0.7842*\nLogloss\n0.3805\n0.3803\n0.3799\n0.3792\n0.3789\n0.3788\n0.3787\n0.3785\n0.3776*\nParams (M)\n64.576\n22.696\n28.187\n22.679\n22.769\n21.026\n27.272\n55.038\n17.595*\nAvazu\nW&D\nAUC\n0.7827\n0.7829\n0.7836\n0.7842\n0.7849\n0.7851\n0.7853\n0.7856\n0.7872*\nLogloss\n0.3788\n0.3785\n0.3777\n0.3772\n0.3768\n0.3767\n0.3767\n0.3766\n0.3756*\nParams (M)\n64.576\n27.976\n35.558\n21.413\n19.457\n17.292\n35.126\n56.401\n14.130*\nAvazu\nDeepFM\nAUC\n0.7842\n0.7845\n0.7852\n0.7858\n0.7863\n0.7866\n0.7867\n0.7870\n0.7881*\nLogloss\n0.3742\n0.3739\n0.3737\n0.3736\n0.3734\n0.3733\n0.3732\n0.3730\n0.3721*\nParams (M)\n64.576\n32.972\n36.128\n22.550\n17.575\n21.605\n29.235\n58.325\n13.976*\n\u201c*\" indicates the statistically significant improvements (i.e., two-sided t-test with \ud835\udc5d< 0.05) over the best baseline. (M=Million)\nThe overall results are shown in Table 2. We can observe: (1)\nFDE achieves the worst recommendation performance and largest\nParams, where FDE is assigned the maximal embedding dimension\n32 to all feature fields. This result demonstrates that allocating same\ndimension to all feature fields is not only memory inefficient, but\nintroduces numerous noises into the model. (2) RaS, AutoDim-s,\nAutoDim performs better than MDE, DPQ, NIS, MGQE, AutoEmb.\nThe major differences between these two groups of methods are: (i)\nthe first group aims to assign different embedding dimensions to\ndifferent feature fields, while embeddings in the same feature field\nshare the same dimension; (ii) the second group attempts to assign\ndifferent embedding sizes to different feature values within the\nsame feature fields, which are based on the frequencies of feature\nvalues. The second group of methods surfer from several challenges:\n(ii-a) there are numerous unique values in each feature field, e.g.,\n2.7 \u00d7 104 values for each feature field on average in Criteo dataset.\nThis leads to a huge search space (even after bucketing) in each\nfeature field, which makes it difficult to find the optimal solution,\nwhile the search space for each feature field is \ud835\udc41= 5 in AutoDim;\n(ii-b) allocating dimensions solely based on feature frequencies (i.e.,\nhow many times a feature value appears in the training set) may\nlose other important characteristics of the feature; (ii-c) the fea-\nture values frequencies are usually dynamic and not pre-known\nin real-time recommender system, e.g., the cold-start users/items;\nand (ii-d) it is difficult to manage embeddings with different di-\nmensions for the same feature filed. (3) AutoDim outperforms RaS\nand AutoDim-s, where AutoDim updates the architectural weights\n\ud835\udf36on the validation batch, which can enhance the generalization;\nAutoDim-s updates the \ud835\udf36with DLRS on the same training batch si-\nmultaneously, which may lead to overfitting; RaS randomly search\nthe dimensions, which has a large search space. AutoDim-s has\nmuch larger Params than AutoDim, which indicates that larger\ndimensions are more efficient in minimizing training loss.\nTo sum up, we can draw an answer to the first question: com-\npared with the representative baselines, AutoDim achieves signifi-\ncantly better recommendation performance, and saves 70% \u223c80%\nembedding parameters. These results prove the effectiveness of the\nAutoDim framework.\n3.5\nComponent Analysis (RQ2)\nIn this paper, we propose two embedding lookup methods in Sec-\ntion 2.2.1 (i.e. separate embeddings v.s. weight-sharing embeddings)\nand two transformation methods in Section 2.2.2 (i.e. linear transfor-\nmation v.s. zero-padding transformation). In this section, we inves-\ntigate their influence on performance. We systematically combine\nthe corresponding model components by defining the following\nvariants of AutoDim: (i) AD-1: weight-sharing embeddings and\nzero-padding transformation; (ii) AD-2: weight-sharing embed-\ndings and linear transformation; (iii) AD-3: separate embeddings\nand zero-padding transformation; (iv) AD-4: separate embeddings\nand linear transformation.\nThe results of DeepFM on the Criteo dataset are shown in Fig-\nure 6. We omit similar results on other models/datasets due to\nthe limited space. We make the following observations: (1) In Fig-\nure 6 (a), we compare the embedding component parameters of in\nthe dimension search stage, i.e., all the candidate embeddings and\nAD-1 AD-2 AD-3 AD-4\n0\n20\n40\n60\n80\n100\n(a) Search Params (M)\nAD-1 AD-2 AD-3 AD-4\n0\n5\n10\n15\n(b) Train Time (H)\nAD-1 AD-2 AD-3 AD-4\n0.800\n0.805\n0.810\n0.815\n(c) AUC\nAD-1 AD-2 AD-3 AD-4\n0.430\n0.435\n0.440\n0.445\n0.450\n(d) LogLoss\nAD-1 AD-2 AD-3 AD-4\n11.0\n11.2\n11.4\n11.6\n11.8\n12.0\n(e) Params (M)\nAD-1 AD-2 AD-3 AD-4\n0\n30\n60\n90\n120\n150\n(f) Infer Time (ms)\nFigure 6: Component analysis of DeepFM on Criteo dataset. *(f) Infer time is averaged for one batch (batch size = 2,000)\nthe transformation neural networks shown in Figure 4 or 5. We\ncan observe that AD-1 and AD-2 save significant model parame-\nters by introducing the weight-sharing embeddings, which also\nleads to a faster training speed in Figure 6 (b). Therefore, weight-\nsharing embeddings can benefit real-world recommenders where\nexist thousands of feature fields and the computing resources are\nlimited. (2) Compared with linear transformation, leveraging zero-\npadding transformation have slightly fewer parameters, and result\nin slightly faster training speed (e.g., AD-1 v.s. AD-2 in Figure 6\n(a) and (b)). However, we can observe the final DLRS architecture\nselected by AD-1 loses to that of AD-2 in Figure 6 (c) AUC and\n(d) Logloss. The reason is that zero-padding embeddings may lose\ninformation when conduct inner product. For instance, to compute\nthe inner product of \ud835\udc4e= [\ud835\udc4e1,\ud835\udc4e2,\ud835\udc4e3] and \ud835\udc4f= [\ud835\udc4f1,\ud835\udc4f2], we first pad \ud835\udc4f\nto \ud835\udc4f= [\ud835\udc4f1,\ud835\udc4f2, 0], then < \ud835\udc4e,\ud835\udc4f>= \ud835\udc4e1 \u00b7 \ud835\udc4f1 + \ud835\udc4e2 \u00b7 \ud835\udc4f2 + \ud835\udc4e3 \u00b7 0, where the\ninformation \ud835\udc4e3 is lost via multiplying 0. Models without element-\nwise product between embeddings, such as FNN [46], do not suffer\nfrom this drawback. (3) In Figure 6 (e) and (f), we can observe that\nthe final embedding dimensions selected by AD-2 save most model\nparameters and has the fastest inference speed. (4) From Figure 6\n(c) and (d), variants with weight-sharing embeddings have better\nperformance than variants using separate embeddings. This is be-\ncause the relatively front digits of its embedding space are more\nlikely to be recalled and trained (as shown in Figure 3 (b)), which\nenable the framework capture more essential information in these\ndigits, and make optimal dimension assignment selection.\nIn summary, we can answer the second question: different com-\nponent has its advantages, such as zero-padding has fastest training\nspeed and uses least parameters, linear transformation has best per-\nformances on AUC/Logloss/Params metrics, and has good training\nspeed and model parameters. If not specified, the results in other\nsubsections are based on the AD-2, and we use linear transforma-\ntion for RaS and AutoDim-s in Section 3.4.\n3.6\nEfficiency Analysis (RQ3)\nIn addition to model effectiveness, the training and inference effi-\nciency are also essential metrics for deploying a recommendation\nmodel into commercial recommender systems. In this section, we\ninvestigate the efficiency of applying search methods to DeepFM\non Criteo dataset (on one Tesla K80 GPU). Similar results on other\nmodels/dataset are omitted due to the limited space. We illustrate\nthe results in Figure 7.\nFDE\nMDE\nDPQ\nNIS\nMGQE\nAutoEmb\nRaS\nAutoDim-s\nAutoDim\n0\n10\n20\n30\n40\n50\n(a) Training Time (h)\nFDE\nMDE\nDPQ\nNIS\nMGQE\nAutoEmb\nRaS\nAutoDim-s\nAutoDim\n0\n50\n100\n150\n200\n(b) Infer Time / Batch (ms)\nFigure 7: Efficiency analysis of DeepFM on Criteo dataset.\nFor the training time in Figure 7 (a), we can observe that Au-\ntoDim and AutoDim-s have fast training speed. As discussed in\nSection 3.4, the reason is that they have a smaller search space\nthan other baselines. FDE\u2019s training is fast since we directly set its\nembedding dimension as 32, i.e., no searching stage, while its recom-\nmendation performance is worst among all methods in Section 3.4.\nFor the inference time, which is more crucial when deploying a\nmodel in commercial recommender systems, AutoDim achieves the\nleast inference time as shown in Figure 7 (b). This is because the\nfinal recommendation model selected by AutoDim has the least\nembedding parameters, i.e., the Params metric.\nTo summarize, AutoDim can efficiently achieve better perfor-\nmance, which makes it easier to be launched in real-world recom-\nmender systems.\n3.7\nParameter Analysis (RQ4)\nIn this section, we investigate how the essential hyper-parameters\ninfluence model performance. Besides common hyper-parameters\nof deep recommender systems such as the number of hidden layers\n(we omit them due to limited space), our model has one particular\nhyper-parameter, i.e., the frequency to update architectural weights\n\ud835\udf36, referred to as \ud835\udc53. In Algorithm 1, we alternately update DLRS\u2019s\nparameters on the training data and update \ud835\udf36on the validation\ndata. In practice, we find that updating \ud835\udf36can be less frequently\n0\n5\n10\n15\n20\nf\n0.8075\n0.8100\n0.8125\n(a) AUC\n0\n5\n10\n15\n20\nf\n0.4400\n0.4425\n0.4450\n(b) Logloss\n0\n5\n10\n15\n20\nf\n10\n12\n(c) Params (M)\n0\n5\n10\n15\n20\nf\n3\n9\n15\n(d) Train Time (H)\nFigure 8: Parameter analysis of DeepFM on Criteo dataset.\nthan updating DLRS\u2019s parameters, which apparently reduces lots\nof computations, and also enhances the performance.\nTo study the impact of \ud835\udc53, we investigate how DeepFM with Au-\ntoDim performs on Criteo dataset with the changes of \ud835\udc53, while\nfixing other parameters. Figure 8 shows the parameter sensitivity\nresults, where in \ud835\udc65-axis, \ud835\udc53= \ud835\udc56means updating \ud835\udf36once, then updat-\ning DLRS\u2019s parameters \ud835\udc56times. We can observe that the AutoDim\nachieves the optimal AUC/Logloss when \ud835\udc53= 10. In other words,\nupdating \ud835\udf36too frequently/infrequently results in suboptimal per-\nformance. Figure 8 (d) shows that setting \ud835\udc53= 10 can reduce \u223c50%\ntraining time compared with setting \ud835\udc53= 1.\nFigure 8 (c) shows that lower \ud835\udc53leads to lower Params, vice versa.\nThe reason is that AutoDim updates \ud835\udf36by minimizing validation\nloss, which improves the generalization of model [24, 29]. When\nupdating \ud835\udf36frequently (e.g., \ud835\udc53= 1), AutoDim tends to select smaller\nembedding size that has better generalization, while may has under-\nfitting problem; while when updating \ud835\udf36infrequently (e.g., \ud835\udc53= 20),\nAutoDim prefers larger embedding sizes that perform better on\ntraining set, but may lead to over-fitting problem. \ud835\udc53= 10 is a good\ntrade-off between model performance on training and validation\nsets. Results of the other models/dataset are similar, we omit them\nbecause of the limited space.\n3.8\nTransferability and Stability (RQ5)\n3.8.1\nTransferability of selected dimensions. In this subsection, we\ninvestigate whether the embedding dimensions selected by a sim-\nple model (FM with AutoDim, say FM+AD) can be applied to the\nrepresentative models, such as NFM [14], PNN [30], AutoInt [36],\nto enhance their recommendation performance. From Section 3.4,\nwe know the FM+AD can save 70% \u223c80% embedding parameters.\nThe results are shown in Table 3, where \"Model+AD\" means\nassigning the embedding dimensions selected by FM+AD to this\nModel. We can observe that the performances of three models are\nsignificantly improved by applying embedding dimensions selected\nby FM+AD on two datasets. These observations demonstrate the\ntransferability of embedding dimensions selected by FM+AD.\n3.8.2\nStability of selected dimensions. To study whether the dimen-\nsions selected by AutoDim are stable, we run the search stage of\nDeepFM+AutoDim on Criteo dataset with different random seeds.\nTable 3: Transferability of selected dimensions.\nModel\nCriteo\nAvazu\nAUC\nLogloss\nAUC\nLogloss\nNFM\n0.8018\n0.4491\n0.7741\n0.3846\nNFM+AD\n0.8065*\n0.4451*\n0.7766*\n0.3817*\nIPNN\n0.8085\n0.4428\n0.7855\n0.3772\nIPNN+AD\n0.8112*\n0.4407*\n0.7869*\n0.3761*\nAutoInt\n0.8096\n0.4418\n0.786\n0.3763\nAutoInt+AD\n0.8116*\n0.4403*\n0.7875*\n0.3756*\n\u201c*\" indicates the statistically significant improvements (i.e.,\ntwo-sided t-test with \ud835\udc5d< 0.05).\nThe Pearson correlation of selected dimensions from different seeds\nis around 0.85, which demonstrates the stability of the selected\ndimensions.\nTable 4: Embedding dimensions for Movielens-1m\nfeature field\nW&D (one field)\nAutoDim\nAUC\nLogloss\nDimension\nmovieId\n0.7321\n0.5947\n8\nyear\n0.5763\n0.6705\n2\ngenres\n0.6312\n0.6536\n4\nuserId\n0.6857\n0.6272\n8\ngender\n0.5079\n0.6812\n2\nage\n0.5245\n0.6805\n2\noccupation\n0.5264\n0.6805\n2\nzip\n0.6524\n0.6443\n4\n3.9\nCase Study (RQ6)\nIn this section, we investigate whether AutoDim can assign larger\nembedding dimensions to more important features. Since feature\nfields are anonymous in Criteo and Avazu, we apply W&D with Au-\ntoDim on MovieLens-1m dataset 5. MovieLens-1m is a benchmark\nfor evaluating recommendation algorithms, which contains users\u2019\nratings on movies. The dataset includes 6,040 users and 3,416 movies\nwith 1 million user-item interactions. We binarize the ratings into\na binary classification task, where ratings of 4 and 5 are viewed\nas positive and the rest as negative. There are \ud835\udc40= 8 categorical\nfeature fields: movieId, year, genres, userId, gender, age, occupation,\nzip. Since MovieLens-1m is much smaller than Criteo and Avazu,\nwe set the candidate embedding dimensions as {2, 4, 8, 16}.\nTo measure the contribution of a feature field to the final predic-\ntion, we build a W&D model with only this field, train this model\nand evaluate it on the test set. A higher AUC and a lower Logloss\nmeans this feature field is more predictive for the final prediction.\nThen, we build a comprehensive W&D model incorporating all\nfeature fields, and apply AutoDim to select the dimensions for all\nfeature fields. The results are shown in Table 4. It can be observed\nthat: (1) No feature fields are assigned 16-dimensional embedding\n5https://grouplens.org/datasets/movielens/1m/\nspace, which means candidate embedding dimensions {2, 4, 8, 16}\nare sufficient to cover all possible choices. (2) Compared to the\nAUC/Logloss of W&D with each feature field, we can find that Au-\ntoDim assigns larger embedding dimensions to important (highly\npredictive) feature fields, such as movieId and userId, vice versa.\n(3) We build a full dimension embedding (FDE) version of W&D,\nwhere all feature fields are assigned as the maximal dimension 16.\nIts performances are AUC=0.8077, Logloss=0.5383, while the perfor-\nmances of W&D with AutoDim are AUC=0.8113, Logloss=0.5242,\nand it saves 57% embedding parameters.\nIn short, above observations validates that AutoDim can assign\nlarger embedding dimensions to more predictive feature fields,\nwhich significantly enhances model performance and reduce em-\nbedding parameters.\n4\nRELATED WORK\nIn this section, we will discuss the related works. We summarize\nthe works related to our research from two perspectives, say, deep\nrecommender systems and AutoML for neural architecture search.\nDeep recommender systems have drawn increasing attention\nfrom both the academia and the industry thanks to its great advan-\ntages over traditional methods [45]. Various types of deep learning\napproaches in recommendation are developed. Sedhain et al. [35]\npresent an AutoEncoder based model named AutoRec. In their work,\nboth item-based and user-based AutoRec are introduced. They are\ndesigned to capture the low-dimension feature embeddings of users\nand items, respectively. Hidasi et al. [15] introduce an RNN based\nrecommender system named GRU4Rec. In session-based recommen-\ndation, the model captures the information from items\u2019 transition\nsequences for prediction. They also design a session-parallel mini-\nbatches algorithm and a sampling method for output, which make\nthe training process more efficient. Cheng et al. [8] introduce a\nWide&Deep framework for both regression and classification tasks.\nThe framework consists of a wide part, which is a linear model\nimplemented as one layer of a feed-forward neural network, and a\ndeep part, which contains multiple perceptron layers to learn ab-\nstract and deep representations. Guo et al. [13] propose the DeepFM\nmodel. It combines the factorization machine (FM) and MLP. The\nidea of it is to use the former to model the lower-order feature\ninteractions while using the latter to learn the higher-order inter-\nactions. Wang et al. [40] attempt to utilize CNN to extract visual\nfeatures to help POI (Point-of-Interest) recommendations. They\nbuild a PMF based framework that models the interactions between\nvisual information and latent user/location factors. Chen et al. [6]\nintroduce hierarchical attention mechanisms into recommendation\nmodels. They propose a collaborative filtering model with an item-\nlevel and a component-level attention mechanism. The item-level\nattention mechanism captures user representations by attending\nvarious items and the component-level one tries to figure out the\nmost important features from auxiliary sources for each user. Wang\net al. [39] propose a generative adversarial network (GAN) based\ninformation retrieval model, IRGAN, which is applied in the task\nof recommendation, and also web search and question answering.\nThe research of AutoML for neural architecture search can be\ntraced back to NAS [51], which first utilizes an RNN based controller\nto design neural networks and proposes a reinforcement learning\nalgorithm to optimize the framework. After that, many endeavors\nare conducted on reducing the high training cost of NAS. Pham\net al. [29] propose ENAS, where the controller learns to search a\nsubgraph from a large computational graph to form an optimal\nneural network architecture. Brock et al. [3] introduce a framework\nnamed SMASH, in which a hyper-network is developed to generate\nweights for sampled networks. DARTS [24] and SNAS [43] formu-\nlate the problem of network architecture search in a differentiable\nmanner and solve it using gradient descent. Luo et al. [25] investi-\ngate representing network architectures as embeddings. Then they\ndesign a predictor to take the architecture embedding as input to\npredict its performance. They utilize gradient-based optimization\nto find an optimal embedding and decode it back to the network\narchitecture. Some works raise another way of thinking, which is to\nlimit the search space. The works [4, 23, 31, 48] focus on searching\nconvolution cells, which are stacked repeatedly to form a convolu-\ntional neural network. Zoph et al. [52] propose a transfer learning\nframework called NASNet, which train convolution cells on smaller\ndatasets and apply them on larger datasets. Tan et al. [37] introduce\nMNAS. They propose to search hierarchical convolution cell blocks\nin an independent manner, so that a deep network can be built based\non them. Mixed Dimension Embedding [11], Differentiable Product\nQuantization [7], Neural Input Search [9, 18], Multi-granular Quan-\ntized Embedding [20], and Automated Embedding Dimensionality\nSearch [47] are designed for tuning the embedding layer of deep\nrecommender system. But they aim to tune the embedding sizes\nwithin the same feature field, and we discuss the detailed differences\nand drawbacks of these models in Section 3.4.\n5\nCONCLUSION\nIn this paper, we propose a novel framework AutoDim, which tar-\ngets at automatically assigning different embedding dimensions\nto different feature fields in a data-driven manner. In real-world\nrecommender systems, due to the huge amounts of feature fields\nand the highly complex relationships among embedding dimen-\nsions, feature distributions and neural network architectures, it is\ndifficult, if possible, to manually allocate different dimensions to\ndifferent feature fields. Thus, we proposed an AutoML based frame-\nwork to automatically select from different embedding dimensions.\nTo be specific, we first provide an end-to-end differentiable model,\nwhich computes the weights over different dimensions for differ-\nent feature fields simultaneously in a soft and continuous form,\nand we propose an AutoML-based optimization algorithm; then\naccording to the maximal weights, we derive a discrete embedding\narchitecture, and re-train the DLRS parameters. We evaluate the\nAutoDim framework with extensive experiments based on widely\nused benchmark datasets. The results show that our framework can\nmaintain or achieve slightly better performance with much fewer\nembedding space demands.\nREFERENCES\n[1] Jie Bao, Yu Zheng, David Wilkie, and Mohamed Mokbel. 2015. Recommendations\nin location-based social networks: a survey. Geoinformatica 19, 3 (2015), 525\u2013565.\n[2] John S Breese, David Heckerman, and Carl Kadie. 1998. Empirical analysis of\npredictive algorithms for collaborative filtering. In Proceedings of the Fourteenth\nconference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers\nInc., 43\u201352.\n[3] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. 2017. Smash:\none-shot model architecture search through hypernetworks. arXiv preprint\narXiv:1708.05344 (2017).\n[4] Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. 2018. Path-\nlevel network transformation for efficient architecture search. arXiv preprint\narXiv:1806.02639 (2018).\n[5] Chia-Hui Chang, Mohammed Kayed, Moheb R Girgis, and Khaled F Shaalan.\n2006. A survey of web information extraction systems. IEEE transactions on\nknowledge and data engineering 18, 10 (2006), 1411\u20131428.\n[6] Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-\nSeng Chua. 2017. Attentive collaborative filtering: Multimedia recommendation\nwith item-and component-level attention. In Proceedings of the 40th International\nACM SIGIR conference on Research and Development in Information Retrieval.\n335\u2013344.\n[7] Ting Chen, Lala Li, and Yizhou Sun. 2019. Differentiable product quantization\nfor end-to-end embedding compression. arXiv preprint arXiv:1908.09756 (2019).\n[8] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.\n2016. Wide & deep learning for recommender systems. In Proceedings of the 1st\nworkshop on deep learning for recommender systems. ACM, 7\u201310.\n[9] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Differentiable Neural\nInput Search for Recommender Systems. arXiv preprint arXiv:2006.04466 (2020).\n[10] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks\nfor youtube recommendations. In Proceedings of the 10th ACM conference on\nrecommender systems. 191\u2013198.\n[11] Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James\nZou. 2019. Mixed Dimension Embeddings with Application to Memory-Efficient\nRecommendation Systems. arXiv preprint arXiv:1909.11810 (2019).\n[12] Emil Julius Gumbel. 1948. Statistical theory of extreme values and some practical\napplications: a series of lectures. Vol. 33. US Government Printing Office.\n[13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. In\nProceedings of the 26th International Joint Conference on Artificial Intelligence.\n1725\u20131731.\n[14] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse\npredictive analytics. In Proceedings of the 40th International ACM SIGIR conference\non Research and Development in Information Retrieval. 355\u2013364.\n[15] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).\n[16] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating\nDeep Network Training by Reducing Internal Covariate Shift. In International\nConference on Machine Learning. 448\u2013456.\n[17] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization\nwith gumbel-softmax. arXiv preprint arXiv:1611.01144 (2016).\n[18] Manas R Joglekar, Cong Li, Jay K Adams, Pranav Khaitan, and Quoc V Le. 2019.\nNeural input search for large scale recommendation models. arXiv preprint\narXiv:1907.04471 (2019).\n[19] Manas R Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K Adams,\nPranav Khaitan, Jiahui Liu, and Quoc V Le. 2020. Neural input search for large\nscale recommendation models. In Proceedings of the 26th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Mining. 2387\u20132397.\n[20] Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin,\nLichan Hong, and Ed H Chi. 2020. Learning Multi-granular Quantized Embed-\ndings for Large-Vocab Categorical Features in Recommender Systems. arXiv\npreprint arXiv:2002.08530 (2020).\n[21] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and\nGuangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-\nteractions for recommender systems. In Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining.\n[22] Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon. com recommenda-\ntions: Item-to-item collaborative filtering. IEEE Internet computing 7, 1 (2003),\n76\u201380.\n[23] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,\nLi Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. 2018. Progressive\nneural architecture search. In Proceedings of the European Conference on Computer\nVision (ECCV). 19\u201334.\n[24] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable\narchitecture search. arXiv preprint arXiv:1806.09055 (2018).\n[25] Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. Neural\narchitecture optimization. In Proceedings of the 32nd International Conference on\nNeural Information Processing Systems. 7827\u20137838.\n[26] Raymond J Mooney and Loriene Roy. 2000. Content-based book recommending\nusing learning for text categorization. In Proceedings of the fifth ACM conference\non Digital libraries. ACM, 195\u2013204.\n[27] Hanh TH Nguyen, Martin Wistuba, Josif Grabocka, Lucas Rego Drumond, and\nLars Schmidt-Thieme. 2017. Personalized Deep Learning for Tag Recommen-\ndation. In Pacific-Asia Conference on Knowledge Discovery and Data Mining.\nSpringer.\n[28] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun,\nand Quan Lu. 2018. Field-weighted factorization machines for click-through\nrate prediction in display advertising. In Proceedings of the 2018 World Wide Web\nConference. 1349\u20131357.\n[29] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. 2018. Efficient\nNeural Architecture Search via Parameters Sharing. In International Conference\non Machine Learning. 4095\u20134104.\n[30] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.\n2016. Product-based neural networks for user response prediction. In 2016 IEEE\n16th International Conference on Data Mining (ICDM). IEEE, 1149\u20131154.\n[31] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2019. Regularized\nEvolution for Image Classifier Architecture Search. In Proceedings of the AAAI\nConference on Artificial Intelligence, Vol. 33. 4780\u20134789.\n[32] Steffen Rendle. 2010. Factorization machines. In Data Mining (ICDM), 2010 IEEE\n10th International Conference on. IEEE, 995\u20131000.\n[33] Paul Resnick and Hal R Varian. 1997. Recommender systems. Commun. ACM 40,\n3 (1997), 56\u201358.\n[34] Francesco Ricci, Lior Rokach, and Bracha Shapira. 2011. Introduction to recom-\nmender systems handbook. In Recommender systems handbook. Springer.\n[35] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.\nAutorec: Autoencoders meet collaborative filtering. In Proceedings of the 24th\ninternational conference on World Wide Web. 111\u2013112.\n[36] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,\nand Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-\nattentive neural networks. In Proceedings of the 28th ACM International Conference\non Information and Knowledge Management. 1161\u20131170.\n[37] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew\nHoward, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture\nsearch for mobile. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. 2820\u20132828.\n[38] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural\nnetworks for session-based recommendations. In Proceedings of the 1st Workshop\non Deep Learning for Recommender Systems. 17\u201322.\n[39] Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng\nZhang, and Dell Zhang. 2017. Irgan: A minimax game for unifying generative\nand discriminative information retrieval models. In Proceedings of the 40th In-\nternational ACM SIGIR conference on Research and Development in Information\nRetrieval. 515\u2013524.\n[40] Suhang Wang, Yilin Wang, Jiliang Tang, Kai Shu, Suhas Ranganath, and Huan Liu.\n2017. What your images reveal: Exploiting visual contents for point-of-interest\nrecommendation. In Proceedings of the 26th International Conference on World\nWide Web. International World Wide Web Conferences Steering Committee,\n391\u2013400.\n[41] Sai Wu, Weichao Ren, Chengchao Yu, Gang Chen, Dongxiang Zhang, and Jingbo\nZhu. 2016. Personal recommendation using deep recurrent neural networks in\nNetEase. In Data Engineering (ICDE), 2016 IEEE 32nd International Conference on.\nIEEE, 1218\u20131229.\n[42] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional factorization machines: Learning the weight of feature interac-\ntions via attention networks. arXiv preprint arXiv:1708.04617 (2017).\n[43] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. 2018. SNAS: stochastic\nneural architecture search. arXiv preprint arXiv:1812.09926 (2018).\n[44] Shuai Zhang, Lina Yao, and Aixin Sun. 2017. Deep Learning based Recommender\nSystem: A Survey and New Perspectives. arXiv preprint arXiv:1707.07435 (2017).\n[45] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-\nommender system: A survey and new perspectives. ACM Computing Surveys\n(CSUR) 52, 1 (2019), 1\u201338.\n[46] Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep learning over multi-field\ncategorical data. In European conference on information retrieval. Springer, 45\u201357.\n[47] Xiangyu Zhao, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing Liu, and\nJiliang Tang. 2020. AutoEmb: Automated Embedding Dimensionality Search in\nStreaming Recommendations. arXiv preprint arXiv:2002.11252 (2020).\n[48] Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. 2018. Practical\nblock-wise neural network architecture generation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition.\n[49] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through\nrate prediction. In Proceedings of the 24th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining. 1059\u20131068.\n[50] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2020. Fux-\niCTR: An Open Benchmark for Click-Through Rate Prediction. arXiv preprint\narXiv:2009.05794 (2020).\n[51] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement\nlearning. arXiv preprint arXiv:1611.01578 (2016).\n[52] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. 2018. Learning\ntransferable architectures for scalable image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition. 8697\u20138710.\n"
}