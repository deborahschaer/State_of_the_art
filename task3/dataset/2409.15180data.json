{
    "id": "2409.15180",
    "title": "A Comprehensive Survey with Critical Analysis for Deepfake Speech Detection",
    "abstract": "Thanks to advancements in deep learning, speech generation systems now power\na variety of real-world applications, such as text-to-speech for individuals\nwith speech disorders, voice chatbots in call centers, cross-linguistic speech\ntranslation, etc. While these systems can autonomously generate human-like\nspeech and replicate specific voices, they also pose risks when misused for\nmalicious purposes. This motivates the research community to develop models for\ndetecting synthesized speech (e.g., fake speech) generated by\ndeep-learning-based models, referred to as the Deepfake Speech Detection task.\nAs the Deepfake Speech Detection task has emerged in recent years, there are\nnot many survey papers proposed for this task. Additionally, existing surveys\nfor the Deepfake Speech Detection task tend to summarize techniques used to\nconstruct a Deepfake Speech Detection system rather than providing a thorough\nanalysis. This gap motivated us to conduct a comprehensive survey, providing a\ncritical analysis of the challenges and developments in Deepfake Speech\nDetection. Our survey is innovatively structured, offering an in-depth analysis\nof current challenge competitions, public datasets, and the deep-learning\ntechniques that provide enhanced solutions to address existing challenges in\nthe field. From our analysis, we propose hypotheses on leveraging and combining\nspecific deep learning techniques to improve the effectiveness of Deepfake\nSpeech Detection systems. Beyond conducting a survey, we perform extensive\nexperiments to validate these hypotheses and propose a highly competitive model\nfor the task of Deepfake Speech Detection. Given the analysis and the\nexperimental results, we finally indicate potential and promising research\ndirections for the Deepfake Speech Detection task.",
    "date": "2024-09-23T16:34:53+00:00",
    "fulltext": "A Comprehensive Survey with Critical Analysis\nfor Deepfake Speech Detection\nLam Pham1\u2217, Phat Lam2\u2217, Tin Nguyen3\u2217, Hieu Tang4, Huyen Nguyen5, Alexander Schindler6, Canh Vu7\nAbstract\u2014 Thanks to advancements in deep learning, speech\ngeneration systems now power a variety of real-world ap-\nplications, such as text-to-speech for individuals with speech\ndisorders, voice chatbots in call centers, cross-linguistic speech\ntranslation, etc. While these systems can autonomously generate\nhuman-like speech and replicate specific voices, they also pose\nrisks when misused for malicious purposes. This motivates the\nresearch community to develop models for detecting synthesized\nspeech (e.g., fake speech) generated by deep-learning-based\nmodels, referred to as the Deepfake Speech Detection task.\nAs the Deepfake Speech Detection task has emerged in recent\nyears, there are not many survey papers proposed for this\ntask. Additionally, existing surveys for the Deepfake Speech\nDetection task tend to summarize techniques used to construct\na Deepfake Speech Detection system rather than providing\na thorough analysis. This gap motivated us to conduct a\ncomprehensive survey, providing a critical analysis of the\nchallenges and developments in Deepfake Speech Detection.\nOur survey is innovatively structured, offering an in-depth\nanalysis of current challenge competitions, public datasets, and\nthe deep-learning techniques that provide enhanced solutions to\naddress existing challenges in the field. From our analysis, we\npropose hypotheses on leveraging and combining specific deep\nlearning techniques to improve the effectiveness of Deepfake\nSpeech Detection systems. Beyond conducting a survey, we\nperform extensive experiments to validate these hypotheses and\npropose a highly competitive model for the task of Deepfake\nSpeech Detection. Given the analysis and the experimental\nresults, we finally indicate potential and promising research\ndirections for the Deepfake Speech Detection task.\nItems\u2014 Deepfake speech detection (DSD), challenge compe-\ntition, ensemble, audio embedding, pre-trained model.\nI. INTRODUCTION\nIn recent years, remarkable advancements in deep learning\ntechniques and neural networks have revolutionized the field\nof generative AI. Today, core communication mediums such\nas audio, images, video, and text can be automatically\ngenerated and applied across various domains, including\nchatbot systems (e.g., ChatGPT), film production [10], code\ngeneration [11], and audio synthesis [12], [13], etc. However,\nAI-synthesized data could pose a serious threat to social se-\ncurity when there is an increasing number of crimes related to\nleveraging the synthesized data [14]. To address this concern,\nL. Pham and A. Schindler are with Austrian Institute of Technology,\nVienna, Austria.\nP. Lam and T. Nguyen are with HCM University of Technology, Ho Chi\nMinh city, Vietnam\nH. Tang is with FPT University, Ho Chi Minh city, Vietnam\nH. Nguyen is with Tokyo University of Agriculture and Technology,\nTokyo, Japan\nC. Vu is with Laboratory for Applied and Industrial Mathematics, Institute\nfor Computational Science and Artificial Intelligence, Van Lang University,\nHo Chi Minh City, Viet Nam\n(*) Main and equal contribution into the paper.\nthe tasks, which are proposed for detecting synthesized data\n(e.g. fake data) generated from deep-learning-based methods,\nreferred to as deepfake detection, have drawn much attention\nfrom the research community recently.\nFocusing on human speech, this paper provides a com-\nprehensive survey for the task of Deepfake Speech De-\ntection (DSD). To this end, the milestones presenting the\ndevelopment progress of the DSD task are first presented\nin Fig. 1. As the figure showns, the earliest public dataset\nand challenge proposed for the DSD task was introduced in\n2015, focusing exclusively on the English language. Then,\nthe first challenge for video deepfake detection (DFDC [15])\nwas introduced in 2020. In subsequent years, datasets for the\nDSD task in Japanese [16], Korean [16], and Chinese [17]\nwere introduced in 2021 and 2022, respectively. Recently,\nin 2024, multilingual datasets for the DSD task have been\npublished, including MLAAD [18] for conversational speech\nand SVDD [19] for singing. Fig. 1 also highlights a growing\nnumber of papers, datasets, and challenge competitions for\nthe DSD task from 2021 to the present. This trend indicates\nthat the DSD task has recently gained prominence and has\nattracted significant interest from the research community.\nTo further understand the DSD task, we summarized recent\nsurvey papers related to the DSD task in Table I. As shown\nin the table, most of these surveys focus on detecting general\nfake data (e.g., images, videos, audio, or text), with audio or\nhuman speech typically being addressed only as a subsection\nor a part of the broader discussion [8], [2], [3]. Therefore, the\nmain techniques, existing concerns, and potential research\nfor the DSD task have not been comprehensively analyzed\nin these papers. Among the survey papers, only two survey\npapers of [5] and [9] focus on the DSD task. However,\nas conventional surveys, these papers primarily summarize\nthe technologies used to construct a DSD system such as\ndatasets, feature extraction, classification model, loss func-\ntions, rather than providing a comprehensive analysis and\nhighlighting existing concerns. For instance, while challenge\ncompetitions proposed for the DSD task are very important\nin advancing the research community, their importance and\nvarious aspects have not been thoroughly analyzed (i.e., The\nnumber of research teams participating in these competitions\nand their results are interesting to analyzed, etc). Although\nthis information reflects the level of interest in DSD within\nthe research community, it has not been addressed in any\nexisting survey papers. The second concern is related to\npublic datasets proposed for the DSD task. In particular,\nthe current survey papers do not adequately analyze the\nimbalance among (1) the number of utterances, (2) the AI-\narXiv:2409.15180v1  [cs.SD]  23 Sep 2024\n2015\n2020\n2022\n2023\n2024\nFirst challenge for English speech \n(ASVspoof 2015-LA Task)\nFirst challenge for English video (DFDC)\n2021\nFirst dataset for Japanese\nFirst dataset for Korean\nFirst challenge for Chinese\n26 INTERSPEECH papers;\n9 ICASSP papers\nFirst dataset for spatial fake detection;\n23 INTERSPEECH papers;\n10 ICASSP papers\nFirst challenge for singing \n(SVDD 2024 , 6 languages);\nFirst  dataset for multiple languages \n(MLAAD, 23 languages);\n31 INTERSPEECH papers;\n18 ICASSP papers\nFig. 1.\nThe timeline of Deepfake Speech Detection (DSD) task\nTABLE I\nTHE MAIN FACTORS ANALYZED IN SURVEY PAPERS\nPapers\nYears\nAudio/Video\nChallenge\nPublic\nData\nFeature\nClassification\nLoss\nTraining\nProposed\nContinue\nCompetitions\nDatasets\nAugmentation\nExtraction\nModels\nFunctions\nStrategies\nModels\nUpdating\n[1]\n2021\nYes/Yes\nNo\nYes\nNo\nNo\nYes\nNo\nNo\nNo\nNo\n[2]\n2023\nYes/Yes\nNo\nNo\nNo\nYes\nYes\nNo\nNo\nNo\nNo\n[3]\n2023\nYes/Yes\nNo\nNo\nNo\nYes\nYes\nYes\nNo\nNo\nNo\n[4]\n2023\nYes/Yes\nNo\nYes\nNo\nNo\nYes\nYes\nYes\nNo\nNo\n[5]\n2023\nYes/No\nYes\nYes\nNo\nYes\nYes\nYes\nYes\nYes\nNo\n[6]\n2023\nYes/Yes\nNo\nYes\nNo\nNo\nYes\nNo\nNo\nNo\nNo\n[7]\n2024\nYes/Yes\nNo\nYes\nNo\nNo\nYes\nNo\nNo\nNo\nNo\n[8]\n2024\nYes/Yes\nNo\nYes\nNo\nYes\nYes\nNo\nNo\nNo\nNo\n[9]\n2024\nYes/No\nNo\nYes\nYes\nYes\nYes\nYes\nYes\nNo\nNo\nOur Survey\n2024\nYes/No\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nsynthesized speech systems used to generate fake speech; and\n(3) the original/real human speech resource used to generate\nfake speech utterances. These key factors are essential in\ncreating a high-quality DSD dataset for evaluating DSD\nmodels. Additionally, survey papers are at risk of becoming\noutdated as new datasets, techniques, and models continue\nto emerge. However, current surveys do not offer solutions\nfor regularly updating essential information, such as details\nabout challenge competitions, public datasets, and the top-\nperforming models on specific datasets. As regards technolo-\ngies used to construct a DSD model such as feature extrac-\ntion, classification model, or loss functions, current survey\npapers mainly summarize and then present conclusions rather\nthan conducting experiments to provide strong evidence and\nvalidation.\nGiven the existing concerns about the current survey\npapers for the DSD task, these motivate and inspire us to\nprovide a much more comprehensive survey in this paper.\nBy addressing these concerns, we mainly contribute:\n\u2022 We provide a comprehensive analysis and then indicate\nconcerns related to three main topics: The current chal-\nlenge competition, the published datasets, and the deep-\nlearning-based techniques used to develop a DSD sys-\ntem. We construct each topic by describing three main\nparts including \u2018Analysis\u2019, \u2018Discussion\u2019, and \u2018Contribu-\ntion\u2019. The \u2018Analysis\u2019 summarizes concrete information\nabout the topic. Meanwhile, the \u2018Discussion\u2019 indicates\nconcerns in each topic. Finally, the \u2018Contribution\u2019 pro-\nvides our suggestion and solution to further contribute\nto each topic.\n\u2022 To solve the out-of-date issue of a survey paper, we set\nup a Github repository to update further challenge com-\npetitions, public datasets, and top-performance systems.\n\u2022 More than a survey, we conduct extensive experiments\nto verify assumptions from the comprehensive analysis,\nachieving a competitive DSD model. Given the analysis\nand experimental results, we indicate potential research\ndirections for the DSD task.\nThe remainder of this paper is structured as follows:\nSection II discusses challenge competitions for the DSD\ntask. Section III deeply analyses the public and benchmark\ndatasets proposed for the DSD task. In Section IV, we\nsummarize the key techniques for constructing the main\ncomponents of a DSD system, including data augmentation,\nfeature extraction, classification models, and loss functions\nSection V presents extensive experiments that validate the\ntechniques described in Section IV. Building on the analysis\nand results from the previous sections, Section VI outlines\nour proposed research directions in the DSD task. Finally,\nSection VII concludes the paper.\nII. CHALLENGE COMPETITIONS PROPOSED FOR\nDEEPFAKE SPEECH DETECTION\nAnalysis: Challenge competitions for the DSD task play\na crucial role in motivating the research community. These\ncompetitions not only introduce new benchmark datasets\nbut also host workshops where research teams can discuss\ntheir ideas and share their motivations. This environment\nencourages the community to publish more datasets and\nTABLE II\nTHE CHALLENGE COMPETITIONS PROPOSED FOR DEEPFAKE SPEECH DETECTION\nChallenge Competitions\nYears\nData Types\nLanguages\nPublic Labels\nAudio\nVisual\nTeam No.\nTop-1\n(Number)\n(train&dev/test)\nSystem\nASVspoof 2015 [20]\n2015\nSpeech\nEnglish\nYes/Yes\nYes\nNo\n16\nEnsemble Model\nASVspoof 2019 (LA Task) [21]\n2019\nSpeech\nEnglish\nYes/Yes\nYes\nNo\n48\nEnsemble Model\nDFDC [15]\n2020\nSpeech\nEnglish\nYes/Yes\nYes\nYes\n2114\nEnsemble Model\nFTC [22]\n2020\nSpeech\nEnglish\nNo/No\nYes\nNo\nn/a\nn/a\nASVspoof 2021 (LA Task) [23]\n2021\nSpeech\nEnglish\nYes/Yes\nYes\nNo\n41\nEnsemble Model\nASVspoof 2021 (DF Task) [23]\n2021\nSpeech\nEnglish\nYes/Yes\nYes\nNo\n33\nEnsemble Model\nADD 2022 Track 1 [17]\n2022\nSpeech\nChinese\nYes/Yes\nYes\nNo\n48\nSingle Model\nADD 2022 Track 2 [17]\n2022\nSpeech\nChinese\nYes/Yes\nYes\nNo\n27\nSingle Model\nADD 2022 Track 3.2 [17]\n2022\nSpeech\nChinese\nYes/Yes\nYes\nNo\n33\nSingle Model\nADD 2023 Track 1.2 [24]\n2023\nSpeech\nChinese\nNo/No\nYes\nNo\n49\nEnsemble Model\nADD 2023 Track 2 [24]\n2023\nSpeech\nChinese\nNo/No\nYes\nNo\n16\nSingle Model\nAV-Deepfake1M [25], [26]\n2024\nSpeech\nEnglish\nYes/No\nYes\nYes\nn/a\nn/a\nASVspoof 2024 [27]\n2024\nSpeech\nEnglish\nYes/No\nYes\nNo\n53\nEnsemble Model\nSVDD 2024 [28], [19]\n2024\nSinging\nMultilanguages (6)\nYes/No\nYes\nNo\n47\nEnsemble Model\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nYears\n1\n0\n1\n2\n3\n2\n3\nNumber of challenges\nFig. 2.\nThe number of competitions proposed for DSD task from 2015\ndevelop new techniques to address the DSD challenges. To\nanalyze DSD challenge competitions, we first summarize all\nchallenges in Table II. Importantly, we will continuously\nupdate information about future DSD challenge competitions\nin our GitHub repository1.\nAs Table II shows, most challenge competitions focus\non detecting fake speech in a conversation except for the\nSVDD 2024 challenge [28] for the fake singing detection.\nAll challenge competitions for fake speech detection in a\nconversation have been proposed for a single language (i.e.,\nWhile ADD 2022 and ADD 2023 are for Chinese, the others\nare proposed for English). Regarding the number of DSD\nchallenge competitions, Fig. 2 shows that there has been\nan increase in recent years. This trend indicates that the\nDSD task has gained attention from the research commu-\nnity, particularly due to the rise of advanced deep learning\nsystems capable of generating highly realistic human-like\nspeech, which poses significant security risks. DSD challenge\ncompetitions, which explore fake speech in a conversation,\ncan be separated into two groups. The first group is proposed\nfor only audio [20], [29], [21], [23], [27], [22], [17], [24].\nMeanwhile, the second group is for video in which a fake\nvideo is identified by fake audio, fake image, or both fake\naudio and image [15], [26]. This indicates that DSD is not\nonly treated as an individual task independently but also\n1https://github.com/AI-ResearchGroup/A-Comprehensive-Survey-with-\nCritical-Analysis-for-Deepfake-Speech-Detection\nconsidered as a sub-task in multimodal systems. It is also\nevident that the second group, which focused on fake video\ndetection, has attracted significantly more research teams\n(e.g. 2,114 teams in the DFDC challenge [15]) compared\nto the first group (e.g. the largest team count was 54 in the\nASVspoof 2021 challenge [23]). This provides an insight\nthat fake video detection is a more compelling task, draw-\ning greater interest and participation from research teams.\nRegarding top-1 systems in these challenge competitions,\nthey leveraged the ensemble techniques which combine a\nwide range of input features or multiple models (i.e., most\nsubmitted systems mainly use deep learning based models).\nDiscussion: Given the recent analysis of challenge com-\npetitions proposed for the DSD task, some concerns can be\nindicated. Firstly, the DSD task has drawn attention from\nthe research community and is now recognized as one of\nthe critical components in a complex system of deepfake\ndetection. However, most current challenge competitions\nare limited to single languages, such as Chinese or En-\nglish, and primarily focus on detecting fake speech within\nconversations. Second, some challenge competitions have\nnot published datasets for different reasons. For example,\nFTC [22] was organized by the US government, and the\ntop-performing systems are used by the US government.\nSimilarly, ADD 2023 [24] only provides the dataset for the\nteams that attended during the competition. These limitations\nhinder research motivation and further development once\nthe challenges conclude. Third, it is recognized that fake\nspeech utterances are mainly generated from deep-learning-\nbased speech generation systems. Therefore, if selected deep-\nlearning-based speech generators are not general or up-to-\ndate, this significantly affects the effectiveness and qual-\nity of the challenge competition. This highlights the need\nfor collaboration between two tasks of deep-learning-based\nspeech generation and detection within the same challenge\ncompetition. Competitions like ASVspoof 2024 [27] and\nADD 2022 [17] have addressed this by not only published\ndatasets but also presented a two-phase or two-track chal-\nlenge in which the first phase/track is for Deepfake Speech\nGeneration and the second phase/track is for Deepfake\nSpeech Detection. Finally, regarding techniques used in these\ncompetitions, ensemble models have become widely used to\nTABLE III\nPUBLIC AND BENCHMARK DATASETS PROPOSED FOR DEEPFAKE SPEECH DETECTION\nDatasets\nYears\nLanguages\nSpeakers\nUtt. No.\nFake speech\nSpeech\nReal Speech\nUtt. length (s)\nEvaluation\n(Male/Female)\n(Real/Fake)\nGenerators\nCondition\nResources\nMetrics\nASVspoof 2015 [20](audio)\n2015\nEnglish\n45/61\n16,651/246,500\n10\nClean\nSpeaker Volunteers\n1 to 2\nEER\nFoR [30](audio)\n2019\nEnglish\n140\n-/195541\n7\nClean\nKaggle [31]\n2.35\nAcc\nASVspoof 2019 (LA task) [21](audio)\n2019\nEnglish\n46/61\n12,483/108,978\n19\nClean\nSpeaker Volunteers\nn/a\nEER\nDFDC [15](video)\n2020\nEnglish\n3426\n128,154/104,500\n1\nClean & Noisy\nSpeaker Volunteers\n68.8\nPre., Rec.\nASVspoof 2021 (LA task) [23](audio)\n2021\nEnglish\n21/27\n18,452/163,114\n13\nClean & Noisy\nSpeaker Volunteers\nn/a\nEER\nASVspoof 2021 (DF task) [23](audio)\n2021\nEnglish\n21/27\n22,617/589,212\n100+\nClean & Noisy\nSpeaker Volunteers\nn/a\nEER\nWaveFake [16](audio)\n2021\nEnglish,\n0/2\n-/117,985\n6\nClean\nLJSPEECH [32],\n6/4.8\nEER\nJapanese\nJSUT [33]\nKoDF [34](video)\n2021\nKorean\n198/205\n62,116/175,776\n2\nClean\nSpeaker Volunteers\n90/15 (real/fake)\nAcc, AuC\nADD 2022 [17]\n2022\nChinese\n40/40\n3012/24072\n2\nClean\nAISHELL-3 [35]\n1 to 10\nEER\nFakeAVCeleb [36](video)\n2022\nEnglish\n250/250\n570/25,000\n2\nClean & Noisy\nVox-Celeb2 [37]\n7\nAuC\nIn-the-Wild [38](video)\n2022\nEnglish\n58\n19963/11816\n0\nClean & Noisy\nSelf-collected\n4.3\nEER\nLAV-DF [39](video)\n2022\nEnglish\n153\n36,431/99,873\n1\nClean & Noisy\nVox-Celeb2 [37]\n3 to 20\nAP\nVoc.v [40](audio)\n2023\nEnglish\n46/61\n14,250/41,280\n5\nClean & Noisy\nASVspoof 2019\nn/a\nEER\nPartialSpoof [41](audio)\n2023\nEnglish\n46/61\n12,483/108,978\n19\nClean & Noisy\nASVspoof 2019\n0.2 to 6.4\nEER\nLibriSeVoc [42](audio)\n2023\nEnglish\nn/a\n13,201/79,206\n6\nClean & Noisy\nLibrispeech\n5 to 34\nEER\nAV-Deepfake1M [25], [26](video)\n2023\nEnglish\n2,068\n286,721/860,039\n2\nClean & Noisy\nVoxceleb2 [37]\n5 to 35\nAcc, AuC\nCFAD [43](audio)\n2024\nChinese\n1023\n-/374,000\n11\nClean & Noisy\nAISHELL1-3 [44], [45]\nn/a\nEER\n& Codecs\nMAGICDATA [46]\nMLAAD [47](audio)\n2024\nMultilanguages (23)\nn/a\n-/76,000\n54\nClean & Noisy\nM-AILABS [18]\nn/a\nAcc\nASVspoof 2024 [27](audio)\n2024\nEnglish\nn/a\n188,819/815,262\n28\nClean & Noisy\nMLS [48]\nn/a\nEER\nSVDD2024 [28](audio)\n2024\nMutilanguages (6)\n59\n12,169/72,235\n48\nClean\nMandarin,\nn/a\nEER\nJapanese\nenhance performance in many challenge competitions, often\nleading research teams to develop top-performing systems.\nHowever, this approach has several drawbacks, including\nlimited explainability, increased system complexity, high\ntraining costs, and concerns related to power consumption\nand green AI. Therefore, different aspects of using deep-\nlearning-based models such as using a single model, low\ncomplexity, or real-time inference can be regarded as main\nconstraints in challenge competitions for the DSD task in the\nfuture. For example, the DCASE challenge Task 1 [49] for\nSound Scene Classification requires the submitted systems to\nobey two constraints: (1) not larger than 128 K parameters\nand (2) not larger than 30 MMAC units.\nOur contribution: Given the analysis and the discussion\nabout the DSD challenge competitions above, our work\nfurther motivates the research community by:\n\u2022 We present and highlight the important role of DSD\nchallenge competitions. We then provide a comprehen-\nsive analysis and indicate the concerns.\n\u2022 We continue updating new challenge competitions in the\nfuture by creating a Github project2. The GitHub reposi-\ntory serves as a reference for up-to-date information on\nchallenge competitions and current concerns. In other\nwords, it provides a summary of challenge competitions\nrelated to the DSD task, ensuring that this survey paper\nremains current.\nIII. PUBLIC DATASETS PROPOSED FOR DEEPFAKE\nSPEECH DETECTION\nAnalysis: Public datasets proposed for the DSD task,\nincluding those introduced through challenge competitions,\nplay a crucial role in motivating the research community\nto develop and evaluate DSD systems. In this section, we\npresent a summary of the public and benchmark datasets for\nthe DSD task, as shown in Table III. These datasets have\nbeen introduced through various challenge competitions and\npublished papers.\n2https://github.com/AI-ResearchGroup/A-Comprehensive-Survey-with-\nCritical-Analysis-for-Deepfake-Speech-Detection\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nYears\n1\n0\n2\n1\n4\n5\n3\nNumber of datasets\nFig. 3.\nThe number of public datasets proposed for DSD task from 2015\nAs illustrated in Fig.3, the number of public datasets for\nthe DSD task has grown significantly in recent years. Most of\nthese datasets include both clean and noisy speech. Notably,\nnearly all datasets have been designed for English, with\nWaveFake [16], KoDF [50], and ADD 2022 [17] being the\nexceptions, focusing on Japanese, Korean, and Chinese lan-\nguages, respectively. Recently, the first multilingual datasets\nfor the DSD task were introduced in [47] and [19]. The\nMLAAD dataset [18] provides fake speech in conversations\ngenerated in 23 widely spoken languages. Meanwhile, the\nSVDDD dataset [19] was proposed for deepfake singing\ndetection with six different languages (i.e., the Chinese songs\nare the majority).\nMost deepfake datasets are generated from one of three\ngenerator techniques: Text-to-Speech (TTS), Voice Conver-\nsion (VC), and Adversarial Attacks (AT), as shown in\nTable IV. Notably, ASVspoof 2024 [27] is the first dataset\nthat uses AT systems to generate fake speech. While TTS\nsystems generate fake speech from text, VC systems generate\nfake speech from real speech (e.g. audio). To mimic the target\nspeakers, TTS and VC systems attempt to explore the audio\nembeddings extracted from the target speakers. The audio\nembeddings extracted from target speakers are treated as a\npart of the feature map in the entire network architecture in\nTTS and VC systems. Regarding AC systems, they mainly\napply Malafide [124] and Malocopula [125] methods to gen-\nTABLE IV\nDEEPFAKE SPEECH GENERATION SYSTEMS USED IN PUBLIC DSD DATASETS\n(TTS: TEXT TO SPEECH, VC: VOICE CONVERSION, AT: ADVERSARIAL ATTACH USING MALAFIDE OR MALOCOPULA)\nDatasets\nYear\nNo. of TTS/VC/AT\nDeepfake Speech Generation Systems\nASVspoof 2015 [20]\n2015\n7 VC, 3 TTS\nVC-01 [51], [52], VC-02 [53], TTS-01 [54], TTS-02 [54], VC-03 [55],\nVC-04 [56], VC-05 [56], VC-06 [57], VC-07 [58], TTS-03 [59]\nFoR [30]\n2019\n7 TTS\nDeep Voice 3, Amazon AWS Polly, Baidu TTS, Google Traditional TTS,\nGoogle Cloud TTS, Google Wavenet TTS, Microsoft Azure TTS\nASVspoof 2019 (LA task) [21]\n2019\n8 VC, 11 TTS\nTTS-01 [60], TTS-02 [60], [61], TTS-03 [62], TTS-04 [63], VC-01 [64], VC-02 [65],\nTTS-05 [62], [66], TTS-06 [60], [67], TTS-07 [68], [69], TTS-08 [70], [71], TTS-09 [70], [71], [72],\nTTS-10 [73], VC-03+TTS [74], VC-04+TTS [75], [76], VC-05+TTS [75], [76], TTS-11 [63],\nVC-06 [77], [78], VC-07 [79], [80], [81], VC-08 [65]\nDFDC [15]\n2020\n1 TTS\nTTS Skins voice conversion [82]\nKoDF [34]\n2021\n2 TTS\nATFHP [50] and Wav2Lip [83]\nASVspoof 2021 (LA task) [23]\n2021\n13 TTS/VC\nReuse ASVspoof 2019\nASVspoof 2021 (DF task) [23]\n2021\n100 TTS/VC\nVocoders [84]\nWaveFake [16]\n2021\n6 TTS\nMelGAN [85], FB-MelGAN [85], HiFi-GAN [86], WaveGlow [87], PWG [88], MB-MelGAN [85]\nFakeAVCeleb [36]\n2022\n2 TTS\nSV2TTS [89], [90]\nIn-the-Wild [38]\n2022\nn/a\nn/a\nLAV-DF [39]\n2022\n1 TTS\nSV2TTS [91]\nVoc.v [40]\n2023\n5 TTS\nHiFi-GAN [86], MB-MelGAN [85], WaveGlow [87], PWG [88], Hn-NSF [92]\nPartialSpoof [41]\n2023\n21 TTS/VC\nReuse ASVspoof 2019\nLibriSeVoc [42]\n2023\n6 TTS/VC\nWaveNet [73], WaveRNN [93], MelGAN [85], Parallel WaveGAn [94], WaveGrad [95], DiffWave [96]\nAV-Deepfake1M [25], [26]\n2023\n2 TTS\nVITS [97], YoursTTS [98]\nCFAD [43]\n2024\n11 TTS\nSTRAIGHT [99], Griffin-Lim [100], LPCNet [101], WaveNet [73], PWG [88], HiFi-GAN[102],\nMB-MelGAN [85], MelGAN [85], WORLD [103], FastSpeech [104], Tacotron-HifiGAN [105]\nMLAAD [47]\n2024\n54 TTS\nBark, Capacitron, FastPitch, GlowTTS, Griffin Lim, Jenny, NeuralHMM, Overflow,\nParler TTS, Speech5, Tacotron DDC, Tacotron2, Tacotron2 DCA, Tacotron2 DH, Tcotron2-DDC,\nTortoise, VITS, VITS Neon, VITS-MMS, XTTS v1.1, XTTS v2\nASVspoof 2024 [27]\n2024\n15 TTS, 6 VC, 7 AT\nTTS-01 [106], TTS-02 [107], TTS-03 [108], TTS-04 [109], TTS-05 [110], TTS-06[111], TTS-07[112],\nTTS-08(self-develop), VC-01[113], TTS-09[114], VC-02 [115], VC-03(self-develop), TTS-10 [116],\nAT-01 (Malafide+TTS-10 [116]), TTS-11 [117], AT-02(self-Develop), TTS-12 [118], TTS-13 [119],\nAT-03(Malafide+TTS [120]), VC-04(self-develop), VC-05 [121][24], VC-06(add noise),\nAT-04(Malacopula+VC-06), TTS-14 [122], TTS-15 [123], AT-05(Malacopula+AT-01),\nAT-06(Malacopula+TTS-13 [119]), AT-07(Malacopula+VC-05 [121])\nerate fake speech. Both Malafide [124] and Malocopula [125]\nmethods involve leveraging filter banks. Malafide [124] ap-\nplies multiple techniques of linear time-invariant (LTI), non-\ncausal filter, and the coefficients (e.g., tap weights) to create\nTTS/VC-based fake speech that mimics the target speaker.\nMeanwhile, Malocopula [125] combines both linear filter and\nnon-linear filter (e.g., one-dimensional convolutional layer)\nto replicate the target speaker\u2019s voice.\nTo compare among DSD datasets, we analyze three dif-\nferent aspects: (1) the number of fake utterances; (2) the AI-\nsynthesized speech systems used to generate fake speech; and\n(3) the original/real human speech resource used to generate\nfake speech utterances. As Table III shows, most datasets\npresent lower than 300,000 utterances of fake speech, ex-\npected ASVspoof 2021 (DF Task) [23], ASVspoof 2024 [27],\nAV-Deepfake1M dataset [25], [26], and DFDC dataset [15]\nwith 589212, 815262, 860039, and 104500 fake samples,\nrespectively. Although DFDC [15], [82] and AV-Deepfake1M\ndataset [25], [26] present a large number of fake data, this\nwas proposed for video in which audio may not be fake. Ad-\nditionally, these fake utterances were generated from only a\nfew deep-learning-based speech-generation systems. Indeed,\ntwo TTS models of VITS [97], YoursTTS [98] and one TTS\nmodel [82] were used to generate fake speech in DFDC [15]\nand AV-Deepfake1M dataset [25], [26] datasets, respectively.\nOn the other hand, the ASVspoof 2021 (DF Eva) dataset [23]\ncontains 589212 fake utterances, generated using over 100\nvoice conversion (VC) and text-to-speech (TTS) systems. To\ncatch up with state-of-the-art deepfake speech generators,\nTable IV presents the architectures and resources of deepfake\nspeech generators. The table indicates that the ASVspoofing\nseries shows up-to-date and diverse deepfake speech gener-\nators compared to the others. In terms of the original human\nspeech resources, most DSD datasets are based on recordings\nfrom a limited number of speaker volunteers. For example,\nalthough the ASVspoof 2021 (DF Eva) dataset [23] used\n100 VC and TTS systems to create fake utterances, the real\nspeech resource is from 107 speaker volunteers. Some DSD\ndatasets of AV-Deepfake1M [25], [26], CFAD [43] leveraged\nthe large and available human speech datasets to generate\nfake utterances such as Voxceleb2 [37], AISHELLI-3 [35],\nMAGICDATA [46]. However, these datasets use a limited\nnumber of speech generators (e.g. 2 TTS and 11 TTS for\nAV-Deepfake1M [25], [26] and CFAD [43], respectively).\nRegarding metrics evaluation, all datasets proposed for the\nDSD task come together with a baseline and metrics for\nthe evaluation. Regarding the baseline systems, all baselines\nleveraged convolutional neural network (CNN) based archi-\ntectures. These baselines are evaluated mainly by the Equal\nError Rate (EER) metric. Some datasets such as KoDF [34],\nAV-Deepfake1M [25], [26], MLAAD [47], FoR [30] used\nAccuracy (Acc.) and Area Under The Curve (AUC) metrics\ninstead of EER.\nDiscussion: Given the analysis of benchmark datasets pro-\nposed for the DSD task, some existing issues can be outlined.\nThese include the limited number of datasets available for\nmultiple languages and the imbalance of several aspects\nwithin existing datasets.\nFirstly, more public and benchmark datasets have been\nproposed for the DSD task. However, there is only one mul-\ntilingual dataset currently. The lack of multilingual datasets\nfor DSD tasks presents several challenges for current model\ndevelopment and evaluation such as performance degradation\non cross-language settings that leads to a limited applicability\nOriginal\nUtterance\nAugmented\nUtterances\nspectrograms\nFeature \nExtraction\nClassification \nModel\nOffline Data\nAugmentation\n.\n.\ne1\ne2\neC\nembedding\npfake\npreal\nPredicted \nProbabilities\nn-dim Tensors\nLoss Function\nFig. 4.\nThe high-level architecture of Deepfake Speech Detection (DSD) systems\n \n Real Speakers\nFake Utterances \n \n \n \n Fake Speech Generators\n \n Real Speakers\n106\n246500\n10\n107\n108978\n19\n48\n163114\n13\n48\n589221\n100\n80\n24072\n2\n107\n41280\n5\n2\n117985\n6\n1033\n347400\n11\nASVspoof 2015\nASVspoof 2019\nASVspoof 2021 LA\nASVspoof 2021 DF\nADD 2022\nVoC.v\nWaveFake\nCFAD\nFig. 5.\nThe imbalance among the fake speech utterances, the fake speech\ngenerators, and the real speaker volunteers in benchmark DSD datasets\nin real-world applications. This motivates the research com-\nmunity to propose more datasets for multiple languages to\nenhance model\u2019s capability in real-life settings. Secondly, we\nalso highlight an imbalance among DSD datasets regarding\nthree aspects: (1) the number of fake utterances; (2) the AI-\nsynthesized speech systems used to generate fake speech;\nand (3) the original/real human speech resource used to gen-\nerate fake speech utterances. The imbalance can be clearly\ndescribed in Fig. 5 regarding DSD datasets using speaker\nvolunteers.\n\u2022 The number of utterances: The quantity of utter-\nances within the datasets is not uniform. Some datasets\nmay contain a large number of samples, while others\nhave significantly fewer. A small number of real or\nfake utterances within datasets (e.g. FakeAVCeleb [36],\nADD [17]) limits the model\u2019s exposure to a wide\nvariety of speech patterns and scenarios, affecting the\ndetection robustness and generalization new, unseen\ndata. Additionally, a controlled ratio between real and\nfake samples created within datasets (e.g ASVspoof\n2024 [27], ASVsproof 2021 [23]) also ensure diversity\nof fake techniques and avoid overfitting on the fake\ndata, especially if the fake samples are generated using\nsimilar techniques. Therefore, maintaining a moderately\ncontrolled ratio between real and fake utterances, along\nwith a diverse range of these utterances, is essential for\nfuture dataset development.\n\u2022 Deepfake speech generation systems: The variety of\ndeep-learning-based systems used to generate deepfake\nspeech is another area of concern. As Table IV shows,\nsome of datasets such as MLAAD [47], ASVspoof\n2021(DF task) [23], ASVspoof 2024 [27] present more\nthan 20 systems (e.g. TTS, VC, or AT systems). Among\nthese datasets, ASVspoof 2021 (DF Task) [23] and\nASVspoof 2024 [27] present diverse TTS, VC, and\nAT systems. In particular, while more than 100 TTS\nand VC are for ASVspoof 2021 (DF Task) [23], 28\nTTS, VC, and AT are used in ASVspoof 2024 [27].\nAlthough MLAAD [47] has been the unique multiple-\nlanguage dataset currently, fake speech in this dataset\nwas only generated from TTS systems (e.g. 54 TTS\nsystems). Overall, some datasets may predominantly\nfeature speech synthesized by a few specific deep-\nlearning-based generators or techniques, while others\nmight include a broader range. Datasets generated from\na limited number of deep-learning-based generators pos-\nsibly lead to over-specialization, reducing the model\u2019s\nability to detect deepfakes generated by other systems\nand affecting the performance in real-world scenarios.\nTherefore, this imbalance motivates the research com-\nmunity to create more diverse datasets that include a\nwide range of AI-synthesized speech methods.\n\u2022 Real human speech resource: The source of real voice\nplays a crucial role in shaping the effectiveness, gen-\neralizability, and ethical aspects of deepfake detection\nmodels. As highlighted in Table IV, there are two main\nsources for building DSD datasets: voice samples from\nvolunteer speakers or from existing datasets. Voice sam-\nples from volunteers offer greater control over diversity\n(if managed thoroughly) and address ethical concerns,\nas they are collected with explicit informed consent.\nHowever, this approach can be resource-intensive in\nterms of time and cost and may not scale efficiently. In\ncontrast, utilizing existing human speech datasets offers\nbetter accessibility and scalability. However, it may\nintroduce biases toward certain groups, such as public\nfigures, reducing diversity in real-world applications\nand especially raising significant ethical issues. These\nproblems suggest other balanced approaches to build\nDSD datasets that consider both diversity and scalability\nin the future.\nBased on the above discussions and statistic informa-\ntion in Fig. 5, it can be concluded that ASVspoof 2019\n(LA task) [21], ASVspoof 2021 (LA & DF tasks) [23],\nASVspoof 2024 [27] are among the most balanced datasets\nTABLE V\nINDIVIDUAL DSD SYSTEMS EXPLORING RAW AUDIO\nSystems\nYears\nDatasets\nFeatures\nData Augmentation\nModels\nLoss Functions\n(Distoration/Compression)\n[126]\n2021\nASVspoof 2021 (LA Task)\nRaw Audio\nComp.: MP3, ACC, OGG\nRawNet2\nFocal loss\n[127]\n2021\nASVspoof 2021 (LA&DF Tasks)\nRaw Audio\nComp.: G.723, G.726,\nRawNet2\nCross Entropy (CE)\nGSM, opus, speex, mp2,\nogg, tta, wma, acc, ra\n[128]\n2021\nASVspoof 2019 (LA Task)\nRaw Audio\nDis.: Channel Drop,\nSinC+CRNN\nMSE Loss\nFrequency masking\n[129]\n2021\nASVspoof 2021 (LA Task)\nRaw Audio\nComp.: mp3, mp2, m4a, m4r,\nRawNet2\nOC-Softmax\nopus, ogg, mov, PCM \u00b5-law,\nPCM a-law, speex, ilbc,\nG.729, GSM, G.722, AMR\n[130]\n2021\nASVspoof 2021 (LA&DF Tasks)\nRaw Audio\nDis.: Time-wise,\nRawNet2\nCross Entropy\nSilence Strimming\n[131]\n2021\nASVspoof 2021 (LA&DF Tasks)\nRaw Audio\nn/a\nEncoder: SinC+Residual\nWCE Loss\nDecoder: Graph Attention Network\n[132]\n2021\nASVspoof 2021 (LA&DF Tasks)\nRaw Audio\nDis.: Mixup, FIR filters\nSinc+CNN\nWCE Loss\n[133]\n2021\nASVspoof 2021 (LA Task)\nRaw Audio\nComp.: G.711-alaw,G.722,\nSinC+RawNet2\nAM-softmax\nGSM-FR, and G.729\n[38]\n2022\nASVspoof 2019 (LA Task)\nRaw Audio\nn/a\nRawNet2, RawNet-GAT, CRNNSpoof\nCross Entropy\nIn The Wild\n[134]\n2022\nASVspoof 2019 (LA Task)\nRaw Audio\nn/a\nEncoder: RawNet2\nWCE Loss\nDecoder: Graph Attention Neural Network\n[135]\n2022\nASVspoof 2021 (LA&DF Tasks)\nRaw Audio\nDis.: RawBoost [136]\nEncoder: Sinc+CNN, Wave2Vec2.0+CNN\nWCE loss\nDecoder: Graph Attention network\n[137]\n2023\nASVspoof 2019 (LA Task),\nRaw Audio\nDis.: Stereo speech\nEncoder: SinC+ResNet\nAM-softmax\nASVspoof 2021 (LA&DF Tasks)\nDecoder: Graph Attention network\n[138]\n2023\nASVspoof 2019 (LA Task)\nRaw Audio\nn/a\nEncoder: Wav2vec2.0 [139], HuBERT [140]\nCross Entropy\nDecoder: LCNN-LSTM-Graph Attention\n[141]\n2023\nADD 2023\nRaw Audio\nDis.: Add noise, mix utterance\nEncoder: Wav2Vec2.0\nCross Entropy\nDecoder: ECAPA-TDNN\n[142]\n2022\nASVspoof 2019 (LA Task),\nRaw Audio\nn/a\nEncoder: ECAPA-TDNN, RawNet\nCross Entropy,\nDecoder: Linear layers\nTriplet loss,\nAM-Softmax\n[143]\n2023\nADD 2023\nRaw Audio\nDis.:Add noise, vibration, mixup\nEncoder: Wav2Vec2.0\nA-Softmax,\nDecoder:CNN-Transformer\nTriplet loss,\nAdversial loss\n[144]\n2023\nASVspoof 2019 (LA Task),\nRaw Audio\nn/a\nEncoder: Wav2Vec2.0 [139]\nTriplet, BCE,\nWaveFake,\nDecoder: LCNN-Transformer\nAdversarial loss\nFakeAVCeleb\n[145]\n2024\nASVspoof 2019 (LA Task),\nRaw Audio\nn/a\nSincNet/LEAF+ResNet\nCross Entropy\nASVspoof 2021 (LA&DF Tasks),\nIn The Wild [38]\n[145]\n2024\nASVspoof 2021 (LA&DF Tasks)\nRaw Audio\nn/a\nEncoder: EnCodec [146], AudioDec [147],\nCross Entropy\nAudioMAE [148], HuBERT [140],\nWavLM [149], Whisper [150]\nDecoder: ResNet\n[151]\n2024\nASVspoof 2019 (LA Task),\nRaw Audio\nDis.: Add noise, overlapping\nEncoder: WavLM [149],\nCross Entropy\nASVspoof 2021 (LA&DF Tasks)\nDecoder: Multi-Fusion Attentive\n[152]\n2024\nASVspoof 2019 (LA Task),\nRaw Audio\nn/a\nEncoder: Wav2vec2.0 [139], BEATS [153],\nn/a\nASVspoof 2021 (LA Task),\nLationCLAP [154], AudioCLIP [155],\nIn The Wild\nDecoder: Similarity Score Measurement\nat the writing time. Additionally, the MLAAD [47] is the\nlargest and most suitable DSD dataset for evaluating cross-\nlanguages. The discussions on existing datasets for the DSD\ntask underscore the importance of future efforts by the\nresearch community to release comprehensive, multilingual,\nand balanced datasets. Also, Fig. 5 emphasizes the significant\ncosts and workload involved in creating such datasets, while\nensuring compliance with essential security protocols for\nspeaker volunteers.\nOur contribution: Given the analysis and the discussion\nabove, our work mainly contributes:\n\u2022 We focus on the important role of public datasets\nproposed for the DSD task, providing a comprehensive\nanalysis and indicating the existing issues. The analysis\nshows different aspects that are not mentioned in the\nother surveys: (1) We report the original resource of\nreal human speech; (2) We provide an overview of deep\nlearning-based systems used to generate fake speech; (3)\nThe survey is not only for fake speech but also for fake\nvideo; (4) We highlight imbalances and other concerns\nin current public DSD datasets, along with their impact\non model performance and practical applicability.\n\u2022 In line with the evolution of challenge competitions,\nwe will continue to update new DSD datasets via\nour GitHub repository3 in the future. This ensures the\nongoing relevance of the survey and provides an up-to-\ndate resource for DSD datasets.\nIV. OVERVIEW ON PROPOSED SYSTEMS FOR DEEPFAKE\nSPEECH DETECTION\nTo conduct a comprehensive analysis of DSD systems, we\nfirst review state-of-the-art research papers addressing the\nDSD task. Notably, a large number of the selected papers\nare from high-reputation journals and conferences such as\nINTERSPEECH (48 papers) and ICASSP (29 papers) in\nrecent years. Then, we categorize these DSD systems into\n3https://github.com/AI-ResearchGroup/A-Comprehensive-Survey-with-\nCritical-Analysis-for-Deepfake-Speech-Detection\nTABLE VI\nINDIVIDUAL DSD SYSTEMS EXPLORING SPECTROGRAM BASED FEATURES\nSystems\nYears\nDatasets\nData Augmentation\nFeatures\nModels\nLoss Functions\n(Distoration/Compression)\n[156]\n2020\nASVspoof 2019 (LA Task)\nDis.: Add noise, reverberation,\nLFCC\nResNet\nLMC loss,\nFreqAugment\nCross Entropy\n[126]\n2021\nASVspoof 2021 (LA Task)\nComp.: MP3, ACC, OGG\nLFCC\nLCNN\nFocal loss,\nMEL\nTDNN\nFocal, Cross Entropy\n[157]\n2021\nASVspoof 2021 (LA Task)\nn/a\nLFB, SPEC, LFCC\nLCNN, LCNN-LSTM\nCross Entropy, MSE\n[158]\n2021\nASVspoof 2021 (LA Task)\nComp.: MP3, ACC,\nLFCC\nECAPA-TDNN\nFocal loss\nlandlie, cellular, VoiP\n[127]\n2021\nASVspoof 2021 (LA&DF Tasks)\nComp.: G.723, G.726, GSM,\nCQT\nLCNN\nCross Entropy\nopus, speex, mp2, ogg,\nCQCC, LFCC\nGMM\ntta, wma, acc, ra\nLFCC\nGMM, LCNN\n[129]\n2021\nASVspoof 2021 (LA Task)\nComp.: G.723, G.726, GSM\nPSCC, LFCC,\nResnet18, TDNN\nOC-Softmax\nopus, speex, mp2, ogg,\nDCT-DFT, LLFB\ntta, wma, acc, ra\n[130]\n2021\nASVspoof 2021 (LA&DF Tasks)\nDis.: Time-wise,\nCQT\nResNet, CNN, LSTM\nCross Entropy\nSilence Strimming\n[132]\n2021\nASVspoof 2021 (LA&DF Tasks)\nDis.: Mixup, FIR filters\nMSTFT\nResNet, LCNN\nCentral loss\n[159]\n2021\nASVspoof 2019, 2021 (LA Task)\nn/a\nLFCCs, logLFBs,\nSqueeze CNN\nCross Entropy,\nGM-LFBs,\nA-Softmax loss\nTextrograms\nMLC loss\n[160]\n2021\nASVspoof 2021 (LA&DF Tasks)\nComp.: MP3, AAC,\nLFCCs\nECAPA-TDNN, ResNet\nOC-Softmax,\nLandlie, cellular;\nP2SGrad losses\nDis.: device impulse\n[133]\n2021\nASVspoof 2021 (LA Task)\nComp.: G.711-alaw, G.722,\nLFCCs\nLCNN\nAM-softmax\nGSM-FR, and G.729\n[161]\n2021\nASVspoof 2019 (LA Tasks)\nn/a\nLFCC\nResNet\nOC-Softmax\n[162]\n2021\nASVspoof 2019 (LA Tasks)\nn/a\nLFCC\nLSTM-SECNN\nMSE loss\n[163]\n2021\nASVspoof 2019 (LA Tasks)\nDis.: SpecAug\nlog-Mel\nResNet\nn/a\n[38]\n2022\nASVspoof 2019 (LA Task),\nn/a\nCQT, log-STFT\nLCNN, CNN-LSTM, Inception,\nCross Entropy\nIn the Wild\nMEL\nResNet, Transformer\n[164]\n2022\nADD 2022\nDis.: Add noise/music/babele,\nLFCC\nResNet\nFocal loss\nReverb, Modify Volume, SpecAug;\nComp.: MP3, OGG, AAC, OPUS\n[165]\n2023\nASVspoof 2019 (LA Task),\nn/a\nLFCC\nLCNN-LSTM\nCross Entropy,\nWaveFake, FakeAVCeleb\nAdversarial loss,\nTriplet loss\n[166]\n2023\nASVspoof 2019 (LA Task)\nComp.: FLAC\nMEL\nFinetune SSAT Transformer\nCross Entropy\n[143]\n2023\nASVspoof 2019 (LA Task)\nn/a\nSTFT+F0 sub-bands\nSENet34\nA-Softmax,\nKL loss\n[167]\n2023\nASVspoof 2019 (LA Task)\nn/a\nLFCC, CQT\nTeacher-Student\nOC-Softmax,\n(ResNet, LCNN)\nMSE loss\n[145]\n2024\nASVspoof 2019 (LA Task),\nn/a\nCQT, MEL,\nResNet\nCross Entropy\nlogSpec, LFCC\n[168]\n2024\nASVspoof 2019 (LA Task),\nDis.: SpecAugment\nFBank\nECAPA-TDNN\nAM-Softmax\nASVspoof 2021 (LA&DF Tasks)\n[169]\n2024\nASVspoof 2019 (LA Task),\nDis.: RawBoost [136]\nlog-MEL\nEncoder: CNN, ResNet,\nCross Entropy,\nASVspoof 2021 (LA&DF Tasks)\nSE-ResNet\nContrastive loss\nDecoder: GAN networks [170]\n[171]\n2024\nASVspoof 2019 (LA Task)\nDis.: Oversampling\nSTFT\nEncoder: Transformer\nCross Entropy\nDecoder: Transformer\n[172]\n2024\nASVspoof 2019 (LA Task),\nDis.: RawBoost [136]\nMEL\nFinetune Wav2Vec2.0\nCross Entropy,\nASVspoof 2021 (LA&DF Tasks),\n(XLSR-53 [139])\nContrastive loss\nFakeAVCeleb, WaveFake\n[173]\n2024\nASVspoof 2019 (LA Task)\nComp.: aac, flac, mp3, m4a\nLFCC\nEncoder: Transformer\nOC-Softmax\nASVspoof 2021 (DF Task)\nwma, ogg, wa\nDecoder: Transformer\nDis.: Speed perturbation, SpecAug\nthree groups based on input type, as detailed in Tables V, VI,\nand VII. The first group, shown in Table V, consists of DSD\nsystems that directly process audio utterances using single\nmodels. These models are based on a single machine learning\nalgorithm or one specific network architecture. In the second\ngroup (Table VI), audio utterances are first transformed\ninto spectrograms, representing temporal-frequency features.\nAfter this transformation, a single model is applied to analyze\nthe data. The final group, shown in Table VII, features a\ndiverse range of ensemble models that utilize various input\nfeatures and combine multiple models. Given the summary\nof DSD systems in Table V, VI, VII, we describe the high-\nlevel architecture of DSD systems as shown in Fig. 4. From\nFig 4, we then identify and analyze four main components\nthat directly impact the DSD system performance: (1) Offline\ndata augmentation, (2) feature extraction, (3) classification\nmodel, and (4) loss function and training strategy.\nA. Offline data augmentation\nAnalysis: Data augmentation involves generating vari-\nations of the original data to increase the size of DSD\ndatasets, which enhances the robustness and generalization\ncapabilities of machine learning models. Since this step\nis applied to original audio utterances before the training\nprocess, it can be referred to as offline data augmentation. As\nshown in Tables V, VI, and VII, offline data augmentation\nmethods can be separated into two main groups, referred\nto as compression and distortion. The compression methods\ninvolve compress and decompress algorithms, mainly using\naudio codec techniques. A codec, short for \u2018coder-decoder\u2019,\nis software used to compress and decompress digital audio.\nTABLE VII\nDSD SYSTEMS LEVERAGING ENSEMBLE TECHNIQUES TO ENHANCE THE PERFORMANCE\nSystems\nYears\nDatasets\nFeatures\nData Augmentation\nModels\nLoss Functions\nEnsemble Methods\n(Distoration/Compression)\n[174]\n2019\nASVspoof 2019 (LA Task),\nLFCC, CQT, FFT\nn/a\nLCNN\nA-Softmax\nMultiple inputs\n[175]\n2021\nASVspoof 2019 (LA Task)\nRaw Audio\nDis.: Mixup\nResNet\nCross Entropy\nMultiple branches\n[176]\n2021\nASVspoof 2019 (LA Task)\nLSB, SPEC, LFCC\nn/a\nLCNN, LCNN-LSTM\nCross Entropy,\nMultiple inputs, models\nMSE for P2SGrad\n[158]\n2021\nASVspoof 2021 (LA&DF Tasks)\nLFCC\nComp.: MP3, ACC, landlie,\nVariants of ECAPA-TDNN\nOC-Softmax\nMultiple models\ncellular, VoiP\n[177]\n2021\nASVspoof 2021 (LA&DF Tasks)\nLFCC\nDis.: Reverberation, add noise,\nResNet, MLP, SWA[18]\nlarge margin cosine,\nMultiple models\nComp.: mp3, mp4\nCross Entropy\n[126]\n2021\nASVspoof 2021 (LA Task)\nLFCC, MFCC, draw\nComp.: MP3, ACC, OGG\nTDNN, RawNet2\nFocal loss\nMultiple inputs, models\n[127]\n2021\nASVspoof 2021 (LA&DF Tasks)\nDraw, CQCC, LFCC\nComp.: G.723, G.726,\nGMM, LCNN\nCross Entropy\nMultiple inputs, models\nGSM, opus, speex, mp2, ogg,\ntta, wma, acc, ra\n[129]\n2021\nASVspoof 2021 (LA Task)\nRaw, PSCC, LFCC,\nComp.: TODO set 1+2\nResNet18, GMM,\nOC-Softmax\nMultiple inputs, models\nDCT-DFT, LLFB\nTDNN, RawNet2\n[132]\n2021\nASVspoof 2021 (LA Task)\nMSTFT\nDis.: Mixup, FIR filters\nResnet18, LCNN, Sinc+CNN\nCentral loss\nMultiple inputs, models\n[161]\n2021\nASVspoof 2019 (LA Tasks)\nLFCC\nn/a\nResNet\nOC-Softmax\nMultiple branches\n[178]\n2022\nASVspoof 2021 (LA&DF Tasks)\nLFCC\nComp.: G.711-alaw, G.711-\u00b5law\nGMM-MobileNet\nCross Entropy\nMultiple branches\n[179]\n2022\nASVspoof 2021 (LA Task)\nCQT, MEL\nDis.: Mixup, Frequency Masking\nBC-ResNet, FreqCNN\nn/a\nMultiple inputs, models\n[180]\n2022\nASVspoof 2019 (LA Tasks)\nLFCC\nn/a\nResNet, LSTM\nOC-Softmax loss\nMultiple branches\n[181]\n2022\nASVspoof 2019, 2021 (LA Task)\nLog-Mel\nDis.: Add music, noise, speech\nResNet\nA-Softmax\nMultiple models\nReverb, pitch shift, SpecAug\n[182]\n2023\nASVspoof 2019, 2021 (LA Task)\nRaw Audio\nDis.: Mixup, SpecAug\nResNet\nCross Entropy\nMultiple branches\n[183]\n2023\nADD 2023\nRaw Audio, Log-Mel\nDis.: Add noise, room inpulse,\nResNet\nCross Entropy,\nMultiple branches\nmixup, speed shifting,\nKL loss\nfrequency masking\n[138]\n2023\nASVspoof 2019 (LA Task)\nWav2vec, Duration,\nn/a\nLCNN-LSTM-GAP\nCross Entropy\nMultiple inputs\nPronunciation\nCross Entropy\n[171]\n2024\nASVspoof 2019 (LA Task)\nSTFT phase, magnitude\nDis.: Oversampling\nTransformer\nEntropy\nMultiple inputs\n[184]\n2024\nASVspoof 2019 (LA Task),\nLFCC, MPE\nn/a\nLCNN\nCross Entropy\nMultiple inputs\nIn The Wild\n[185]\n2024\nASVspoof 2019 (LA Tasks)\nRaw Audio\nDis.: Noise, Reverb, SpecAug,\nEncoders:\nCross Entropy,\nMultiple models\nASVspoof 2021 (LA Task)\nDrop Frequencies\nWav2vec-XLSR-ASR,\nMSE for P2SGrad\nIn-the-wild, MLAAD-EN\nWav2vec-XLSR-SER\n[145]\n2024\nASVspoof 2019 (LA Task),\nRaw Audio\nn/a\nEncoders: XLS-R,\nCross Entropy\nMultiple inputs, models\nASVspoof 2021 (LA&DF Tasks)\nHubert, WavLM\nDecoder: ResNet\nAmong these methods, MP3, AAC, OGG, G.7XX, and Opus\nformats are commonly applied. Codec data augmentation\nhelps simulate these real-world conditions through various\ncompression schemes (e.g., phone calls, music streaming,\nor online video playback on applications such as Facebook,\nWhatsApp, etc.). Since different codecs use various com-\npression and decompression algorithms, they impact audio-\nrelated factors such as signal-to-noise ratio (SNR), high-\nfrequency formants, energy loss, sample rate, bit depth, and\nbitrate in distinct ways. This suggests that if there are subtle\ndifferences between real and fake speech in these aspects,\ngenerating diverse audio utterances using different codecs\ncan be an effective approach for distinguishing between\nthem.\nCodec methods can be divided into three main categories\nbased on the quality of audio data: uncompressed format,\nlossless compressed format, and lossy compressed format.\nAudio files with uncompressed formats such as WAV, AIFF,\nor PCM are large and contain all audio information recorded\nfrom an audio device. The lossless compressed formats such\nas FLAC, WMA, or ALAC only reduce unnecessary features\nof audio data and retain the almost original audio data.\nMeanwhile, lossy compressed formats such as MP3 or AAC\nsignificantly reduce audio features such as sample rate or bit\ndepth to achieve low-volume audio files, which is suitable for\nstreaming-based applications with real-time requirements.\nThe second distortion method tends to modify the raw au-\ndio by adding reverberation, background, and music in [177],\n[185], [181] or using techniques of time-wise, silence stream-\ning in [130] without affecting the audio quality such as sam-\nple rate, bit depth, or bit rate. The distortion method enforces\nclassification models to learn distinct features between fake\nand real speech while these features are mixed by different\nnoise resources. Notably, conventional data augmentation\nmethods, such as pitch shifting and time stretching, which are\ncommonly applied to raw audio in tasks like Acoustic Scene\nClassification [186], Speech Emotion Detection [187], and\nSpeech Separation [188], have not been applied popularly to\nthe DSD task [183], [181].\nDiscussion: Although compression methods and distortion\nmethods present different approaches to generate more au-\ndio data, none of the papers has compared, analyzed, and\nindicated if one of the approaches is superior in the DSD\ntask. Indeed, the statistical information in Fig 6 indicates\nthat the state-of-the-art DSD systems using offline distor-\ntion augmentation and offline compression augmentation are\nequal.\nRegarding codec-based data augmentation, little research\nhas examined the differences among codec methods to iden-\ntify which are most suitable for the DSD task in certain real-\nlife scenarios. Indeed, social networks such as Facebook,\nInstagram, or YouTube and Internet-based communication\ntools such as WhatsApp, and WeChat (VoIP call) utilize\nspecific and relevant codec methods. For example, YouTube\nshares audio with MP3 formats, while VoIP calls normally\nuse G.722 audio format as the standard. However, many\nproposed DSD systems have been evaluated on current and\nbenchmark datasets with WAV files, which do not accurately\nreflect the codec-specific conditions of real-life DSD appli-\ncations.\nIn speech-relevant tasks such as speaker recognition,\nspeaker emotion detection, etc., some distortion data aug-\nCompression \n (using codecs)\n39.4%\nDistortion \n (on raw audio)\n39.4%\nDistortion \n (on spectrogram)\n21.2%\nFig. 6.\nThe statistics of data augmentation methods obtained from\nTable V, VI, VII\nmentations of Mixup [189] or SpecAugment [190], which are\ninspired from the computer vision domain, are widely used.\nThese data augmentation methods focus on synthesizing new\nspectrograms in various manners (e.g., merging, masking),\nwhich might not accurately reflect artifacts of the audio\nsignal. Additionally, these data augmentation methods are\napplied to batches of spectrograms, referred to as online\ndata augmentation. As shown in Fig. 6, Mixup [189] or\nSpecAugment [190] are also used in a wide range of DSD\nsystems. However, none of the papers has analyzed or\ncompared the efficiency between offline data augmentation\nand online data augmentation.\nOur contribution: Given the analysis and the existing\nconcerns above, our work mainly contributes:\n\u2022 To evaluate the role and the effect of the online and\noffline data augmentation methods, we conducted ex-\ntensive experiments in this paper. Based on our find-\nings, we identify data augmentation techniques that\nare compatible with DSD systems. In particular, we\ncompare the performance of codec-based methods with\nthe Mixup [189] and SpecAugment [190] in this paper.\n\u2022 On our GitHub repository, we regularly update codec-\nbased methods and other data augmentation techniques\nfeatured in the latest research. We also released code\nfor implementing codec-based methods and other data\naugmentation methods in this GitHub repository, which\nare used to conduct our experiments in this paper.\nB. Feature extraction\nAnalysis: As shown in Fig. 4, feature extraction methods\ncan be categorized into two main groups: non-parameter and\ntrainable-parameter methods.\nIn non-parameter feature extraction, a raw audio ut-\nterance (e.g., a 1-D tensor) is first transformed into a time-\nfrequency spectral features (e.g., a 2-D tensor) using var-\nious transformation ranging from spectral coefficients (e.g.,\nMFCC [191], [126], LFCC [129], [180], [161], CQCC [127],\netc) to spectrogram-based representations such as STFT-\nspectrogram [171], [132], CQT-spectrogram [127], [130],\netc. Once the time-frequency spectrograms are generated,\nsome DSD systems directly use them for training with\nclassification models [130], while other systems use several\napproaches to enhance feature quality before applying a\nclassification model. The first approach involves applying\nauditory filter banks such as Mel [158], [145], Linear Fil-\nter [126], [160], [133] (LF), etc, to capture the relation-\nships between frequency bands. Then a Discrete Fourier\nTransform (DFT) is applied to analyze the relationship\nacross temporal dimension before the features are fed into\na model for the training process [126], [158], [160], [133].\nNotably, the output of Mel, LF, or DFT operations remains\na 2-D tensor (similar to a spectrogram), representing both\ntemporal and spectral features. In the second approach,\nspectrograms are fed into pre-trained models, such as XLS-\nR [192], Hubert [140], WavLM [149], or Whisper [150], to\nextract embeddings. These embeddings are the output feature\nmaps from a specific layer of the pre-trained model [145].\nTypically, the embeddings form a 1-D tensor, similar to\na vector, where each dimension of the vector is treated\nas an independent value. In this approach, the choice of\nspectrogram depends on the one used to train the pre-trained\nmodels. Typically, the Mel-spectrogram is preferred, as most\npre-trained models use it as input for training upstream\ntasks [149], [140], [150]. In general, non-parameter feature\nextraction leverages various spectrogram transformations,\nauditory filters, auditory statistics, and pre-trained models to\ngenerate distinct features (e.g., 1-D audio embeddings, 2-D\nspectrograms) of audio input.\nTrainable-parameter feature extraction involves ex-\ntracting audio features by applying trainable network layers.\nIn particular, systems proposed in [145], [128], [131] applied\nSincNet layers [193], LEAF layers [194], FBanks [168] to\nlearn and extract features from raw audio. These techniques\nconstruct learnable filterbanks or approximate the standard\nfiltering process. For example, SincNet and LEAF layers\nkeep the role of adaptive and bandpass filters to capture fre-\nquency features between two pre-defined cut-off frequencies.\nThe outputs of these trainable layers are the feature maps\nthat are then fed into the next parts of detection systems. In\nother words, trainable feature extraction includes trainable\nnetwork layers as a part of entire network architectures that\ndirectly train and learn features from raw audio without the\nspectrogram transformation steps.\nDiscussion: By allowing learnable temporal-spatial fea-\ntures during the training process, trainable-parameter feature\nextraction is compatible with end-to-end systems and shows\neffectiveness in distinguishing artifacts in fake speech. How-\never, as most proposed systems using trainable features were\nevaluated on single datasets rather than cross-dataset settings,\nthis possibly leads to challenges in generalization since\nlearned feature sets perform well under specific conditions\nbut fail in unseen fake speech in real-world environments.\nRegarding feature extraction using audio embeddings from\npre-trained models, although these pre-trained models are\neffective for many audio tasks, using them for deepfake\ndetection presents several challenges. Firstly, as pre-trained\nmodels are initially trained for upstream tasks such as\nspeech-to-text, speaker identification, emotion detection, etc,\nthat focus on different aspects (i.e., speech-to-text or emotion\nCross Entropy\n41.3%\nSoftmax loss\n20.6%\nFocal loss\n7.9%\nMSE loss\n7.9%\nTriplet loss\n6.3%\nAdversarial loss\n4.8%\nContrastive loss\n4.8%\nOthers\n6.3%\nFig. 7.\nThe statistics of loss functions obtained from Table V, VI, VII\ndetection), the audio melody and harmony (i.e. emotion de-\ntection), or distinct frequencies (i.e., speaker identification),\nembeddings can fail to capture subtle artifacts specific to\nsynthesized speech. Secondly, audio deepfakes are generated\nto closely mimic real speech, they often have the same\nformants, pitch, and rhythm as real audio, especially when\ngenerated by advanced deep-learning-based speech genera-\ntion systems. Additionally, the use of pre-trained models can\nadd complexity due to their large network architectures.\nFor systems using spectrograms such as CQT, MEL,\nGAM, etc., each spectrogram is designed to capture specific\nfrequency ranges. These spectrograms focus on different\ncentral frequencies, which allows them to highlight distinct\nfeatures of an audio signal. However, human speech contains\na wide range of formants - characteristics of sound deter-\nmined by factors such as language, accent, vocal tract shape,\nand vocal fold behavior. Therefore, relying on only one\ntype of spectrogram may miss important features, leading\nto incomplete or insufficient representations of the speech\nsignal that are useful for deepfake detection. To address\nthis, DSD systems have begun to use ensembles of mul-\ntiple spectrogram inputs [126], [158], [127], [129], [132].\nBy leveraging the unique strengths of each spectrogram\ntype, this approach aims to enhance detection accuracy\nand has shown significant improvements in model perfor-\nmance. Many top-performing systems in recent competitions\nhave demonstrated the effectiveness of using ensembles to\nboost overall system robustness. However, ensemble models\npresent several limitations, including reduced interpretability,\nincreased system complexity, and higher training costs.\nOur contribution: Given the analysis and the discussion\nabove, our work mainly contributes:\n\u2022 We presented the commonly used feature extraction\nmethods in DSD systems, highlighting their charac-\nteristics and potential challenges associated with each\napproach.\n\u2022 In the next section, we conduct extensive experiments\nof various feature extraction methods to evaluate the\nMultiple inputs\n19.0%\nMultiple \n branches \n or models\n47.6%\nMultiple inputs \n and models\n33.3%\nFig. 8.\nThe statistics of ensemble methods obtained from Table V, VI, VII\nmost effective approach for the DSD task. Additionally,\nwe explore different feature ensembles to determine the\noptimal combinations for enhancing performance.\n\u2022 In our GitHub, we release code for different spectro-\ngram transformations using Librosa toolbox [195].\nC. Classification models\nAnalysis: Early models proposed for DSD task ap-\nproached conventional machine learning algorithms. For ex-\nample, 9 over 16 submitted systems in ASVspoofing 2015\nchallenge [191] extract MFCC feature (i.e. Systems A, B,\nE, G, H, I, N, O, and P in [191]). Then, various ma-\nchine learning-based models such as Mahalanobis distance\nmeasurement, Gaussian-based model (GMM), support vector\nmachine-based models (SVM, SVM-RBF), or fusion models\n(GMM and SVM) are used to explore MFCC features.\nHowever, recent DSD systems as shown in Table V, VI, VII\npresent a wide range of neural network architectures due to\nthe powerful deep learning techniques. Recently proposed\ndeep neural networks for the DSD task can be separated\ninto four main approaches. The first approach, which fo-\ncuses on exploring spatial features, leverages convolutional-\nbased network architectures (CNN). Among the CNN-based\nnetworks, Resnet, LCNN, and RawNet architectures are\nwidely approached. ResNet and LCNN are used to explore\nspectrogram-based features such as LFCC [127], CQT [132],\nand MEL [145]. Meanwhile, RawNet architectures are nor-\nmally combined with SincNet layer [193] to learn raw\naudio [126], [127], [129], [130], [133], [145]. The second\napproach, which focuses on exploring the temporal features,\npresents recurrent neural network (RNN) based architectures.\nFor example, LSTM-based networks, TDNN, or ECAPA-\nTDNN are proposed in [130], [126], [158], [129] and [168],\nrespectively. As shown in Table V, VI, VII, RNN-based\nnetworks have not been popularly applied for the DSD\ntask compared to the CNN-based architectures. The third\napproach involves combining both convolutional layers and\nrecurrent layers to explore both temporal and spatial features,\nreferred to as hybrid network architectures. In particular,\nrecurrent network-based layers such as LSTM, GRU are\ncombined with CNN-based layers to perform convolutional-\nrecurrent neural network (CRNN) architectures [126], [168],\n[165].\nRecently, encoder-decoder based network architectures\nhave been popularly used for the DSD task. Indeed, along\nwith conventional encoder and decoder in transformer-based\narchitectures [171], [173], various encoder architectures such\nas XLSR-53 [172], WavLM [151], CNN or ResNet [169]\nare explored. Decoder architectures also show diverse using\nGAN-based architecture [169], Multi-feature attention [151],\nGraph Attention Network [131], [135], [134], etc.\nTo further enhance the DSD performance, the DSD re-\nsearch community leverages a wide range of ensemble\nmodels. These ensemble models can be separated into three\nmain approaches which are marked in the final column in\nTable VII. In the first approach (Multiple inputs), multi-\nple input features are explored [174], [138], [171]. This\napproach is inspired by the idea that multiple features\ncontain different and distinct features between fake and real\nutterances. Given different features, each feature is trained\nby the same classification model (i.e., the individual model\nshares the same network architecture but presents different\ntraining parameters after the training process). For example,\nwhile [171] explores the magnitude and phase features of\nSTFT spectrogram, different features of Wav2Vec embed-\ndings, duration, and pronunciation are explored in [138].\nSimilarly, multiple spectrograms such as LFCC, CQT, and\nSTFT are trained by one classification model of CNN [196].\nFinally, the scores obtained from individual models are fused\nto achieve the final and best result. The second approach\n(Multiple branches or models) leverages different network\narchitectures that explore one type of input feature [158],\n[177], [178], [161], [175], [182], [183], [181], [185]. This\napproach is inspired by the idea that different network archi-\ntectures are likely to capture distinct properties from the input\nfeature. For example, [177] proposed multiple branches of\nGMM-DNN and ResNet to explore the LFCC spectrogram.\nSimilarly, [158] explores the raw audio by different variants\nof ECAPA-TDNN. The final approach (Multiple inputs,\nmodels) leverages both multiple input features and different\nnetwork architectures. For example, [126] explore raw audio\nby RawNet2. Meanwhile, TDNN and LFCC spectrogram are\nexplored by LCNN. Then, the authors fused three results\nobtained from three individual models. Similarly, multiple\ninput features of raw audio, CQCC, and LFCC are ex-\nplored by different models of LCNN, GMM, and RawNet2\nin [127]. Ensemble methods are widely adopted in many\ntop-performing systems in DSD challenge competitions.\nDiscussion: Although many deep neural network archi-\ntectures have been proposed for the DSD task and evaluated\non various benchmark datasets, the best results have been\nobtained from ensemble methods with multiple inputs or/and\ndifferent network architectures. The statistics of ensemble\nmodels, as shown in Fig 8, indicate that multiple branches or\nmodels are the majority. However, ensemble models present\nthe concern of large trainable parameters. Moreover, none\nof the research has been analyzed to indicate the individual\nroles of input features or types of network architectures used\nin ensemble methods. To demonstrate a robust and general\nDSD model, the proposed model needs to be evaluated\nwith multiple datasets, cross-datasets, or cross-languages.\nHowever, only some recent research [172], [38], [152], [144]\nevaluated the proposed models with multiple datasets such as\nASVspoof 2019 (LA Task), ASVspoof 2021 (LA&DF Task),\nIn The Wild, etc. To the best of our knowledge, none of the\nresearch has proposed the evaluation on cross-languages.\nOur contribution: Given the analysis and the discussion\nabove, our work mainly contributes:\n\u2022 We evaluate various input features, indicating the effec-\ntive input feature for DSD system performance.\n\u2022 We also evaluate a wide range of network architectures\nleveraging the transfer learning technique, end-to-end\ntraining approach, and audio embeddings extracted from\nstate-of-the-art pre-trained models.\n\u2022 Given extensive experiments on different input features\nand various network architectures, we propose an en-\nsemble model that is competitive to the state-of-the-art\nDSD systems\nD. Loss function and training strategy\nFrom Table V, VI, VII, it can be seen that most proposed\nmodels use a single loss function. Statistics of the individual\nloss functions are also presented in Fig. 7. As shown in\nFig. 7, the cross entropy (CE) based losses (e.g. Binary\nCross Entropy (BCE), Weight Cross Entropy (WCE), etc.)\nand Softmax-based losses (e.g., Additive-Margin-Based Soft-\nmax (AM-Softmax), Angular-Margin-Based Softmax (A-\nSoftmax), etc.) present the most popular loss functions. Some\nmodels combine different loss functions. For example, CE\nand Contrastive loss were used in [169]. Similarly, authors\nin [165] combined three loss functions of Cross Entropy,\nTriplet loss, and Adversarial loss. Some papers such as [159]\nand [160] compared the DSD performance between large\nmargin cosine loss (LMC loss), and A-Softmax loss functions\nor between OC-Softmax, MSE for P2SGrad loss functions,\nrespectively.\nGenerally, a single loss function is used in end-to-end\nbased systems. Meanwhile, the combination of multiple loss\nfunctions is related to different training strategies. For exam-\nple, [172] proposed a teacher-student scheme in which the\nteacher was trained with contrastive loss and the student was\ntrained by a combination of contrastive loss, Cross Entropy,\nand MSE loss. Similarly, the student network in [197],\n[167] was trained by a combination of Cosine Similarity/OC-\nSoftmax and MSE loss functions. It can be seen that muliple-\nloss functions used for teacher-student schemes help achieve\na low-complexity model for the DSD task [172], [197], [167].\nAdditionally, using multiple-loss function in [142] aims for\nmultiple-task learning strategy. Rather than focusing on loss\nfunctions, some researchers improve the DSD system by\nexploring the training strategy. For example, authors in [198]\nsuggested to mix three datasets for the training process.\nThis enhances the generalization and stabilization of the\nauthors\u2019 proposed DSD system. Meanwhile, authors in [199]\ngenerated more fake utterances by leveraging four types of\nVocoders: HiFiGAN, MB-MelGAN, PWG, and WaveGlow,\nwhich helps to improve their DSD system performance.\nV. OUR PROPOSED DEEPFAKE SPEECH DETECTION\nSYSTEM AND EXTENSIVE EVALUATION\nA. Our motivation\nGiven the comprehensive analysis of the DSD systems in\nSection IV, we are motivated to conduct extensive experi-\nments that address and evaluate the main concerns below.\n\u2022 We evaluate the role of offline data augmentation\n(codec) and compare this method with the conventional\nonline data augmentation methods of Mixup [189] and\nSpecAugment [190]. We also indicate if a combination\nof offline and online data augmentation methods is\neffective in enhancing the DSD system performance.\n\u2022 We conduct extensive experiments to evaluate different\ninputs and various network architectures. Given the\ncomparison, we indicate which input features, network\narchitectures, combination of input features, and net-\nwork architectures have the potential to be further ex-\nplored. We then propose the best DSD ensemble system\nwhich is competitive to the state-of-the-art systems.\n\u2022 To address the real-time ability, our proposed models\nare evaluated on two-second utterances and present low-\ncomplexity architectures.\nB. Selected datasets and evaluating metrics\nAs the trade-off among the number of utterances, the\ndeep-learning-based fake speech generation systems, the\noriginal/real human speech resource shown in Fig. 2 and\nthe comprehensive analysis in Section III, we decide to use\nASVspoof 2019 (LA Task) to evaluate the effect of data\naugmentations, different types of input features, and various\nnetwork architectures.\nWe obey the ASVspoof 2019 (LA Task) challenge, then\nuse the Equal Error Rate (ERR) as the main metric for\nevaluating proposed models. We also report the Accuracy, F1\nscore, and AUC score to compare the performance among\nevaluating models.\nC. Proposed systems and experimental settings\nData augmentations: We evaluate the role of two data\naugmentation methods: offline data augmentation (codecs)\nand online data augmentation (Mixup and SpecAugment).\nRegarding offline data augmentation using codec-based\nmethods, we use six popular codec formats MP3, OPUS,\nOGG, GSM, G722, and M4A. While the codec-based meth-\nods compress and decompress raw audio before the training\nprocess, the online data augmentation methods of Mixup\nand SpecAugment work on batches of spectrograms during\nthe training process. By evaluating these two groups of\ndata augmentation individually, we indicate if each of them\npresents a significant contribution and a combination of two\ndata augmentation methods can help to enhance DSD task\nperformance.\n2-second segment\nMel filter\nLinear filter\nGamma filter\nWavelet\nSTFT\nCQT\nFig. 9.\nGenerate spectrograms using different spectrogram transformation\nmethods and auditory filter models\nTABLE VIII\nTHE CNN, RNN, AND C-RNN NETWORK ARCHITECTURES\nModels\nConfiguration\nCNN-based model\n3 \u00d7 {Conv(32/64/128)-ReLU-AP-Dropout(0.2)}\n1 \u00d7 {Dense(256)-ReLU-Dropout(0.2)}\n1 \u00d7 {Dense(2)-Softmax}\nRNN-based model\n2 \u00d7 {BiLSTM(128/64)-ReLU-Dropout(0.2)}\n1 \u00d7 {Dense(256)-ReLU-Dropout(0.2)}\n1 \u00d7 {Dense(2)-Softmax}\nC-RNN-based model\n3 \u00d7 {Conv(32/64/128)-ReLU-AP-Dropout(0.2)}\n2 \u00d7 {BiLSTM(128/64)-ReLU-Dropout(0.2)}\n1 \u00d7 {Dense(256)-ReLU-Dropout(0.2)}\n1 \u00d7 {Dense(2)-Softmax}\nMultiple input features: Fig. 9 presents seven types of\ninput features: raw audio and six different spectrograms,\nwhich are evaluated in this paper. In particular, we use\nthree transformation methods of Short-time Fourier Trans-\nform (STFT), Constant-Q Transform (CQT), and Wavelet\nTransform. Presumably, each type of spectrogram focuses on\ndifferent perspectives on frequency content and might catch\ndifferent inconsistencies in the audio signal. We then leverage\ndifferent auditory-based filters: Mel and Gammatone filters\nfocus on subtle variations relevant to human auditory per-\nception and the linear filter (LF) isolates specific frequency\nbands.\nAs we set the window length, the hop length, and the filter\nnumber with 1024, 512, and 64, we achieve the same spectro-\ngram shape of 64\u00d764. Then, we apply Discrete Cosine Trans-\nform (DCT) to spectrograms across the temporal dimension.\nFinally, the first and the second-order derivatives are applied\nto these spectrograms, generating a three-dimensional tensor\nof 64\u00d764\u00d73 (i.e., the original spectrogram, the first-order\nderivative, and the second-order derivative are concatenated\nacross the third dimension).\nBack-end classification models: This paper proposes\nthree main approaches for back-end classification models:\nthe end-to-end deep learning approach, the transfer learning\napproach, and the audio-embedding deep learning approach.\nRegarding the end-to-end deep learning approach, four mod-\nels of CNN-based model, SinC-CNN model (CNN architec-\nture in SinC-CNN model reused from CNN-based model),\nRNN-based model, and C-RNN-based model are evaluated\nwith the detailed configuration in Table VIII. The Sinc-CNN\nmodel proves powerful for raw audio input and has been\nwidely used as the survey in Section IV Meanwhile, CNN-\nbased models are commonly used and effectively capture\nand learn spectral features. We also use RNNs to focus on\ndetecting natural sequential patterns that can be disrupted in\nsynthetic audio [200] (e.g., temporal coherence, prosodic fea-\ntures such as rhythm, stress, and intonation). Consequently,\nbased on the idea of combining both spectral features and\ntemporal features, we use C-RNN-based model to distinguish\ncharacteristics of real and fake audio utterances.\nWith the transfer learning approach, a wide range of\nbenchmark network architectures in the computer vision\ndomain are evaluated. These networks are ResNet-18,\nMobileNet-V3, EfficientNet-B0, DenseNet-121, SuffleNet-\nV2, Swint, Convnext-Tiny, GoogLeNet, MNASnet, RegNet,\nwhich were trained on the ImageNet1K dataset [201] in\nadvance. Given the pre-trained networks, trainable weights,\nwhich capture rich and generalized features of pattern recog-\nnition in images,have the potential to adapt patterns in\nspectrograms by the fine-tuning process. To adapt the DSD\ntask, we modify the final dense layer of these mentioned\nnetworks to be compatible with the binary classification task.\nFor the audio-embedding deep learning approach, the\nstate-of-the-art audio pre-trained models of Whisper [150],\nSeamless [202], Speechbrain [203], and Pyanote [204], [205]\nare leveraged.\nIn particular, we feed the spectrogram inputs into these\npre-trained models to obtain audio embeddings. Given the\naudio embeddings, We then propose a Multilayer Perceptron\n(MLP) to classify the audio embeddings into fake or real\nclasses. The proposed MLP is shown in Table IX, to detect\nreal or fake audio.\nEnsemble method: As we train individual model works\nwith two-second audio segment, the result on an entire audio\nrecording is computed by averaging of results over all two-\nsecond segments. Let consider p(n) = [p(n)\n1 , p(n)\n2 , ..., p(n)\nC ],\nwhere C is the category number of the n-th out of N two-\nsecond segments, as the predicted probability of one two-\nsecond segment. The predicted probability of an entire audio\nrecording, as described by \u00afp = [\u00afp1, \u00afp2, ..., \u00afpC], is computed\nby:\n\u00afpc = 1\nN\nN\nX\nn=1\np(n)\nc\nfor 1 \u2264c \u2264C\n(1)\nGiven the predicted probabilities from individual models,\nwe propose a MEAN fusion for an ensemble of multiple\nmodels. Let consider the predicted probability of one model\nas \u02c6ps = (\u00afps1, \u00afps2, ..., \u00afpsC), where C is the category number\nand the s-th out of S individual models. Next, the predicted\nprobability after MEAN fusion (\u02c6p1, \u02c6p2, ..., \u02c6pC) is obtained\nby:\n\u02c6pc = 1\nS\nS\nX\ns=1\n\u02c6psc\nfor 1 \u2264c \u2264C\n(2)\nFinally, the predicted label \u02c6y for an entire audio sample is\ncomputed by:\nTABLE IX\nTHE AUDIO PRE-TRAINED MODELS AND THE MULTILAYER PERCEPTRON\nModels\nUsing License\nEmbedding size/\nConfiguration\nWhisper [150]\nMIT\n512\nSpeechBrain [203]\nApache2-0\n192\nSeamLess [202]\nMIT\n1024\nPyannote [204], [205]\nMIT\n512\nMLP\nOur proposal\n1 \u00d7 {Dense(128)-ReLU }\n1 \u00d7 {Dense(2)-Softmax }\n\u02c6y = argmax(\u02c6p1, \u02c6p2, ..., \u02c6pC)\n(3)\nD. Experimental results and discussion\nEvaluation of data augmentation methods: Considering\nthe performance of online and offline data augmentation\nmethods as shown in systems A1 (no data augmentation),\nA2 (online data augmentation with codec), A3 (offline data\naugmentation with Mixup and SpecAugment), and A4 (both\nonline and offline data augmentation), it can be seen that the\noffline data augmentations of Mixup and SpecAugment are\nappropriate for DSD task. Notably, the combination of online\nand offline data augmentations does not help to enhance the\nDSD task performance compared with only using online data\naugmentation.\nEvaluation of input features: Considering the efficacy of\nraw audio and six types of spectrograms in systems from B1\nto B7, STFT outperforms the raw audio and other spectro-\ngrams. Models B2, B5, and B7 achieve the best ERR score\nof 0.08 while the combination of STFT & LF obtains slightly\nbetter accuracy and F1 score of 0.88 and 0.9, respectively.\nThis indicates that STFT and applying filters such as Linear\nFilter or Gammatone filter are suitable for isolating specific\nfrequency bands in classification algorithms.\nMultiple deep learning approaches: Regarding the end-\nto-end deep learning approach from A1 to C2, CNN systems\noutperform RNN or C-RNN systems. Indeed, using the same\ninput feature of STFT+LFCC, RNN and C-RNN approaches\n(C1 and C2 systems) obtain ERR scores of 0.14 and 0.17,\nwhich is significantly worse than CNN system (A3 or B2\nor B7) with the best score of 0.08. This indicates that the\nspecific patterns indicative of deepfake audio might not be\nprimarily temporal but rather frequency in the spectrogram\nrepresentation. Regarding the finetuning approach (D1 to\nD10), Convnext-Tiny stands out as the best system with com-\npetitive EER scores of 0.075. Meanwhile, the embedding-\nbased approach (E1 to E4) achieves the best EER scores of\n0.10 from the pre-trained Whisper model. This suggests the\npotential of these approaches when choosing the appropriate\nnetworks for enhancement.\nEnsembles: Given the performance of individual input\nfeatures and network architecture, we conduct extensive\nexperiments to evaluate a wide range of ensemble models.\nFirst, ensembles of STFT, CQT, and WT spectrograms are\nevaluated, indicating the best EER score of 0.06 from the\ncombination of STFT and CQT (B2+B3). Then, ensembles\nof spectrogram with different filter banks (MEL, LF, GAM)\nare also evaluated, resulting in the best score of 0.065\nTABLE X\nPERFORMANCE COMPARISON AMONG DEEP LEARNING MODELS AND ENSEMBLE OF HIGH-PERFORMANCE MODELS\nON LOGIC ACCESS EVALUATION SUBSET IN ASVSPOOFING 2019\nSystems\nInputs\nAugmentations\nModels\nAcc\nF1\nAUC\nERR\nA1\nSTFT & LF\nNone\nCNN\n0.82\n0.84\n0.91\n0.15\nA2\nSTFT & LF\nCodec\nCNN\n0.81\n0.84\n0.93\n0.13\nA3\nSTFT & LF\nMixup, Spec.\nCNN\n0.88\n0.90\n0.96\n0.08\nA4\nSTFT & LF\nCodec, Mixup, Spec.\nCNN\n0.81\n0.84\n0.93\n0.13\nB1\nRaw Audio\nNone\nSinC-CNN\n0.84\n0.87\n0.96\n0.10\nB2\nSTFT\nMixup, Spec.\nCNN\n0.87\n0.89\n0.96\n0.08\nB3\nCQT\nMixup, Spec.\nCNN\n0.89\n0.90\n0.92\n0.14\nB4\nWT\nMixup, Spec.\nCNN\n0.84\n0.86\n0.89\n0.17\nB5\nSTFT & LF\nMixup, Spec.\nCNN\n0.88\n0.90\n0.96\n0.08\nB6\nSTFT & MEL\nMixup, Spec.\nCNN\n0.86\n0.88\n0.95\n0.11\nB7\nSTFT & GAM\nMixup, Spec.\nCNN\n0.85\n0.87\n0.96\n0.08\nC1\nSTFT & LF\nMixup, Spec.\nRNN\n0.92\n0.91\n0.88\n0.17\nC2\nSTFT & LF\nMixup, Spec.\nCRNN\n0.88\n0.90\n0.96\n0.14\nD1\nSTFT & LF\nMixup, Spec.\nResNet-18\n0.49\n0.58\n0.51\n0.47\nD2\nSTFT & LF\nMixup, Spec.\nMobileNet-V3\n0.59\n0.67\n0.52\n0.48\nD3\nSTFT & LF\nMixup, Spec.\nEfficientNet-B0\n0.52\n0.61\n0.51\n0.48\nD4\nSTFT & LF\nMixup, Spec.\nDenseNet-121\n0.58\n0.66\n0.51\n0.48\nD5\nSTFT & LF\nMixup, Spec.\nShuffleNet-V2\n0.64\n0.71\n0.53\n0.48\nD6\nSTFT & LF\nMixup, Spec.\nSwin T\n0.84\n0.87\n0.94\n0.09\nD7\nSTFT & LF\nMixup, Spec.\nConvNeXt-Tiny\n0.88\n0.90\n0.96\n0.075\nD8\nSTFT & LF\nMixup, Spec.\nGoogLeNet\n0.53\n0.62\n0.51\n0.47\nD9\nSTFT & LF\nMixup, Spec.\nMNASNet\n0.62\n0.70\n0.54\n0.47\nD10\nSTFT & LF\nMixup, Spec.\nRegNet\n0.50\n0.60\n0.50\n0.48\nE1\nRaw Audio\nNone\nWhisper+MLP\n0.85\n0.88\n0.95\n0.10\nE2\nRaw Audio\nNone\nSpeechbrain+MLP\n0.77\n0.81\n0.81\n0.25\nE3\nRaw Audio\nNone\nSeamless+MLP\n0.86\n0.88\n0.87\n0.20\nE4\nRaw Audio\nNone\nPyannote+MLP\n0.64\n0.71\n0.78\n0.27\nB2 + B3\nSTFT, CQT\nMixup, Spec.\nCNN\n0.91\n0.92\n0.98\n0.06\nB2 + B4\nSTFT, WT\nMixup, Spec.\nCNN\n0.88\n0.90\n0.96\n0.09\nB2 + B3 + B4\nSTFT, CQT, WT\nMixup, Spec.\nCNN\n0.90\n0.92\n0.98\n0.07\nB5 + B6\nSTFT&LF, STFT&MEL\nMixup, Spec.\nCNN\n0.88\n0.90\n0.97\n0.08\nB5 + B7\nSTFT&LF, STFT&GAM\nMixup, Spec.\nCNN\n0.87\n0.89\n0.98\n0.065\nB5 + B6 + B7\nSTFT& LF, STFT&MEL, STFT&GAM\nMixup, Spec.\nCNN\n0.88\n0.90\n0.98\n0.069\nB5 + D6\nSTFT&LF\nMixup, Spec.\nCNN, Swint T\n0.87\n0.89\n0.96\n0.078\nB5 + D7\nSTFT&LF\nMixup, Spec.\nCNN, ConvNeXt-Tiny\n0.88\n0.90\n0.97\n0.07\nB5 + D6 + D7\nSTFT&LF\nMixup, Spec.\nCNN, ConvNeXt-Tiny, Swint T\n0.88\n0.89\n0.97\n0.072\nB3 + B5 + B7\nCQT, STFT&LF, STFT&GAM\nMixup, Spec.\nCNN\n0.88\n0.90\n0.98\n0.05\nD7 + E1\nRaw Audio, STFT&LF\nMixup, Spec.\nWhisper, ConvNeXt-Tiny\n0.86\n0.88\n0.99\n0.03\nfrom STFT+LF and STFT+GAM (B5+B7). As a result,\nwhen an ensemble of CQT, STFT+LF, and STFT+GAM is\nconducted (B3+B5+B7), we can achieve the EER score of\n0.05. Regarding the ensemble of network architecture, CNN\nand ConvNeXt-Tiny (B5+D7) help obtain the EER score\nof 0.07. Meanwhile, the combination of Whisper+MLP and\nConvNeXt-Tiny (E1+D7) achieves the best EER score of\n0.03. These results obtained from ensemble models lead to\nsome conclusions:\n\u2022 Ensembling different input features or network archi-\ntectures can significantly help to enhance the DSD task\nperformance.\n\u2022 Not all network architectures are appropriate for the\nDSD task. As the good performance obtained from\nConvNeXt-Tiny, the depthwise convolution layer in this\nnetwork architecture needs to be further investigated.\n\u2022 Leveraging pre-trained models such as Whisper shows\neffectiveness.\nThis\nagain\nproves\nwhy\nthe\nrecent\nEncoder-Decoder\nnetwork\narchitectures\nusing\npre-\ntrained models for the Encoder have been proposed\npopularly.\nVI. OPEN CHALLENGES AND POTENTIAL RESEARCH\nDIRECTIONS\nA. Datasets for Deepfake Speech Detection\n1) Open challenges: Building better datasets for audio\ndeepfake detection is essential for improving the accuracy\nand robustness of detection systems. However, the current\ndiversity of available datasets for audio deepfake detection\nremains limited, especially in terms of speaker identity,\nlanguage, and deepfake generation methods.\nA large number of published datasets feature a narrow\nrange of speaker identities, often focusing on a small group\nof speakers with limited gender, age, and accent diversity.\nFor instance, datasets of ASVspoof and FakeAVCeleb in-\nclude mainly English-speaking voices from certain groups\nof speakers (e.g., celebrity, predominantly synthesized voice)\nwith a small number of speakers from different language\nbackgrounds, resulting in biased models when applied to\ndiverse populations.\nMany existing datasets are domain-specific, focusing\non particular types of audio or speakers. For example,\nFakeAVCeleb primarily includes celebrity interviews, while\nLibriSpeech focuses on read recordings. These datasets often\nhave limited variability in terms of recording conditions,\nspeaker interactions, and speech styles, making it difficult\nto generalize detection models to new domains or unseen\nenvironments, such as detecting deepfakes in real-world\nscenarios with noisy or degraded audio, such as phone calls,\npublic spaces, or online content.\nThe lack of language diversity is also a significant issue\nthat limits the robustness of detection models. As shown at\nTable III, most existing datasets support single languages\n(primarily English or Chinese). This imbalance raises chal-\nlenges that hinder the development of robust, audio deepfake\ndetection systems in multilingual settings.\nAs deepfake generation techniques have been evolving\nrapidly, they produce fake audio that is increasingly difficult\nto detect. This makes it difficult for existing datasets to stay\nup to date as they may be vulnerable to newer methods of\naudio synthesis. Therefore, datasets must be continuously\nupdated to include samples produced by new techniques to\nensure the robustness and adaptability of detection models.\n2) Future directions: Given the open challenges discussed\nin the previous subsection, we highlight some potential\nfuture directions in dataset development for Deepfake Speech\nDetection:\nMultilingual and Multimodal Datasets: To address the\nissue of language diversity, future datasets should include\na broader range of languages, accents, and dialects. This\nvariety will enable detection models to better handle diverse\nlinguistic and phonetic features across different languages,\nensuring their stability in multilingual contexts and their\neffectiveness in developing global solutions.\nMoreover, deepfake content in real-world scenarios often\nincludes both audio and video elements, rather than just au-\ndio. Therefore, integrating multimodal datasets that combine\nboth audio and video deepfakes is a crucial direction for\nfuture research. This integration enhances detection capabil-\nities by allowing models to identify anomalies across mul-\ntiple data types, improving their effectiveness in combating\nincreasingly sophisticated forgeries\nContinuous Dataset Updates: To stay updated, there\nneeds to be ongoing collaboration between researchers de-\nveloping deepfake generation methods and those working on\nthe DSD task. Regular updates to datasets should include\ndeepfake samples created by the latest synthesized generation\ntechniques, allowing detection models to adapt to emerging\nthreats.\nCross-Domain and Real-World Dataset Adaptation:\nOne of the biggest challenges for DSD models is domain\nadaptation \u2014 the ability to generalize across different types\nof audio environments, speakers, and use cases. Future\ndatasets should prioritize cross-domain generalization, in-\ncluding diverse data from various contexts (e.g., podcasts,\nphone calls, interviews, public speeches, and social media\ncontent). In addition, besides varied deepfake generation\nmethods, future dataset development should include data\nfrom diverse online platforms (e.g., YouTube, TikTok, pod-\ncasts) and various speaker demographics that stimulate in-\nclusive real-life scenarios.\nB. The generalization and robustness of Deepfake Speech\nDetection models\n1) Open challenges: A major challenge in developing\ndeepfake detection systems is ensuring they can generalize\nto new samples that are not presented in the training data.\nWhile models may perform well on known attacks, they\noften struggle with novel manipulations and across different\ndomains, such as varying languages, accents, or speaking\nstyles. The limited size and diversity of training datasets\nhinder DSD models\u2019 ability to handle real-world variability\nwithout degraded performance. Some approaches have been\nadopted to address these challenges. For example, ensemble\nmodels, as discussed in Sections II and IV, have been\neffectively utilized to enhance DSD performance and gener-\nalization ability, often achieving top results in competition\nsettings. They are also frequently employed in research\npapers to deliver competitive outcomes [129], [145], [127].\nWhile ensemble models are powerful and versatile, they\noften require significant computational costs during train-\ning. Additionally, detection systems leveraging pre-trained\nmodels have gained popularity [172]. By fine-tuning models\npre-trained on upstream audio tasks like speech-to-text [139],\n[150], the training cost for DSD downstream tasks is greatly\nreduced. However, proving the generalization of these fine-\ntuned single models remains challenging. For instance, ex-\nperiments on ASVspoof 2021 (DF Task) in [172] achieved\nremarkable results, with an EER of 5.67 compared to 15.64\nfrom the top-performing system in the challenge. In contrast,\nthe performance on the ASVspoof 2021 (LA Task) was much\nlower, with an EER of 15.92, compared to 1.32 from the top-\nperforming system.\nIn terms of improving the model\u2019s robustness to adver-\nsarial attacks, the majority of current methods for defending\nagainst adversarial attacks rely on adversarial training [9],\nwhich involves generating adversarial examples from known\nattacks to retrain the model. However, this approach incurs\nhigh computational costs.\n2) Future directions: To improve the generalization and\nrobustness of detection systems, there has been much room\nfor improving existing approaches as well as proposing\nnew methods. For example, future directions can address\nchallenges in ensemble methods by balancing the trade-off\nbetween cost and effectiveness using techniques such as\npruning, quantization, and knowledge distillation or other\nefficient ensembling strategies to reduce model size. In the\napproach using transfer learning or fine-tuning, employing\nseveral strategies such as cross-dataset validation or an\nensemble of fine-tuned models could address the challenges\nof proving generalization. Applying mechanisms to learn in-\nformation from domain-invariant attacks could also enhance\nthe robustness of models against different adversarial attacks.\nC. Interpretability and Explainable AI (XAI) for Deepfake\nSpeech Detection\n1) Open challenges: Improving interpretability and ex-\nplainability in Deepfake Speech Detection remains a complex\ntask due to the unique challenges posed by audio data and\nthe black-box nature of deep learning methods. Although\nvarious explainable AI (XAI) techniques prove effectiveness\nin interpreting deep-learning-based models, applying XAI\nto DSD systems has not drawn much attention from the\nresearch community. Indeed, only some recently published\npapers [206], [207], [208], [209], [210] address the role of\nXAI, which mainly focus on the visualization-based XAI\nmethods. For example, the conventional SHapley Additive\nexPlanations (SHAP) [211] and Local Interpretable Model-\nagnostic Explanations (LIME) [212] methods were used to\ninterpret the feature contribution in [207], [209] and in [208],\nrespectively. Authors in [206] applied Saliency Map [213]\nand Smooth Grad [214] techniques to visualize how their\nmodel processes audio in the frequency domain. Similarly,\nlayer-wise relevance propagation (LRP), a visualization-\nbased XAI method, was leveraged in [210] to indicate the\ndifference of formants among fake and real audio utterances.\nWhile more deep-learning-based models have been proposed\nto solve the DSD task, not many research papers focus on\nexploring XAI methods to interpret DSD systems.\n2) Future directions: Based on the above discussion, there\nis much room for applying XAI to improve transparency\nand trustworthiness within detection systems. Additionally,\nleveraging visualization tools for visualizing audio features\nor feature maps could also provide user-friendly platforms\nand valuable insights into the underlying decision-making\nprocess of detection models.\nD. Real-time deepfake speech detection\n1) Open challenges: Integrating DSD systems into real-\nworld applications still presents several challenges. Key fac-\ntors include the length of the audio utterance, the complexity\nof the model (e.g., the number of trainable parameters), com-\nputational costs (e.g., FLOPs), and the target edge devices\n(e.g., mobile phones, embedded systems, high-performance\ncomputers). These factors directly affect inference time and\nare carefully analyzed to ensure effective implementation.\nFor example, the trade-off between the performance and the\nmodel complexity was comprehensively analyzed in [196]\nand [215] concerning Acoustic Scene Classification (ASC)\ntask and Acoustic Event Detection (AED) task, respectively.\nCurrently, most proposed DSD systems have been currently\nevaluated on high-performance computers with the advance\nof powerful GPUs without any computational constraints,\nwhile there is little research on real-time deepfake detection.\nSeveral studies, such as [216] and [217], have proposed\nreal-time deepfake audio detection systems. However, these\nsystems often face significant limitations, such as being\napplicable to only a limited range of deepfake creation\ntechniques (voice conversion) or domains (communication).\nThese challenges highlight the need for further exploration\nand analysis of real-time DSD systems in future research.\n2) Future directions: Future directions in developing real-\ntime audio deepfake detection systems could rely on better\nhandling the trade-off between model complexity and per-\nformance, facilitating model implementation in low-latency\nconditions. Some techniques such as quantization and prun-\ning can be used to reduce model size, while other methods\nleverage edge computing or distributed computing to reduce\ninference time and handle large-scale data more efficiently.\nE. Ethical and legal considerations\n1) Open challenges: Training audio deepfake detection\nmodels requires large datasets, which may involve the collec-\ntion and the use of personal voice recordings. For example,\nVoxCeleb and FakeAVCeleb corpora contain speech from\nthousands of celebrities in various environments. Personal\ndata handling raises threats of privacy and consent. Further-\nmore, there is also a risk of dual-use dilemma when some bad\nactors could manipulate detection technology and available\nindividuals\u2019s speech for harmful purposes such as reinforcing\ndisinformation narratives, defamation, and fraud, infringing\non individuals\u2019 privacy rights.\n2) Future directions: Future directions in addressing eth-\nical and legal considerations for developing audio deepfake\ntechnologies focus on enhancing data privacy protection, fair-\nness, and facilitating global regulatory frameworks. Develop-\ners will increasingly incorporate privacy-by-design principles\nin developing detection systems, ensuring that personal voice\ndata is handled securely and with consent, minimizing the\nrisk of misuse. Within DSD applications, access control\nmechanisms should be implemented to limit certain groups\nof people and the frequency of using detection technologies,\nreducing the potential risk of misuse by malicious actors.\nIn terms of legal perspectives, legal frameworks may also\nevolve to introduce stricter penalties for misuse of both\ndeepfake creation and detection technology.\nF. The race between Deepfake Speech Generation and De-\ntection\n1) Open challenges:: As mentioned and discussed in\nSection III, there is a tight relationship between Deepfake\nSpeech Generation and Deepfake Speech Detection tasks.\nDeepfake Speech Generation systems (e.g., VC, TTS, and AT\nmodels) have been becoming more powerful and accessible,\nenabling the creation of hyper-realistic fake utterances that\nmimic normal speech patterns and produce fewer detectable\nflaws. This makes it hard for DSD systems to distinguish\nbetween real and manipulated content, presenting challenges\nto keep pace with these deepfake creation advancements.\n2) Future directions:: As deepfakes have evolved rapidly,\ndetection models must also adapt by learning from increas-\ningly realistic fakes. By facilitating collaborative environ-\nments, researchers in both Deepfake Speech Generation and\nDetection can further explore and push boundaries of what\nis technically possible and ensure that detection methods\nkeep pace with advances in deepfake generators. For ex-\nample, ADD 2022 [17], ADD 2023 [24], and ASVspoof\n2024 [27] challenge competitions were established to en-\ngage researchers in both Deepfake Speech Generation and\nDetection. This promotes innovations in addressing the race\nbetween creating and detecting deepfake, improving the\nrobustness of detection systems in combating increasingly\ncomplicated deepfakes.\nG. The availability of Deepfake Speech Detection tools\n1) Open challenges: Deepfake speech detection tools still\nface challenges in increasing their quantity and quality due\nto the rapid development of deepfake speech generation\ntechniques. Although DSD systems act as a critical func-\ntion in Voice over Internet Protocol (VoIP) based platforms\nsuch as WhatsApp, Facebook, etc. or social media such as\nYouTube, Twister, etc. for a thread warning, very few VoIP\nplatforms or social media have announced an available and\nindependent DSD tool. Regarding non-commercial or com-\nmercial solutions, only some DSD tools or platforms such\nas Deepware, WeVerify, TrueMedia, and DeepFake-O-Meter\nare available as highlighted in the survey [218]. However,\ninformation on DSD models used in these tools has been\nnot described in detail except TrueMeida and DeepFake-O-\nMeter with 3 and 5 systems replicated from published papers.\nOverall, the sufficiency of deepfake detection applications\nis primarily due to technical complexity in developing and\nupdating models, resource demands such as computational\ncosts and scalability, accuracy concerns, and privacy issues.\n2) Future directions:: To address the mentioned chal-\nlenges, future improvements in developing deepfake speech\ndetection tools could rely on some approaches such as\nlightweight detection models that can operate on consumer\ndevices such as smartphones, laptops, or cloud-based ser-\nvices. To ensure broader adaption, the development of open-\nsource deepfake detection tools or libraries and established\nstandards for their use could also be promoted by the collab-\noration between tech companies and academic institutions,\nmaking detection tools more accessible and reliable.\nVII. CONCLUSION\nThis paper has provided a comprehensive survey for\nDeepfake Speech Detection (DSD) task by deeply analyz-\ning the challenge competitions, the public and benchmark\ndatasets, the main components in a deep-learning-based DSD\nsystem. From the survey, we indicate exiting concerns and\nprovide enhance solutions to motivate the research commu-\nnity for further contribution on this research topic. More\nthan a survey, we verified the role and the effect of data\naugmentation, feature extraction, and network architectures\nGiven the comprehensive survey and extensive experiments,\nwe indicate potential and promising research directions for\nDeepfake Speech Detection task.\nREFERENCES\n[1] Zahra Khanjani, Gabrielle Watson, and Vandana P Janeja,\n\u201cHow\ndeep are the fakes? focusing on audio deepfake: A survey,\u201d arXiv\npreprint arXiv:2111.14203, 2021.\n[2] Momina Masood, Mariam Nawaz, Khalid Mahmood Malik, Ali\nJaved, Aun Irtaza, and Hafiz Malik,\n\u201cDeepfakes generation and\ndetection: State-of-the-art, open challenges, countermeasures, and\nway forward,\u201d Applied intelligence, vol. 53, no. 4, pp. 3974\u20134026,\n2023.\n[3] Rami Mubarak, Tariq Alsboui, Omar Alshaikh, Isa Inuwa-Dutse,\nSaad Khan, and Simon Parkinson, \u201cA survey on the detection and\nimpacts of deepfakes in visual, audio, and textual formats,\u201d IEEE\nAccess, vol. 11, pp. 144497\u2013144529, 2023.\n[4] Yogesh Patel, Sudeep Tanwar, Rajesh Gupta, Pronaya Bhattacharya,\nInnocent Ewean Davidson, Royi Nyameko, Srinivas Aluvala, and\nVrince Vimal, \u201cDeepfake generation and detection: Case study and\nchallenges,\u201d IEEE Access, vol. 11, pp. 143296\u2013143323, 2023.\n[5] Jiangyan Yi, Chenglong Wang, Jianhua Tao, Xiaohui Zhang,\nChu Yuan Zhang, and Yan Zhao,\n\u201cAudio deepfake detection: A\nsurvey,\u201d arXiv preprint arXiv:2308.14970, 2023.\n[6] Zahra Khanjani, Gabrielle Watson, and Vandana P Janeja, \u201cAudio\ndeepfakes: A survey,\u201d Frontiers in Big Data, vol. 5, pp. 1001063,\n2023.\n[7] Zahid Akhtar, Thanvi Lahari Pendyala, and Virinchi Sai Athmakuri,\n\u201cVideo and audio deepfake datasets and open issues in deepfake\ntechnology: Being ahead of the curve,\u201d Forensic Sciences, vol. 4,\nno. 3, pp. 289\u2013377, 2024.\n[8] Enes Altuncu, Virginia NL Franqueira, and Shujun Li, \u201cDeepfake:\ndefinitions, performance metrics and standards, datasets, and a meta-\nreview,\u201d Frontiers in Big Data, vol. 7, pp. 1400024, 2024.\n[9] Menglu Li, Yasaman Ahmadiadli, and Xiao-Ping Zhang,\n\u201cAudio\nanti-spoofing detection: A survey,\u201d arXiv preprint arXiv:2404.13914,\n2024.\n[10] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and\nHong Lin, \u201cAi-generated content (aigc): A survey,\u201d arXiv preprint\narXiv:2304.06632, 2023.\n[11] Burak Yetis\u00b8tiren, Is\u00b8\u0131k \u00a8Ozsoy, Miray Ayerdem, and Eray T\u00a8uz\u00a8un,\n\u201cEvaluating the code quality of ai-assisted code generation tools:\nAn empirical study on github copilot, amazon codewhisperer, and\nchatgpt,\u201d arXiv preprint arXiv:2304.10778, 2023.\n[12] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu, \u201cA survey on neural\nspeech synthesis,\u201d arXiv preprint arXiv:2106.15561, 2021.\n[13] Berrak Sisman, Junichi Yamagishi, Simon King, and Haizhou Li,\n\u201cAn overview of voice conversion and its challenges: From statistical\nmodeling to deep learning,\u201d\nIEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 29, pp. 132\u2013157, 2021.\n[14] Fatima Dakalbab, Manar Abu Talib, Omnia Abu Waraga, Ali Bou\nNassif, Sohail Abbas, and Qassim Nasir, \u201cArtificial intelligence &\ncrime prediction: A systematic literature review,\u201d Social Sciences &\nHumanities Open, vol. 6, no. 1, pp. 100342, 2022.\n[15] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes,\nMenglin Wang, and Cristian Canton Ferrer, \u201cThe deepfake detection\nchallenge (DFDC) dataset,\u201d arXiv preprint arXiv:2006.07397, 2020.\n[16] Joel Frank and Lea Sch\u00a8onherr, \u201cWavefake: A data set to facilitate\naudio deepfake detection,\u201d NeurIPS, 2024.\n[17] \u201cAudio deep synthesis detection challenge (ADD 2022),\u201d http://\naddchallenge.cn/add2022, 2022.\n[18] \u201cM-ailabs\nspeech\ndataset,\u201d\nhttps://github.com/\nimdatceleste/m-ailabs-dataset, 2024.\n[19] You Zhang, Yongyi Zang, Jiatong Shi, Ryuichi Yamamoto, Tomoki\nToda, and Zhiyao Duan, \u201cSVDD 2024: The inaugural singing voice\ndeepfake detection challenge,\u201d\narXiv preprint arXiv:2408.16132,\n2024.\n[20] Zhizheng Wu, Tomi Kinnunen, Nicholas Evans, Junichi Yamagishi,\nCemal Hanilc\u00b8i, Md. Sahidullah, and Aleksandr Sizov,\n\u201cAsvspoof\n2015: the first automatic speaker verification spoofing and counter-\nmeasures challenge,\u201d in Proc. INTERSPEECH, 2015, pp. 2037\u20132041.\n[21] Xin Wang, Junichi Yamagishi, Massimiliano Todisco, H\u00b4ector Del-\ngado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vest-\nman, Tomi Kinnunen, Kong Aik Lee, et al.,\n\u201cAsvspoof 2019: A\nlarge-scale public database of synthesized, converted and replayed\nspeech,\u201d Computer Speech & Language, vol. 64, pp. 101114, 2020.\n[22] \u201cThe\nftc\nvoice\ncloning\nchallenge,\u201d\nhttps:\n//www.ftc.gov/news-events/contests/\nftc-voice-cloning-challenge, 2023.\n[23] Junichi Yamagishi, Xin Wang, Massimiliano Todisco, Md Sahidullah,\nJose Patino, Andreas Nautsch, Xuechen Liu, Kong Aik Lee, Tomi\nKinnunen, Nicholas Evans, et al.,\n\u201cAsvspoof 2021: accelerating\nprogress in spoofed and deepfake speech detection,\u201d in Workshop-\nAutomatic Speaker Verification and Spoofing Coutermeasures Chal-\nlenge (ASVspoof), 2021.\n[24] \u201cAudio deep synthesis detection challenge (ADD 2023),\u201d http://\naddchallenge.cn/add2023, 2023.\n[25] Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat,\nAbhinav Dhall, and Kalin Stefanov,\n\u201cAv-deepfake1m: A large-\nscale llm-driven audio-visual deepfake dataset,\u201d\narXiv preprint\narXiv:2311.15308, 2023.\n[26] \u201c1m-deepfakes detection challenge,\u201d https://deepfakes1m.\ngithub.io/, 2023.\n[27] \u201cThe\nasvspoof\n2024\nchallenge,\u201d\nhttps://www.asvspoof.\norg/, 2024.\n[28] \u201cThe singing voice deepfake detection challenge (svdd),\u201d https:\n//challenge.singfake.org/, 2024.\n[29] H\u00b4ector Delgado, Massimiliano Todisco, Md Sahidullah, Nicholas\nEvans, Tomi Kinnunen, Kong Aik Lee, and Junichi Yamagishi,\n\u201cAsvspoof 2017 version 2.0: meta-data analysis and baseline en-\nhancements,\u201d in The Speaker and Language Recognition Workshop,\n2018, pp. 296\u2013303.\n[30] Ricardo Reimao and Vassilios Tzerpos, \u201cFor: A dataset for synthetic\nspeech detection,\u201d in International Conference on Speech Technology\nand Human-Computer Dialogue, 2019, pp. 1\u201310.\n[31] \u201cAudio\nsource\nused\nto\ngenerate\nfor\ndataset,\u201d\nhttps:\n//www.kaggle.com/datasets/percevalw/\nenglishfrench-translations, 2018.\n[32] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Nor-\nman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord,\nSander Dieleman, and Koray Kavukcuoglu, \u201cEfficient neural audio\nsynthesis,\u201d in Proc. ICML, 2018, pp. 2410\u20132419.\n[33] Ryosuke Sonobe, Shinnosuke Takamichi, and Hiroshi Saruwatari,\n\u201cJsut corpus: free large-scale japanese speech corpus for end-to-end\nspeech synthesis,\u201d arXiv preprint arXiv:1711.00354, 2017.\n[34] Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and\nGyeongsu Chae,\n\u201cKodf: A large-scale korean deepfake detection\ndataset,\u201d in Proc. IEEE/CVF International Conference on Computer\nVision, 2021, pp. 10744\u201310753.\n[35] Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li, \u201cAishell-3: A\nmulti-speaker mandarin tts corpus,\u201d in Proc. INTERSPEECH, 2021,\npp. 2756\u20132760.\n[36] Hasam Khalid, Shahroz Tariq, Minha Kim, and Simon S Woo,\n\u201cFakeavceleb: A novel audio-video multimodal deepfake dataset,\u201d\nin Thirty-fifth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2), 2021.\n[37] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman,\n\u201cVox-\nCeleb2: Deep Speaker Recognition,\u201d in Proc. INTERSPEECH, 2018,\npp. 1086\u20131090.\n[38] Nicolas M\u00a8uller, Pavel Czempin, Franziska Diekmann, Adam Frogh-\nyar, and Konstantin B\u00a8ottinger,\n\u201cDoes Audio Deepfake Detection\nGeneralize?,\u201d in Proc. INTERSPEECH, 2022, pp. 2783\u20132787.\n[39] Zhixi Cai, Kalin Stefanov, Abhinav Dhall, and Munawar Hayat,\n\u201cDo you really mean that? content driven audio-visual deepfake\ndataset and multimodal method for temporal forgery localization,\u201d in\nInternational Conference on Digital Image Computing: Techniques\nand Applications, 2022, pp. 1\u201310.\n[40] Xin Wang and Junichi Yamagishi, \u201cSpoofed training data for speech\nspoofing countermeasure can be efficiently created using neural\nvocoders,\u201d in Proc. ICASSP, 2023, pp. 1\u20135.\n[41] Lin Zhang, Xin Wang, Erica Cooper, Nicholas Evans, and Junichi\nYamagishi, \u201cThe partialspoof database and countermeasures for the\ndetection of short fake speech segments embedded in an utterance,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language Process-\ning, vol. 31, pp. 813\u2013825, 2022.\n[42] Chengzhe Sun, Shan Jia, Shuwei Hou, and Siwei Lyu,\n\u201cAi-\nsynthesized voice detection using neural vocoder artifacts,\u201d in Proc.\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2023, pp. 904\u2013912.\n[43] Haoxin Ma, Jiangyan Yi, Chenglong Wang, Xinrui Yan, Jianhua Tao,\nTao Wang, Shiming Wang, and Ruibo Fu, \u201cCFAD: A chinese dataset\nfor fake audio detection,\u201d\nSpeech Communication, vol. 164, pp.\n103122, 2024.\n[44] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng,\n\u201cAISHELL-1: An open-source mandarin speech corpus and a speech\nrecognition baseline.,\u201d in Proc. O-COCOSDA, 2017, pp. 1\u20135.\n[45] Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li, \u201cAishell-3: A\nmulti-speaker mandarin tts corpus,\u201d in Proc. INTERSPEECH, 2021,\npp. 2756\u20132760.\n[46] Zehui Yang, Yifan Chen, Lei Luo, Runyan Yang, Lingxuan Ye,\nGaofeng Cheng, Ji Xu, Yaohui Jin, Qingqing Zhang, Pengyuan\nZhang, Lei Xie, and Yonghong Yan,\n\u201cOpen source MagicData-\nRAMC: A rich annotated mandarin conversational(RAMC) speech\ndataset,\u201d in Proc. INTERSPEECH, 2022, pp. 1736\u20131740.\n[47] Nicolas M M\u00a8uller, Piotr Kawa, Wei Herng Choong, Edresson\nCasanova, Eren G\u00a8olge, Thorsten M\u00a8uller, Piotr Syga, Philip Sperl,\nand Konstantin B\u00a8ottinger, \u201cMlaad: The multi-language audio anti-\nspoofing dataset,\u201d International Joint Conference on Neural Networks\n(IJCNN), 2024.\n[48] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and\nRonan Collobert,\n\u201cMLS: A Large-Scale Multilingual Dataset for\nSpeech Research,\u201d in Proc. INTERSPEECH, 2020, pp. 2757\u20132761.\n[49] \u201cDCASE\n2022\nchallenge\ncompetition\nTask\n1A,\u201d\nhttps://dcase.community/challenge2022/\ntask-low-complexity-acoustic-scene-classification,\n2022.\n[50] Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-Jin Liu,\n\u201cAudio-driven talking face video generation with learning-based\npersonalized head pose,\u201d arXiv preprint arXiv:2002.10137, 2020.\n[51] Thierry Dutoit, Andre Holzapfel, Matthieu Jottrand, Alexis Moinet,\nJavier Perez, and Yannis Stylianou,\n\u201cTowards a voice conversion\nsystem based on frame selection,\u201d in Proc. ICASSP, 2007, vol. 4,\npp. IV\u2013513.\n[52] Zhizheng Wu, Tuomas Virtanen, Tomi Kinnunen, Engsiong Chng,\nand Haizhou Li, \u201cExemplar-based unit selection for voice conversion\nutilizing temporal information.,\u201d in Proc. INTERSPEECH, 2013, pp.\n3057\u20133061.\n[53] Toshiaki Fukuda, \u201cAn adaptive algorithm for mel-cepstral analysis\nof speech,\u201d in Proc. ICASSP, 1992, pp. 137\u2013140.\n[54] Junichi Yamagishi, Takao Kobayashi, Yuji Nakano, Katsumi Ogata,\nand Juri Isogai,\n\u201cAnalysis of speaker adaptation algorithms for\nhmm-based speech synthesis and a constrained smaplr adaptation\nalgorithm,\u201d\nIEEE Transactions on Audio, Speech, and Language\nProcessing, vol. 17, no. 1, pp. 66\u201383, 2009.\n[55] \u201cFestvox voice conversion system,\u201d http://www.festvox.org,\n2024.\n[56] Tomoki Toda, Alan W Black, and Keiichi Tokuda, \u201cVoice conver-\nsion based on maximum-likelihood estimation of spectral parameter\ntrajectory,\u201d\nIEEE Transactions on Audio, Speech, and Language\nProcessing, vol. 15, no. 8, pp. 2222\u20132235, 2007.\n[57] Daisuke Saito, Keisuke Yamamoto, Nobuaki Minematsu, and Kei-\nkichi Hirose,\n\u201cOne-to-many voice conversion based on tensor\nrepresentation of speaker space,\u201d\nin Proc. INTERSPEECH, 2011,\npp. 653\u2013656.\n[58] Elina Helander, Hanna Sil\u00b4en, Tuomas Virtanen, and Moncef Gabbouj,\n\u201cVoice conversion using dynamic kernel partial least squares regres-\nsion,\u201d IEEE transactions on audio, speech, and language processing,\nvol. 20, no. 3, pp. 806\u2013817, 2011.\n[59] \u201cMarytts speech synthesis system,\u201d http://mary.dfki.de,\n2024.\n[60] \u201cHts working group, the english tts system flite+hts engine,\u201d http:\n//hts-engine.sourceforge.net/, 2014.\n[61] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa,\n\u201cWorld:\na vocoder-based high-quality speech synthesis system for real-time\napplications,\u201d IEICE Transactions on Information and Systems, vol.\n99, no. 7, pp. 1877\u20131884, 2016.\n[62] Zhizheng Wu, Oliver Watts, and Simon King,\n\u201cMerlin: An open\nsource neural network speech synthesis system,\u201d in Speech Synthesis\nWorkshop, 2016, pp. 202\u2013207.\n[63] Marc Schr\u00a8oder, Marcela Charfuelan, Sathish Pammi, and Ingmar\nSteiner,\n\u201cOpen source voice creation toolkit for the mary tts\nplatform,\u201d in Proc. INTERSPEECH, 2011, pp. 3253\u20133256.\n[64] Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-\nMin Wang,\n\u201cVoice conversion from non-parallel corpora using\nvariational auto-encoder,\u201d in Proc. APSIPA, 2016, pp. 1\u20136.\n[65] Driss Matrouf, J-F Bonastre, and Corinne Fredouille,\n\u201cEffect of\nspeech transformation on impostor acceptance,\u201d in Proc. ICASSP,\n2006, vol. 1, pp. I\u2013I.\n[66] Kou Tanaka, Hirokazu Kameoka, Takuhiro Kaneko, and Nobukatsu\nHojo, \u201cWavecyclegan2: Time-domain neural post-filter for speech\nwaveform generation,\u201d arXiv preprint arXiv:1904.02892, 2019.\n[67] Xin Wang, Shinji Takaki, and Junichi Yamagishi, \u201cNeural source-\nfilter-based waveform model for statistical parametric speech synthe-\nsis,\u201d in Proc. ICASSP, 2019, pp. 5916\u20135920.\n[68] Heiga Zen, Yannis Agiomyrgiannakis, Niels Egberts, Fergus Hender-\nson, and Przemys\u0142aw Szczepaniak, \u201cFast, compact, and high quality\nlstm-rnn based statistical parametric speech synthesizers for mobile\ndevices,\u201d in Proc. INTERSPEECH, 2016, pp. 2273\u20132277.\n[69] Yannis Agiomyrgiannakis, \u201cVocaine the vocoder and applications in\nspeech synthesis,\u201d in Proc. ICASSP, 2015, pp. 4230\u20134234.\n[70] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno,\n\u201cGeneralized end-to-end loss for speaker verification,\u201d\nin Proc.\nICASSP, 2018, pp. 4879\u20134883.\n[71] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Nor-\nman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord,\nSander Dieleman, and Koray Kavukcuoglu, \u201cEfficient neural audio\nsynthesis,\u201d in Proc. ICML, 2018, pp. 2410\u20132419.\n[72] Daniel Griffin and Jae Lim, \u201cSignal estimation from modified short-\ntime fourier transform,\u201d IEEE Transactions on acoustics, speech, and\nsignal processing, vol. 32, no. 2, pp. 236\u2013243, 1984.\n[73] A\u00a8aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan,\nOriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and\nKoray Kavukcuoglu,\n\u201cWaveNet: A Generative Model for Raw\nAudio,\u201d in Proc. Workshop on Speech Synthesis, 2016, p. 125.\n[74] \u201cVoicetext,\u201d\nhttp://dws2.voicetext.jp/tomcat/\ndemonstration/top.html, 2024.\n[75] Li-Juan Liu, Zhen-Hua Ling, Yuan Jiang, Ming Zhou, and Li-\nRong Dai, \u201cWavenet vocoder with limited training data for voice\nconversion.,\u201d in Proc. INTERSPEECH, 2018, pp. 1983\u20131987.\n[76] Hideki Kawahara, Ikuyo Masuda-Katsuse, and Alain De Cheveigne,\n\u201cRestructuring speech representations using a pitch-adaptive time\u2013\nfrequency smoothing and an instantaneous-frequency-based f0 ex-\ntraction: Possible role of a repetitive structure in sounds,\u201d Speech\ncommunication, vol. 27, no. 3-4, pp. 187\u2013207, 1999.\n[77] Kazuhiro Kobayashi, Tomoki Toda, and Satoshi Nakamura, \u201cIntra-\ngender statistical singing voice conversion with direct waveform\nmodification using log-spectral differential,\u201d Speech communication,\nvol. 99, pp. 211\u2013220, 2018.\n[78] Wen-Chin Huang, Yi-Chiao Wu, Kazuhiro Kobayashi, Yu-Huai Peng,\nHsin-Te Hwang, Patrick Lumban Tobing, Yu Tsao, Hsin-Min Wang,\nand Tomoki Toda, \u201cGeneralization of spectrum differential based di-\nrect waveform modification for voice conversion,\u201d in Proc. Workshop\non Speech Synthesis, 2019, pp. 57\u201362.\n[79] Najim Dehak, Patrick J Kenny, R\u00b4eda Dehak, Pierre Dumouchel, and\nPierre Ouellet, \u201cFront-end factor analysis for speaker verification,\u201d\nIEEE Transactions on Audio, Speech, and Language Processing, vol.\n19, no. 4, pp. 788\u2013798, 2010.\n[80] Patrick Kenny, \u201cA small footprint i-vector extractor.,\u201d in Odyssey,\n2012, vol. 2012, pp. 1\u20136.\n[81] Simon JD Prince and James H Elder,\n\u201cProbabilistic linear dis-\ncriminant analysis for inferences about identity,\u201d\nin Proc. IEEE\ninternational conference on computer vision, 2007, pp. 1\u20138.\n[82] Adam Polyak, Lior Wolf, and Yaniv Taigman, \u201cTTS Skins: Speaker\nConversion via ASR,\u201d in Proc. INTERSPEECH, 2020, pp. 786\u2013790.\n[83] KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and\nCV Jawahar, \u201cA lip sync expert is all you need for speech to lip\ngeneration in the wild,\u201d in Proceedings of the 28th ACM international\nconference on multimedia, 2020, pp. 484\u2013492.\n[84] Xuechen Liu, Xin Wang, Md Sahidullah, Jose Patino, H\u00b4ector Del-\ngado, Tomi Kinnunen, Massimiliano Todisco, Junichi Yamagishi,\nNicholas Evans, Andreas Nautsch, and Kong Aik Lee, \u201cAsvspoof\n2021: Towards spoofed and deepfake speech detection in the wild,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language Process-\ning, vol. 31, pp. 2507\u20132522, 2023.\n[85] Kundan Kumar, Rithesh Kumar, Thibault De Boissiere, Lucas Gestin,\nWei Zhen Teoh, Jose Sotelo, Alexandre De Brebisson, Yoshua\nBengio, and Aaron C Courville,\n\u201cMelgan: Generative adversarial\nnetworks for conditional waveform synthesis,\u201d Advances in neural\ninformation processing systems, vol. 32, 2019.\n[86] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, \u201cHifi-gan: Gen-\nerative adversarial networks for efficient and high fidelity speech\nsynthesis,\u201d Advances in neural information processing systems, vol.\n33, pp. 17022\u201317033, 2020.\n[87] Durk P Kingma and Prafulla Dhariwal,\n\u201cGlow: Generative flow\nwith invertible 1x1 convolutions,\u201d Advances in neural information\nprocessing systems, vol. 31, 2018.\n[88] Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim,\n\u201cParallel\nwavegan: A fast waveform generation model based on generative\nadversarial networks with multi-resolution spectrogram,\u201d\nin Proc.\nICASSP, 2020, pp. 6199\u20136203.\n[89] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren,\nPatrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui\nWu, et al.,\n\u201cTransfer learning from speaker verification to multi-\nspeaker text-to-speech synthesis,\u201d Advances in neural information\nprocessing systems, vol. 31, 2018.\n[90] KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and\nCV Jawahar, \u201cA lip sync expert is all you need for speech to lip\ngeneration in the wild,\u201d in Proc. ACM international conference on\nmultimedia, 2020, pp. 484\u2013492.\n[91] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren,\nPatrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui\nWu, et al.,\n\u201cTransfer learning from speaker verification to multi-\nspeaker text-to-speech synthesis,\u201d Advances in neural information\nprocessing systems, vol. 31, 2018.\n[92] Xin Wang, Shinji Takaki, and Junichi Yamagishi, \u201cNeural source-\nfilter waveform models for statistical parametric speech synthesis,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language Process-\ning, vol. 28, pp. 402\u2013415, 2019.\n[93] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman\nCasagrande, Edward Lockhart, Florian Stimberg, Aaron van den\nOord, Sander Dieleman, and Koray Kavukcuoglu, \u201cEfficient neural\naudio synthesis,\u201d in Proc. ICML, 2018, pp. 2410\u20132419.\n[94] Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim,\n\u201cParallel\nwavegan: A fast waveform generation model based on generative\nadversarial networks with multi-resolution spectrogram,\u201d\nin Proc.\nICASSP, 2020, pp. 6199\u20136203.\n[95] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad\nNorouzi, and William Chan, \u201cWavegrad: Estimating gradients for\nwaveform generation,\u201d in Proc. ICLR, 2021.\n[96] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catan-\nzaro, \u201cDiffwave: A versatile diffusion model for audio synthesis,\u201d in\nProc. ICLR, 2021.\n[97] Jaehyeon Kim, Jungil Kong, and Juhee Son, \u201cConditional variational\nautoencoder with adversarial learning for end-to-end text-to-speech,\u201d\nin Proc. ICML, 2021, pp. 5530\u20135540.\n[98] Edresson Casanova, Julian Weber, Christopher D Shulby, Ar-\nnaldo Candido Junior, Eren G\u00a8olge, and Moacir A Ponti, \u201cYourtts:\nTowards zero-shot multi-speaker tts and zero-shot voice conversion\nfor everyone,\u201d in Proc. ICML, 2022, pp. 2709\u20132720.\n[99] Hideki Kawahara,\n\u201cStraight, exploitation of the other aspect of\nvocoder: Perceptually isomorphic decomposition of speech sounds,\u201d\nAcoustical Science and Technology, vol. 27, no. 6, pp. 349\u2013353, 2006.\n[100] Nathana\u00a8el Perraudin, Peter Balazs, and Peter L. S\u00f8ndergaard, \u201cA fast\ngriffin-lim algorithm,\u201d in IEEE Workshop on Applications of Signal\nProcessing to Audio and Acoustics, 2013, pp. 1\u20134.\n[101] Jean-Marc Valin and Jan Skoglund,\n\u201cLpcnet: Improving neural\nspeech synthesis through linear prediction,\u201d in Proc. ICASSP, 2019,\npp. 5891\u20135895.\n[102] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, \u201cHifi-gan: gen-\nerative adversarial networks for efficient and high fidelity speech\nsynthesis,\u201d in Proc. NeurIPS, 2020.\n[103] Masanori MORISE, Fumiya YOKOMORI, and Kenji OZAWA,\n\u201cWorld: A vocoder-based high-quality speech synthesis system for\nreal-time applications,\u201d\nIEICE Transactions on Information and\nSystems, vol. E99.D, no. 7, pp. 1877\u20131884, 2016.\n[104] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and\nTie-Yan Liu, \u201cFastspeech 2: Fast and high-quality end-to-end text to\nspeech,\u201d arXiv preprint arXiv:2006.04558, 2020.\n[105] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J.\nWeiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen,\nSamy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, and\nRif A. Saurous, \u201cTacotron: Towards end-to-end speech synthesis,\u201d\nin Proc. INTERSPEECH, 2017, pp. 4006\u20134010.\n[106] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon,\n\u201cGlow-tts: A generative flow for text-to-speech via monotonic align-\nment search,\u201d Advances in Neural Information Processing Systems,\nvol. 33, pp. 8067\u20138077, 2020.\n[107] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova,\nand Mikhail Kudinov, \u201cGrad-tts: A diffusion probabilistic model for\ntext-to-speech,\u201d in Proc. ICML, 2021, pp. 8599\u20138608.\n[108] Adrian \u0141a\u00b4ncucki,\n\u201cFastpitch: Parallel text-to-speech with pitch\nprediction,\u201d in Proc. ICASSP, 2021, pp. 6588\u20136592.\n[109] Jaehyeon Kim, Jungil Kong, and Juhee Son, \u201cConditional variational\nautoencoder with adversarial learning for end-to-end text-to-speech,\u201d\nin Proc. ICML, 2021, pp. 5530\u20135540.\n[110] Florian Lux, Julia Koch, and Ngoc Thang Vu,\n\u201cLow-resource\nmultilingual and zero-shot multispeaker tts,\u201d in Proc. AACL, 2022,\npp. 741\u2013751.\n[111] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, \u201cHifi-gan: Gen-\nerative adversarial networks for efficient and high fidelity speech\nsynthesis,\u201d Advances in neural information processing systems, vol.\n33, pp. 17022\u201317033, 2020.\n[112] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep\nJaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang,\nRj Skerrv-Ryan, et al., \u201cNatural tts synthesis by conditioning wavenet\non mel spectrogram predictions,\u201d in Proc. ICASSP, 2018, pp. 4779\u2013\n4783.\n[113] Yinghao Aaron Li, Ali Zare, and Nima Mesgarani, \u201cStarganv2-vc: A\ndiverse, unsupervised, non-parallel framework for natural-sounding\nvoice conversion,\u201d in Proc. INTERSPEECH, 2021, pp. 1349\u20131353.\n[114] Edresson Casanova, Julian Weber, Christopher D Shulby, Ar-\nnaldo Candido Junior, Eren G\u00a8olge, and Moacir A Ponti, \u201cYourtts:\nTowards zero-shot multi-speaker tts and zero-shot voice conversion\nfor everyone,\u201d in Proc. ICML, 2022, pp. 2709\u20132720.\n[115] Ehab A AlBadawy and Siwei Lyu, \u201cVoice conversion using speech-\nto-speech neuro-style transfer.,\u201d in Proc. INTERSPEECH, 2020, pp.\n4726\u20134730.\n[116] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao Wang,\nJianwu Dang, Korin Richmond, and Junichi Yamagishi, \u201cZmm-tts:\nZero-shot multilingual and multispeaker speech synthesis conditioned\non self-supervised discrete speech representations,\u201d\nIEEE/ACM\nTransactions on Audio, Speech, and Language Processing, 2024.\n[117] Ingmar Steiner and S\u00b4ebastien Le Maguer, \u201cCreating new language\nand voice components for the updated marytts text-to-speech synthe-\nsis platform,\u201d in Proc. LREC, 2018, pp. 1371\u20131375.\n[118] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and\nSungroh Yoon, \u201cBigvgan: A universal neural vocoder with large-\nscale training,\u201d in Proc. ICLR, 2022.\n[119] Florian Lux, Julia Koch, and Ngoc Thang Vu, \u201cExact prosody cloning\nin zero-shot multispeaker text-to-speech,\u201d in Proc. SLT, 2023, pp.\n962\u2013969.\n[120] Florian Lux, Julia Koch, and Ngoc Thang Vu,\n\u201cLow-resource\nmultilingual and zero-shot multispeaker tts,\u201d in Proc. AACL, 2022.\n[121] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova,\nMikhail Kudinov, and Jiansheng Wei, \u201cDiffusion-based voice con-\nversion with fast maximum likelihood sampling scheme,\u201d in Proc.\nICLR, 2022.\n[122] Edresson Casanova, Julian Weber, Christopher D Shulby, Ar-\nnaldo Candido Junior, Eren G\u00a8olge, and Moacir A Ponti, \u201cYourtts:\nTowards zero-shot multi-speaker tts and zero-shot voice conversion\nfor everyone,\u201d in Proc. ICML, 2022, pp. 2709\u20132720.\n[123] Edresson Casanova, Kelly Davis, Eren G\u00a8olge, G\u00a8orkem G\u00a8oknar, Iulian\nGulea, Logan Hart, Aya Aljafari, Joshua Meyer, Reuben Morais,\nSamuel Olayemi, et al., \u201cXtts: a massively multilingual zero-shot\ntext-to-speech model,\u201d in Proc. INTERSPEECH, 2024, pp. 4978\u2013\n4982.\n[124] Michele\nPanariello,\nWanying\nGe,\nHemlata\nTak,\nMassimiliano\nTodisco, and Nicholas Evans, \u201cMalafide: a novel adversarial convo-\nlutive noise attack against deepfake and spoofing detection systems,\u201d\nin Proc. INTERSPEECH, 2023, pp. 2868\u20132872.\n[125] Massimiliano Todisco, Michele Panariello, Xin Wang, Hector Del-\ngado, Kong Aik Lee, and Nicholas Evans, \u201cMalacopula: adversarial\nautomatic speaker verification attacks using a neural-based gener-\nalised hammerstein model,\u201d arXiv preprint arXiv:2408.09300, 2024.\n[126] Joaqu\u0131n C\u00b4aceres, Roberto Font, Teresa Grau, Javier Molina, and\nBiometric Vox SL,\n\u201cThe biometric vox system for the asvspoof\n2021 challenge,\u201d in Edition of the Automatic Speaker Verification\nand Spoofing Countermeasures Challenge, 2021, pp. 68\u201374.\n[127] Rohan Kumar Das, \u201cKnown-unknown data augmentation strategies\nfor detection of logical access, physical access and speech deepfake\nattacks: Asvspoof 2021,\u201d\nin Edition of the Automatic Speaker\nVerification and Spoofing Countermeasures Challenge, 2021, pp. 29\u2013\n36.\n[128] Wanying Ge, Jose Patino, Massimiliano Todisco, and Nicholas Evans,\n\u201cRaw differentiable architecture search for speech deepfake and\nspoofing detection,\u201d in Edition of the Automatic Speaker Verification\nand Spoofing Countermeasures Challenge, 2021, pp. 22\u201328.\n[129] Woo Hyun Kang, Jahangir Alam, and Abderrahim Fathan, \u201cCrim\u2019s\nsystem description for the asvspoof2021 challenge,\u201d in Edition of\nthe Automatic Speaker Verification and Spoofing Countermeasures\nChallenge, 2021, pp. 100\u2013106.\n[130] Nicolas M M\u00a8uller, Franziska Dieckmann, Pavel Czempin, Roman\nCanals, Konstantin B\u00a8ottinger, and Jennifer Williams,\n\u201cSpeech is\nsilver, silence is golden: What do asvspoof-trained models really\nlearn?,\u201d in Edition of the Automatic Speaker Verification and Spoofing\nCountermeasures Challenge, 2021, pp. 55\u201360.\n[131] Hemlata Tak, Jee-weon Jung, Jose Patino, Madhu Kamble, Massi-\nmiliano Todisco, and Nicholas Evans, \u201cEnd-to-end spectro-temporal\ngraph attention networks for speaker verification anti-spoofing and\nspeech deepfake detection,\u201d\nin Edition of the Automatic Speaker\nVerification and Spoofing Countermeasures Challenge, 2021, pp. 1\u2013\n8.\n[132] Anton\nTomilov,\nAleksei\nSvishchev,\nMarina\nVolkova,\nArtem\nChirkovskiy, Alexander Kondratev, and Galina Lavrentyeva,\n\u201cStc\nantispoofing systems for the asvspoof2021 challenge,\u201d in Edition of\nthe Automatic Speaker Verification and Spoofing Countermeasures\nChallenge, 2021, pp. 61\u201367.\n[133] Xingming Wang, Xiaoyi Qin, Tinglong Zhu, Chao Wang, Shilei\nZhang, and Ming Li, \u201cThe dku-cmri system for the asvspoof 2021\nchallenge: vocoder based replay channel response estimation,\u201d\nin\nEdition of the Automatic Speaker Verification and Spoofing Counter-\nmeasures Challenge, 2021, pp. 16\u201321.\n[134] Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son\nChung, Bong-Jin Lee, Ha-Jin Yu, and Nicholas Evans,\n\u201cAasist:\nAudio anti-spoofing using integrated spectro-temporal graph attention\nnetworks,\u201d in Proc. ICASSP, 2022, pp. 6367\u20136371.\n[135] Hemlata Tak, Massimiliano Todisco, Xin Wang, Jee-weon Jung,\nJunichi Yamagishi, and Nicholas Evans, \u201cAutomatic speaker veri-\nfication spoofing and deepfake detection using wav2vec 2.0 and data\naugmentation,\u201d in The Speaker and Language Recognition Workshop,\n2022.\n[136] Hemlata Tak, Madhu Kamble, Jose Patino, Massimiliano Todisco,\nand Nicholas Evans, \u201cRawboost: A raw data boosting and augmenta-\ntion method applied to automatic speaker verification anti-spoofing,\u201d\nin Proc. ICASSP, 2022, pp. 6382\u20136386.\n[137] Rui Liu, Jinhua Zhang, Guanglai Gao, and Haizhou Li,\n\u201cBetray\nOneself: A Novel Audio DeepFake Detection Model via Mono-to-\nStereo Conversion,\u201d in Proc. INTERSPEECH, 2023, pp. 3999\u20134003.\n[138] Chenglong Wang, Jiangyan Yi, Jianhua Tao, Chu Yuan Zhang,\nShuai Zhang, and Xun Chen,\n\u201cDetection of Cross-Dataset Fake\nAudio Based on Prosodic and Pronunciation Features,\u201d\nin Proc.\nINTERSPEECH, 2023, pp. 3844\u20133848.\n[139] Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mo-\nhamed, and Michael Auli, \u201cUnsupervised Cross-Lingual Represen-\ntation Learning for Speech Recognition,\u201d in Proc. INTERSPEECH,\n2021, pp. 2426\u20132430.\n[140] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal\nLakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, \u201cHu-\nbert: Self-supervised speech representation learning by masked pre-\ndiction of hidden units,\u201d IEEE/ACM transactions on audio, speech,\nand language processing, vol. 29, pp. 3451\u20133460, 2021.\n[141] Xiao-Min Zeng, Jian-Tao Zhang, Kang Li, Zhuo-Li Liu, Wei-Lin\nXie, and Yan Song, \u201cDeepfake algorithm recognition system with\naugmented data for add 2023 challenge.,\u201d in Proc. IJCAI, 2023, pp.\n31\u201336.\n[142] Zhongwei Teng, Quchen Fu, Jules White, Maria E Powell, and Dou-\nglas C Schmidt, \u201cSa-sasv: An end-to-end spoof-aggregated spoofing-\naware speaker verification system,\u201d in Proc. INTERSPEECH, 2022,\npp. 4391\u20134395.\n[143] Jun Xue, Cunhang Fan, Jiangyan Yi, Chenglong Wang, Zhengqi Wen,\nDan Zhang, and Zhao Lv, \u201cLearning from yourself: A self-distillation\nmethod for fake speech detection,\u201d in Proc. ICASSP, 2023, pp. 1\u20135.\n[144] Yuankun Xie, Haonan Cheng, Yutian Wang, and Long Ye, \u201cLearning\na self-supervised domain-invariant feature representation for general-\nized audio deepfake detection,\u201d in Proc. INTERSPEECH, 2023, pp.\n2808\u20132812.\n[145] Yujie Yang, Haochen Qin, Hang Zhou, Chengcheng Wang, Tianyu\nGuo, Kai Han, and Yunhe Wang, \u201cA robust audio deepfake detection\nsystem via multi-view feature,\u201d in Proc. ICASSP, 2024, pp. 13131\u2013\n13135.\n[146] Alexandre D\u00b4efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi,\n\u201cHigh fidelity neural audio compression,\u201d Transactions on Machine\nLearning Research, 2023.\n[147] Yi-Chiao Wu, Israel D Gebru, Dejan Markovi\u00b4c, and Alexander\nRichard, \u201cAudiodec: An open-source streaming high-fidelity neural\naudio codec,\u201d in Proc. ICASSP, 2023, pp. 1\u20135.\n[148] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael\nAuli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer,\n\u201cMasked autoencoders that listen,\u201d Advances in Neural Information\nProcessing Systems, vol. 35, pp. 28708\u201328720, 2022.\n[149] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie\nLiu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong\nXiao, et al., \u201cWavlm: Large-scale self-supervised pre-training for full\nstack speech processing,\u201d IEEE Journal of Selected Topics in Signal\nProcessing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\n[150] Alec Radford et al., \u201cRobust speech recognition via large-scale weak\nsupervision,\u201d in Proc. ICML, 2023, pp. 28492\u201328518.\n[151] Yinlin Guo, Haofan Huang, Xi Chen, He Zhao, and Yuehai Wang,\n\u201cAudio deepfake detection with self-supervised wavlm and multi-\nfusion attentive classifier,\u201d in Proc. ICASSP, 2024, pp. 12702\u201312706.\n[152] Alessandro Pianese, Davide Cozzolino, Giovanni Poggi, and Luisa\nVerdoliva, \u201cTraining-free deepfake voice recognition by leveraging\nlarge-scale pre-trained models,\u201d in Proc. ACM Workshop on Infor-\nmation Hiding and Multimedia Security, 2024, pp. 289\u2013294.\n[153] Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins,\nZhuo Chen, Wanxiang Che, Xiangzhan Yu, and Furu Wei, \u201cBeats:\naudio pre-training with acoustic tokenizers,\u201d in Proc. ICML, 2023,\npp. 5178\u20135193.\n[154] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-\nKirkpatrick, and Shlomo Dubnov, \u201cLarge-scale contrastive language-\naudio pretraining with feature fusion and keyword-to-caption aug-\nmentation,\u201d in Proc. ICASSP, 2023, pp. 1\u20135.\n[155] Andrey Guzhov, Federico Raue, J\u00a8orn Hees, and Andreas Dengel,\n\u201cAudioclip: Extending clip to image, text and audio,\u201d\nin Proc.\nICASSP, 2022, pp. 976\u2013980.\n[156] Tianxiang Chen, Avrosh Kumar, Parav Nagarsheth, Ganesh Sivara-\nman, and Elie Khoury, \u201cGeneralization of audio deepfake detection.,\u201d\nin Odyssey, 2020, pp. 132\u2013137.\n[157] Yuankun Xie, Haonan Cheng, Yutian Wang, and Long Ye, \u201cSingle\ndomain generalization for audio deepfake detection.,\u201d in Proc. IJCAI,\n2023, pp. 58\u201363.\n[158] Xinhui Chen, You Zhang, Ge Zhu, and Zhiyao Duan, \u201cUr channel-\nrobust synthetic speech detection system for asvspoof 2021,\u201d\nin\nEdition of the Automatic Speaker Verification and Spoofing Coun-\ntermeasures Challenge, 2021, pp. 75\u201382.\n[159] Zhor Benhafid, Sid Ahmed Selouani, Mohammed Sidi Yakoub, and\nAbderrahmane Amrouche,\n\u201cLarihs assert reassessment for logical\naccess asvspoof 2021 challenge,\u201d in Edition of the Automatic Speaker\nVerification and Spoofing Countermeasures Challenge, 2021, pp. 94\u2013\n99.\n[160] You Zhang, Fei Jiang, and Zhiyao Duan, \u201cOne-class learning towards\nsynthetic voice spoofing detection,\u201d IEEE Signal Processing Letters,\nvol. 28, pp. 937\u2013941, 2021.\n[161] Woo Hyun Kang, Jahangir Alam, and Abderrahim Fathan, \u201cInvesti-\ngation on activation functions for robust end-to-end spoofing attack\ndetection system,\u201d in Proc. INTERSPEECH, 2021, pp. 83\u201388.\n[162] Lin Zhang, Xin Wang, Erica Cooper, and Junichi Yamagishi, \u201cMulti-\ntask learning in utterance-level and segmental-level spoof detection,\u201d\nin Edition of the Automatic Speaker Verification and Spoofing Coun-\ntermeasures Challenge, 2021.\n[163] Yang Gao, Tyler Vuong, Mahsa Elyasi, Gaurav Bharaj, and Rita\nSingh,\n\u201cGeneralized spoofing detection inspired from audio gen-\neration artifacts,\u201d in Proc. INTERSPEECH, 2021, pp. 4184\u20134188.\n[164] Rui Yan, Cheng Wen, Shuran Zhou, Tingwei Guo, Wei Zou, and\nXiangang Li, \u201cAudio deepfake detection system with neural stitching\nfor add 2022,\u201d in Proc. ICASSP, 2022, pp. 9226\u20139230.\n[165] Yuankun Xie, Haonan Cheng, Yutian Wang, and Long Ye, \u201cDomain\ngeneralization via aggregation and separation for audio deepfake\ndetection,\u201d IEEE Transactions on Information Forensics and Security,\nvol. 19, pp. 344\u2013358, 2024.\n[166] Amit Kumar Singh Yadav, Emily R Bartusiak, Kratika Bhagtani, and\nEdward J Delp, \u201cSynthetic speech attribution using self supervised\naudio spectrogram transformer,\u201d\nElectronic Imaging, vol. 35, pp.\n1\u201311, 2023.\n[167] Yeqing\nRen,\nHaipeng\nPeng,\nLixiang\nLi,\nand\nYixian\nYang,\n\u201cLightweight voice spoofing detection using improved one-class\nlearning and knowledge distillation,\u201d\nIEEE Transactions on Mul-\ntimedia, 2023.\n[168] Yuxiang Zhang, Zhuo Li, Jingze Lu, Wenchao Wang, and Pengyuan\nZhang, \u201cSynthetic speech detection based on the temporal consis-\ntency of speaker features,\u201d IEEE Signal Processing Letters, vol. 31,\npp. 944\u2013948, 2024.\n[169] Junlong Deng, Yanzhen Ren, Tong Zhang, Hongcheng Zhu, and\nZongkun Sun,\n\u201cVfd-net: Vocoder fingerprints detection for fake\naudio,\u201d in Proc. ICASSP, 2024, pp. 12151\u201312155.\n[170] \u201cGan-based\nnetwork\ndecoders,\u201d\nhttps://github.com/\nkan-bayashi/ParallelWaveGAN, 2023.\n[171] Luca Cuccovillo, Milica Gerhardt, and Patrick Aichroth,\n\u201cAudio\ntransformer for synthetic speech detection via formant magnitude\nand phase analysis,\u201d in Proc. ICASSP, 2024, pp. 4805\u20134809.\n[172] Xin Wang and Junichi Yamagishi, \u201cCan large-scale vocoded spoofed\ndata improve speech spoofing countermeasure with a self-supervised\nfront end?,\u201d in Proc. ICASSP, 2024, pp. 10311\u201310315.\n[173] Hyun-seo Shin, Jungwoo Heo, Ju-ho Kim, Chan-yeong Lim, Wonbin\nKim, and Ha-Jin Yu,\n\u201cHm-conformer: A conformer-based audio\ndeepfake detection system with hierarchical pooling and multi-level\nclassification token aggregation methods,\u201d in Proc. ICASSP, 2024,\npp. 10581\u201310585.\n[174] Galina Lavrentyeva, Sergey Novoselov, Andzhukaev Tseren, Marina\nVolkova, Artem Gorlanov, and Alexandr Kozlov, \u201cStc antispoofing\nsystems for the asvspoof2019 challenge,\u201d in Proc. INTERSPEECH,\n2019, pp. 1033\u20131037.\n[175] Guang Hua, Andrew Beng Jin Teoh, and Haijian Zhang, \u201cTowards\nend-to-end synthetic speech detection,\u201d\nIEEE Signal Processing\nLetters, vol. 28, pp. 1265\u20131269, 2021.\n[176] Xin Wang and Junich Yamagishi, \u201cA comparative study on recent\nneural spoofing countermeasures for synthetic speech detection,\u201d in\nProc. INTERSPEECH, 2021, pp. 4259\u20134263.\n[177] Tianxiang Chen, Elie Khoury, Kedar Phatak, and Ganesh Sivaraman,\n\u201cPindrop labs\u2019 submission to the asvspoof 2021 challenge,\u201d in Edition\nof the Automatic Speaker Verification and Spoofing Countermeasures\nChallenge, 2021, pp. 89\u201393.\n[178] Yan Wen, Zhenchun Lei, Yingen Yang, Changhong Liu, and Minglei\nMa,\n\u201cMulti-path gmm-mobilenet based on attack algorithms and\ncodecs for synthetic speech and deepfake detection.,\u201d\nin Proc.\nINTERSPEECH, 2022, pp. 4795\u20134799.\n[179] Il-Youp Kwak, Sunmook Choi, Jonghoon Yang, Yerin Lee, Soyul\nHan, and Seungsang Oh, \u201cLow-quality fake audio detection through\nfrequency feature masking,\u201d in Proceedings of the 1st International\nWorkshop on Deepfake Detection for Audio Multimedia, 2022, pp.\n9\u201317.\n[180] Jiahui Pan, Shuai Nie, Hui Zhang, Shulin He, Kanghao Zhang, Shan\nLiang, Xueliang Zhang, and Jianhua Tao,\n\u201cSpeaker recognition-\nassisted robust audio deepfake detection.,\u201d in Proc. INTERSPEECH,\n2022, pp. 4202\u20134206.\n[181] Alexander Alenin, Nikita Torgashov, Anton Okhotnikov, Rostislav\nMakarov, and Ivan Yakovlev, \u201cA subnetwork approach for spoofing\naware speaker verification.,\u201d\nin Proc. INTERSPEECH, 2022, pp.\n2888\u20132892.\n[182] Shunbo Dong, Jun Xue, Cunhang Fan, Kang Zhu, Yujie Chen,\nand Zhao Lv,\n\u201cMulti-perspective information fusion res2net with\nrandomspecmix for fake speech detection,\u201d in Proc. IJCAI, 2023.\n[183] Ziqian Wang, Qing Wang, Jixun Yao, and Lei Xie, \u201cThe npu-aslp\nsystem for deepfake algorithm recognition in add 2023 challenge.,\u201d\nin Proc. IJCAI, 2023, pp. 64\u201369.\n[184] Chenglong Wang, Jiayi He, Jiangyan Yi, Jianhua Tao, Chu Yuan\nZhang, and Xiaohui Zhang,\n\u201cMulti-scale permutation entropy for\naudio deepfake detection,\u201d in Proc. ICASSP, 2024, pp. 1406\u20131410.\n[185] Yi Zhu, Surya Koppisetti, Trang Tran, and Gaurav Bharaj, \u201cSlim:\nStyle-linguistics mismatch model for generalized audio deepfake\ndetection,\u201d arXiv preprint arXiv:2407.18517, 2024.\n[186] Hu Hu, Chao-Han Huck Yang, Xianjun Xia, Xue Bai, Xin Tang,\nYajian Wang, Shutong Niu, Li Chai, Juanjuan Li, Hongning Zhu,\net al.,\n\u201cDevice-robust acoustic scene classification based on two-\nstage categorization and data augmentation,\u201d in Proc. DCASE, 2020.\n[187] Nhat Truong Pham et al.,\n\u201cHybrid data augmentation and deep\nattention-based dilated convolutional-recurrent neural networks for\nspeech emotion recognition,\u201d Expert Systems with Applications, vol.\n230, pp. 120608, 2023.\n[188] Ashish Alex, Lin Wang, Paolo Gastaldo, and Andrea Cavallaro, \u201cData\naugmentation for speech separation,\u201d Speech Communication, vol.\n152, pp. 102949, 2023.\n[189] Y. Tokozume, Y. Ushiku, and T. Harada, \u201cLearning from between-\nclass examples for deep sound recognition,\u201d in ICLR, 2018.\n[190] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret\nZoph, Ekin D Cubuk, and Quoc V Le, \u201cSpecaugment: A simple data\naugmentation method for automatic speech recognition,\u201d\nin Proc.\nINTERSPEECH, 2019, pp. 2613\u20132617.\n[191] Zhizheng Wu, Junichi Yamagishi, Tomi Kinnunen, Cemal Hanilc\u00b8i,\nMohammed Sahidullah, Aleksandr Sizov, Nicholas Evans, Massimil-\niano Todisco, and Hector Delgado, \u201cAsvspoof: the automatic speaker\nverification spoofing and countermeasures challenge,\u201d IEEE Journal\nof Selected Topics in Signal Processing, vol. 11, no. 4, pp. 588\u2013604,\n2017.\n[192] Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia,\nQiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen,\nYatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and\nMichael Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep-\nresentation Learning at Scale,\u201d in Proc. INTERSPEECH, 2022, pp.\n2278\u20132282.\n[193] Mirco Ravanelli and Yoshua Bengio,\n\u201cSpeaker recognition from\nraw waveform with sincnet,\u201d in IEEE spoken language technology\nworkshop, 2018, pp. 1021\u20131028.\n[194] Neil Zeghidour, Olivier Teboul, F\u00b4elix de Chaumont Quitry, and\nMarco Tagliasacchi, \u201cLEAF: A learnable frontend for audio clas-\nsification,\u201d in Proc. ICLR, 2021.\n[195] McFee, Brian, R. Colin, L. Dawen, Daniel P., M. Matt, B. Eric, and\nN. Oriol, \u201clibrosa: Audio and music signal analysis in python,\u201d in\nProc. Python in Science Conference, 2015, pp. 18\u201325.\n[196] Lam Pham, Dat Ngo, Dusan Salovic, Anahid Jalali, Alexan-\nder Schindler, Phu X. Nguyen, Khoa Tran, and Hai Canh Vu,\n\u201cLightweight deep neural networks for acoustic scene classification\nand an effective visualization for presenting sound scene contexts,\u201d\nApplied Acoustics, vol. 211, pp. 109489, 2023.\n[197] Jingze Lu, Yuxiang Zhang, Wenchao Wang, Zengqiang Shang, and\nPengyuan Zhang,\n\u201cOne-class knowledge distillation for spoofing\nspeech detection,\u201d in Proc. ICASSP, 2024, pp. 11251\u201311255.\n[198] Piotr Kawa, Marcin Plata, and Piotr Syga, \u201cAttack Agnostic Dataset:\nTowards Generalization and Stabilization of Audio DeepFake Detec-\ntion,\u201d in Proc. INTERSPEECH, 2022, pp. 4023\u20134027.\n[199] Xin Wang and Junichi Yamagishi, \u201cSpoofed training data for speech\nspoofing countermeasure can be efficiently created using neural\nvocoders,\u201d in Proc. ICASSP, 2023, pp. 1\u20135.\n[200] Akash Chintha et al., \u201cRecurrent convolutional structures for audio\nspoof and video deepfake detection,\u201d IEEE Journal of Selected Topics\nin Signal Processing, vol. 14, no. 5, pp. 1024\u20131037, 2020.\n[201] Jia Deng et al.,\n\u201cImagenet: A large-scale hierarchical image\ndatabase,\u201d in Proc. CVPR, 2009, pp. 248\u2013255.\n[202] Barrault Lo\u00a8\u0131c et al., \u201cSeamless: Multilingual expressive and stream-\ning speech translation,\u201d arXiv preprint arXiv:2312.05187, 2023.\n[203] Mirco Ravanelli et al.,\n\u201cSpeechBrain: A general-purpose speech\ntoolkit,\u201d 2021, arXiv:2106.04624.\n[204] Alexis Plaquet and Herv\u00b4e Bredin, \u201cPowerset multi-class cross entropy\nloss for neural speaker diarization,\u201d in Proc. INTERSPEECH, 2023,\npp. 3222\u20133226.\n[205] Herv\u00b4e Bredin,\n\u201cpyannote.audio 2.1 speaker diarization pipeline:\nprinciple, benchmark, and recipe,\u201d in Proc. INTERSPEECH, 2023,\npp. 1983\u20131987.\n[206] Nicolas\nM.\nM\u00a8uller,\nPhilip\nSperl,\nand\nKonstantin\nB\u00a8ottinger,\n\u201cComplex-valued neural networks for voice anti-spoofing,\u201d in Proc.\nINTERSPEECH, 2023, pp. 3814\u20133818.\n[207] Wanying Ge, Jose Patino, Massimiliano Todisco, and Nicholas Evans,\n\u201cExplaining deep learning models for spoofing and deepfake detec-\ntion with shapley additive explanations,\u201d in Proc. ICASSP, 2022, pp.\n6387\u20136391.\n[208] Davide Salvi, Paolo Bestagini, and Stefano Tubaro,\n\u201cTowards\nfrequency band explainability in synthetic speech detection,\u201d in Proc.\nEUSIPCO, 2023, pp. 620\u2013624.\n[209] Ning Yu, Long Chen, Tao Leng, Zigang Chen, and Xiaoyin Yi, \u201cAn\nexplainable deepfake of speech detection method with spectrograms\nand waveforms,\u201d Journal of Information Security and Applications,\nvol. 81, pp. 103720, 2024.\n[210] Suk-Young Lim, Dong-Kyu Chae, and Sang-Chul Lee, \u201cDetecting\ndeepfake voice using explainable deep learning techniques,\u201d Applied\nSciences, vol. 12, no. 8, pp. 3926, 2022.\n[211] Scott M. Lundberg and Su-In Lee, \u201cA unified approach to interpreting\nmodel predictions,\u201d\nin Proc. International Conference on Neural\nInformation Processing Systems, 2017, p. 4768\u20134777.\n[212] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin,\n\u201c\u201dwhy\nshould i trust you?\u201d: Explaining the predictions of any classifier,\u201d in\nProceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, p. 1135\u20131144.\n[213] Karen Simonyan and Andrew Zisserman, \u201cVery deep convolutional\nnetworks for large-scale image recognition,\u201d in Proc. ICLR, 2015.\n[214] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Vi\u00b4egas, and\nMartin Wattenberg, \u201cSmoothgrad: removing noise by adding noise,\u201d\nCoRR, vol. abs/1706.03825, 2017.\n[215] Reza Amini Gougeh, Zhang Nu, and Zeljko Zilic,\n\u201cOptimizing\nAuditory Immersion Safety on Edge Devices: An On-Device Sound\nEvent Detection System,\u201d\nin Proc. The Speaker and Language\nRecognition Workshop, 2024, pp. 225\u2013231.\n[216] Jordan J Bird and Ahmad Lotfi,\n\u201cReal-time detection of ai-\ngenerated speech for deepfake voice conversion,\u201d\narXiv preprint\narXiv:2308.12734, 2023.\n[217] Jonat John Mathew, Rakin Ahsan, Sae Furukawa, Jagdish Gau-\ntham Krishna Kumar, Huzaifa Pallan, Agamjeet Singh Padda, Sara\nAdamski, Madhu Reddiboina, and Arjun Pankajakshan, \u201cTowards\nthe development of a real-time deepfake audio detection system in\ncommunication platforms,\u201d arXiv preprint arXiv:2403.11778, 2024.\n[218] Shuwei Hou, Yan Ju, Chengzhe Sun, Shan Jia, Lipeng Ke, Riky Zhou,\nAnita Nikolich, and Siwei Lyu, \u201cDeepfake-o-meter v2. 0: An open\nplatform for deepfake detection,\u201d arXiv preprint arXiv:2404.13146,\n2024.\n",
    "ref": [
        "2111.14203",
        "2308.14970",
        "2404.13914",
        "2304.06632",
        "2304.10778",
        "2106.15561",
        "2006.07397",
        "2408.16132",
        "2311.15308",
        "1711.00354",
        "2002.10137",
        "1904.02892",
        "2006.04558",
        "2408.09300",
        "2407.18517",
        "2312.05187",
        "2106.04624",
        "1706.03825",
        "2308.12734",
        "2403.11778",
        "2404.13146"
    ]
}