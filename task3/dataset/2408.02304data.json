{
    "id": "2408.02304",
    "title": "Embedding Compression in Recommender Systems: A Survey",
    "abstract": "To alleviate the problem of information explosion, recommender systems are\nwidely deployed to provide personalized information filtering services.\nUsually, embedding tables are employed in recommender systems to transform\nhigh-dimensional sparse one-hot vectors into dense real-valued embeddings.\nHowever, the embedding tables are huge and account for most of the parameters\nin industrial-scale recommender systems. In order to reduce memory costs and\nimprove efficiency, various approaches are proposed to compress the embedding\ntables. In this survey, we provide a comprehensive review of embedding\ncompression approaches in recommender systems. We first introduce deep learning\nrecommendation models and the basic concept of embedding compression in\nrecommender systems. Subsequently, we systematically organize existing\napproaches into three categories, namely low-precision, mixed-dimension, and\nweight-sharing, respectively. Lastly, we summarize the survey with some general\nsuggestions and provide future prospects for this field.",
    "date": "2024-08-05T08:30:16+00:00",
    "fulltext": "130\nEmbedding Compression in Recommender Systems: A Survey\nSHIWEI LI\u2217, Huazhong University of Science and Technology, China\nHUIFENG GUO\u2217, Huawei Noah\u2019s Ark Lab, China\nXING TANG\u2020, Tencent, China\nRUIMING TANG and LU HOU, Huawei Noah\u2019s Ark Lab, China\nRUIXUAN LI\u2021, Huazhong University of Science and Technology, China\nRUI ZHANG\u2021, ruizhang.info, China\nTo alleviate the problem of information explosion, recommender systems are widely deployed to provide\npersonalized information filtering services. Usually, embedding tables are employed in recommender systems\nto transform high-dimensional sparse one-hot vectors into dense real-valued embeddings. However, the\nembedding tables are huge and account for most of the parameters in industrial-scale recommender systems.\nIn order to reduce memory costs and improve efficiency, various approaches are proposed to compress the\nembedding tables. In this survey, we provide a comprehensive review of embedding compression approaches\nin recommender systems. We first introduce deep learning recommendation models and the basic concept\nof embedding compression in recommender systems. Subsequently, we systematically organize existing\napproaches into three categories, namely low-precision, mixed-dimension, and weight-sharing, respectively.\nLastly, we summarize the survey with some general suggestions and provide future prospects for this field.\nCCS Concepts: \u2022 Information systems \u2192Recommender systems.\nAdditional Key Words and Phrases: recommender systems; embedding tables; model compression; survey\nACM Reference Format:\nShiwei Li, Huifeng Guo, Xing Tang, Ruiming Tang, Lu Hou, Ruixuan Li, and Rui Zhang. 2024. Embedding\nCompression in Recommender Systems: A Survey. ACM Comput. Surv. 56, 5, Article 130 (January 2024),\n21 pages. https://doi.org/10.1145/3637841\n1\nINTRODUCTION\nTo alleviate the problem of information explosion, recommender systems [55, 61] are extensively\ndeployed to provide personalized information filtering services, including online shopping [80],\nadvertising systems [46] and so on. Meanwhile, deep learning techniques have shown impressive\ncapabilities in capturing user preferences for candidate items. Thereupon, both the industry and\nresearch communities have proposed a variety of deep learning recommendation models (DLRMs)\n\u2217Shiwei Li and Huifeng Guo contributed equally to this research.\n\u2020This work was done when Xing Tang worked at Huawei Noah\u2019s Ark Lab.\n\u2021Ruixuan Li and Rui Zhang are the corresponding authors.\nAuthors\u2019 addresses: Shiwei Li, lishiwei@hust.edu.cn, Huazhong University of Science and Technology, Wuhan, China,\n430074; Huifeng Guo, huifeng.guo@huawei.com, Huawei Noah\u2019s Ark Lab, Shenzhen, China, 518129; Xing Tang, xing.tang@\nhotmail.com, Tencent, Shenzhen, China, 518054; Ruiming Tang, tangruiming@huawei.com; Lu Hou, houlu3@huawei.com,\nHuawei Noah\u2019s Ark Lab, Shenzhen, China, 518129; Ruixuan Li, rxli@hust.edu.cn, Huazhong University of Science and\nTechnology, Wuhan, China, 430074; Rui Zhang, rayteam@yeah.net, ruizhang.info, China.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2024 Association for Computing Machinery.\n0360-0300/2024/1-ART130 $15.00\nhttps://doi.org/10.1145/3637841\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\narXiv:2408.02304v1  [cs.IR]  5 Aug 2024\n130:2\nShiwei Li and Huifeng Guo, et al.\nto enhance the performance of recommender systems, such as Wide & Deep [6] in Google Play,\nDIN [80] in Alibaba and DeepFM [18] in Huawei.\n1.1\nDeep Learning Recommendation Models\nRecommender systems are utilized for a diverse range of tasks, such as candidate item matching [30],\nclick-through rate (CTR) prediction [51, 52], and conversion rate (CVR) prediction [44]. For each of\nthese tasks, the employed DLRMs have undergone meticulous design processes to ensure optimal\nperformance. However, without loss of generality, most DLRMs follow the embedding table and\nneural network paradigm [18, 45, 57, 58], despite the specific design of the neural network component\nmay vary across different model architectures.\nFig. 1. The embedding table and neural network paradigm of deep learning recommendation models (DLRMs).\nNote that the neural network component may vary in different model architectures, here we only present the\nneural network with the classic dual tower architecture as an example.\nAs illustrated in Figure 1, the embedding table is responsible for converting input rows into\ndense embedding vectors. It is worth noting that the input rows of DLRMs typically consist of\ncategorical features, which are encoded as high-dimensional one-hot vectors. Each category feature\nwill be referred to as feature for short, and all features under the same category form a feature\nfield. Generally, each feature is associated with a unique embedding stored in the embedding table\nE \u2208R\ud835\udc5b\u00d7\ud835\udc51, where \ud835\udc5bdenotes the total number of features and \ud835\udc51denotes the embedding dimension.\nOn the other hand, the neural network is primarily engaged in interacting, processing, and\nanalyzing feature embeddings, along with making predictions. Recent studies [18, 29, 36, 40, 65, 70]\nhave consistently focused on optimizing the feature extraction capabilities of the neural networks.\nFor example, [18, 58] utilize product operators to model the feature interactions between different\nfeature fields. [36, 40] employ convolutions on embeddings to capture feature interactions of\narbitrary order. [65] introduces an additional attention network to assign varying importance to\ndifferent feature interactions. Additionally, [29, 70] automatically search for suitable interaction\nfunctions using AutoML techniques [21]. In this manuscript, we do not delve into the detailed\ndesign of the neural network component. Instead, we recommend referring to the works [61, 74, 75]\nfor a comprehensive understanding of the neural networks used in DLRMs.\nDespite incorporating various intricate designs, the neural network usually entails relatively\nshallow layers and a limited number of model parameters. In contrast, the embedding table occupies\nthe vast majority of model parameters. Especially in industrial-scale recommender systems, where\nthere are billions or even trillions of categorical features, the embedding table may take hundreds of\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:3\nGB or even TB to hold [17]. For example, the size of embedding tables in Baidu\u2019s advertising systems\nreaches 10 TB [66]. As the scale of recommender systems perpetually expands, the continuous\ngrowth in the number of features will bring greater storage overhead.\n1.2\nEmbedding Compression in Recommender Systems\nIn addition to increasing storage overhead, larger embedding tables will also result in higher latency\nduring table lookup 1, which will reduce the efficiency of model training and inference. Therefore,\nto deploy the DLRMs with large embedding tables in real production environment efficiently and\neconomically, it is necessary to compress their embedding tables.\nHowever, embedding compression in DLRMs differs significantly from model compression in\nother fields, such as Computer Vision (CV) [8] and Natural Language Processing (NLP) [19]. These\ndifferences primarily manifest in three aspects: model architectures, properties of input data, and\nmodel size. Firstly, vision models and language models are usually very deep neural networks\nstacked by fully-connected layers, convolutional layers, or transformers. Consequently, compression\nmethods designed for these models focus on compressing the aforementioned modules rather than\nembedding tables. In contrast, DLRMs are typically shallow models, with the majority of parameters\nconcentrated in the embedding tables. Secondly, the input data of vision models and language models\nare usually images and texts, inherently containing abundant visual and semantic information that\ncan be leveraged for model compression. For example, [4] use the semantic information as a prior\nknowledge to compress the word embeddings, while [20] exploit the similarity between feature\nmaps derived from image inputs to compress convolution kernels. However, in recommender\nsystems, there is generally limited visual or semantic information available. Fortunately, DLRMs\npossess unique properties in the input data that can facilitate embedding compression. Specifically,\ncategorical features are organized in feature fields and often follow a highly skewed long-tail\ndistribution, with varying numbers of features in different fields. We can compress embedding\ntables based on feature frequency and field size. Thirdly, embedding tables of DLRMs are usually\nhundreds or even thousands of times larger than vision models or language models [53], which\npresents a more challenging and necessary task for compression.\nRecently, embedding compression has gained increasing attention in recommender systems,\nleading to the development and application of various embedding compression techniques for\nDLRMs. However, there is currently no comprehensive survey summarizing the methods em-\nployed for embedding compression. Therefore, the primary objective of this paper is to review\nand summarize representative research in this field. The embedding table can be regarded as a\nmatrix with three dimensions, that are the precision of weights, the dimension of embeddings,\nand the number of embeddings. To this end, we summary the embedding compression methods\ninto three categories according to the dimensions they compress, as illustrated in Figure 2. Firstly,\nlow-precision methods reduce the memory of each weight by decreasing its bit width. According\nto the size of bit width and its corresponding advantages, we further divide the low-precision meth-\nods into binarization and quantization. Secondly, mixed-dimension methods reduce the memory\nof specific embeddings by decreasing their dimensions and using mixed-dimension embeddings.\nAccording to the techniques of determining the embedding dimension for different features, we\ncategorize the mixed-dimension methods into rule-based approaches, NAS-based approaches, and\npruning. Thirdly, weight-sharing methods reduce the actual parameters of the embedding table by\nsharing weights among different embeddings. Considering that the number of features is given by\nthe dataset, a solution to reduce the number of embeddings is to reuse embeddings among features.\nFurthermore, we generalize the sharing to the weight level and define the weight-sharing methods\n1The process of retrieving an embedding from the embedding table based on the input feature or index.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:4\nShiwei Li and Huifeng Guo, et al.\nEmbedding\nCompression\nLow-precision\nMixed-dimension\nWeight-sharing\nBinarization:\n[42], [77], DCF [73], DCMF [33], DPR [76], DFM [37]\nCIGAR [28], HashGNN [54], L2Q-GCN [5]\nQuantization:\n[16], [69], [66], ALPT [31]\nRule-based Approaches:\nMDE [15], CpRec [53]\nNAS-based Approaches:\nNIS [25], ESAPN [39], AutoIAS [59],\nAutoEmb [78], AutoDim [79], RULE [3], OptEmbed [43]\nPruning:\nDeepLight [10], DNIS [7], AMTL [68], PEP [41], SSEDS [48]\nHashing:\nQR [50], MEmCom [47], BCH [67], FDH [72], LMA [12], ROBE [11]\nVector Quantization:\nSaec [62], MGQE [26], xLightFM [24], LightRec [34], LISA [63]\nDecomposition:\nMLET [14], ANT [35], DHE [27], TT-Rec[71], LLRec [56], [64]\nFig. 2. Summary of representative studies on embedding compression in recommender systems.\nas generating embeddings with shared weights. According to the way embeddings are generated,\nwe categorize the mixed-dimension methods into hashing, vector quantization, and decomposition.\nWe will introduce the three primary categories in Sections 2, 3 and 4, respectively.\nNote that embeddings are fed into the neural network as representations of categorical features\nand form the foundations of DLRMs. Therefore, when compressing embeddings, it may affect\nthe model performance on many aspects, including model accuracy, inference efficiency, training\nefficiency, and training memory usage. We will discuss the pros and cons of different methods\nregarding these metrics at the end of each section. In Section 5, the survey is summarized, providing\ngeneral suggestions for different scenarios and discussing future prospects for this field.\n2\nLOW-PRECISION\nAs we all know, embedding weights are typically stored in the format of FP32 2 which occupies 32\nbits. To reduce the storage of each weight, low-precision approaches are developed to represent a\nweight with fewer bits. In particular, according to the bit width of weights, low-precision approaches\ncan be further divided into binarization and quantization.\n2.1\nBinarization\nBinarization is to compress a full-precision weight into a binary code that only occupy 1 bit. It\nis widely used in the embedding-based similarity search of recommender systems [28, 54], since\nthe binary embeddings have two distinct advantages compared to the full-precision ones: (1) less\nmemory or disk cost for storing embeddings; (2) higher inference efficiency as the similarity (i.e.,\ninner product) between binary embeddings can be calculated more efficiently through the Hamming\ndistance, which has been proved in [73].\n2The short of single-precision floating-point format.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:5\n[42, 77] pioneered to obtain binary embeddings in a two-stage (i.e, post-training) manner. Specif-\nically, they first learn a full-precision embedding table while ignoring the binary constraints, and\nthen perform binarization (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc65)) on the full-precision embeddings to get binary embeddings.\nHowever, the binarization procedure is not in the training process and thus cannot be optimized\nby minimizing the training objective, which will bring large irreparable errors and fail to meet an\nacceptable accuracy. To reduce accuracy degradation, subsequent works have focused on end-to-end\napproaches to learn the binary embeddings during training.\nAs shown in Figure 3, recent works typically learn binary embeddings following two optimization\nparadigms, namely direct optimization and indirect optimization. As Figure 3(a) shows, in the\ndirect optimization of binarization, the binary embeddings are maintained as part of the model\nparameters and will be optimized directly by the training loss. For example, to improve the efficiency\nof Collaborative Filtering (CF), DCF [73] learns a binary embedding table B \u2208{\u00b11}\ud835\udc5b\u00d7\ud835\udc51. To maximize\nthe information encoded in each binary embedding, DCF further adds a balance-uncorrelation\nconstraint to B (i.e., B\ud835\udc471 = 0, B\ud835\udc47B = \ud835\udc5bI), where I is an identity matrix. However, it is NP-hard to\noptimize the binary embeddings with such constraint. To resolve this problem, DCF also maintains\na full-precision embedding table E \u2208R\ud835\udc5b\u00d7\ud835\udc51with the same balance-uncorrelation constraint. The\nconstraint of B is then replaced by adding the mean-square-error (MSE) of (B \u2212E) to the objective\nfunction. During training, DCF will update B and E alternatively through different optimization\nalgorithms. Specifically, B is updated by Discrete Coordinate Descent (DCD) and E is updated with\nthe aid of Singular Value Decomposition (SVD). This optimization paradigm has been widely used\nto learn binary embeddings in recommender systems, such as DPR [76], DCMF [33], DFM [37].\nDPR changes the objective function of DCF (i.e., rating prediction) to personalized items ranking.\nDCMF and DFM extends this binarization paradigm to Content-aware Collaborative Filtering [32]\nand Factorization Machine (FM) [49], respectively.\nFig. 3. End-to-end optimization paradigms of learning binary embeddings. \ud835\udc35(yellow) and \ud835\udc38(green) represent\nthe binary and full-precision weights, respectively. \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60(red) is the training loss where \u03a8(\u00b7) is the objective\nfunction and MSE(\u00b7) is the mean-square-error function.\nAs shown in Figure 3(b), another paradigm is the indirect optimization, where the binary em-\nbeddings B are generated from full-precision embeddings E on the fly and will be optimized\nindirectly by optimizing E. However, it is infeasible to optimize E by the standard gradient descent\nas the gradients of the binary operations (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc65)) are constantly zero. To solve this problem,\nCIGAR [28] replaces \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc65) with the scaled tanh function \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udefc\ud835\udc65) as lim\ud835\udefc\u2192\u221e\ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udefc\ud835\udc65) = \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc65)\nand \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udefc\ud835\udc65) has better differential property. In the early stages of training, a smaller value of\n\ud835\udefcis utilized to yield superior representations, and as the training progresses, its value gradually\nincreases to approximate \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(). Another way to solve the non-propagable gradient is straight-\nthrough-estimator (STE) [9], which treats some operations as identity maps during backpropagation.\nHashGNN [54] employs the STE variant of \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b() and thus updating E with the gradients of B.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:6\nShiwei Li and Huifeng Guo, et al.\nHowever, the huge gap between B and E will cause an imprecise update for E. To solve this issue,\nHashGNN further develops a dropout-based binarization. Specifically, \u02c6\nE = (1 \u2212P) \u2299E + P \u2299B\nwill be fed into the following networks, where P \u2208{0, 1}\ud835\udc5b\u00d7\ud835\udc51and \u2299is the element-wise product.\nEach element in P is a Bernoulli random value with probability \ud835\udc5d. During backpropagation, only\nthe embeddings that are binarized will be updated through STE and the rest will be updated by\nstandard gradient descent. To ensure convergence, HashGNN adopts a small value for \ud835\udc5din the initial\ntraining phase, gradually increasing it as the training progresses. Similarly, L2Q-GCN [5] uses STE\nto optimize the full-precision embeddings, while introducing a positive scaling factor \ud835\udc60= \ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b(|\ud835\udc86|)\nfor each binary embedding to enhance its presentation capability, where \ud835\udc86is the full-precision\nembedding. The comparison of the above three methods is summarized in Algorithm 1.\nAlgorithm 1: Comparison between CIGAR [28], HashGNN [54] and L2Q-GCN [5].\nInput: a full-precision embedding \ud835\udc86.\nOutput: the output embedding \u02c6\n\ud835\udc86.\n// \u02c6\n\ud835\udc86will be fed into following networks.\nFunc CIGAR(\ud835\udc86):\n\u02c6\n\ud835\udc86= \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udefc\u00b7 \ud835\udc86)\n// \ud835\udefcwill increase as training progresses.\nFunc HashGNN(\ud835\udc86):\n\ud835\udc83= \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b_\ud835\udc60\ud835\udc61\ud835\udc52(\ud835\udc86)\n// \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b_\ud835\udc60\ud835\udc61\ud835\udc52() is the STE variant of \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b().\n\ud835\udc91:= {0, 1}\ud835\udc51\n// sample from Bernoulli distribution with probability \ud835\udc5d.\n\u02c6\n\ud835\udc86= (1 \u2212\ud835\udc91) \u2299\ud835\udc86+ \ud835\udc91\u2299\ud835\udc83\n// \ud835\udc5dwill increase as training progresses.\nFunc L2Q-GCN(\ud835\udc86):\n\ud835\udc83= \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b_\ud835\udc60\ud835\udc61\ud835\udc52(\ud835\udc86)\n\u02c6\n\ud835\udc86= \ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b(|\ud835\udc86|) \u00b7 \ud835\udc83\n2.2\nQuantization\nAlthough binarization has better efficiency and less memory cost at the inference stage, it may\nlead to a significant drop of accuracy, which is not acceptable in several scenarios. As Cheng et\nal. [6] claim, even 0.1% decrease of the prediction accuracy may result in large decline in revenue.\nTo trade off the memory cost and the prediction accuracy, quantization is used to represent each\nweight with a multi-bit integer.\nQuantization is the mapping of a 32-bit full-precision weight to an element in the set of quantized\nvalues S = {\ud835\udc5e0,\ud835\udc5e1, ...,\ud835\udc5e\ud835\udc58}, where \ud835\udc58= 2\ud835\udc60\u22121 and \ud835\udc60is the bit width. The most commonly used\nquantization function is uniform quantization, where the quantized values are uniformly distributed.\nSpecifically, the step size \u0394 = \ud835\udc5e\ud835\udc56\u2212\ud835\udc5e\ud835\udc56\u22121 remains the same for any \ud835\udc56\u2208[1,\ud835\udc58]. Let \ud835\udc64be a value clipped\ninto the range [\ud835\udc5e0,\ud835\udc5e\ud835\udc58], we can quantize it into an integer as \u02c6\n\ud835\udc64= \ud835\udc5f\ud835\udc51((\ud835\udc64\u2212\ud835\udc5e0)/\u25b3), where \ud835\udc5f\ud835\udc51(\ud835\udc65)\nrounds \ud835\udc65to an adjacent integer. The integer \u02c6\n\ud835\udc64will be de-quantized into a floating-point value\n( \u02c6\n\ud835\udc64\u00d7 \u25b3+ \ud835\udc5e0) when used. Existing work on embedding quantization either performs post-training\nquantization or trains a quantized embedding table from scratch.\nGuan et al. [16] studies post-training quantization (PTQ) on the embedding tables and proposes\na uniform and a non-uniform quantization algorithm. Specifically, in the uniform quantization,\nthey maintain a quantization range for each embedding and find the best quantization range by a\ngreedy search algorithm. In the non-uniform quantization, they divide similar embeddings into\ngroups and apply k-means clustering on the weights to produce a codebook for each group. The\nweights in each group will be mapped to the index of a value in the corresponding codebook. These\ntwo algorithms improve the accuracy of PTQ, however, they still suffer from accuracy degradation.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:7\nFig. 4. Training frameworks of quantization. \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60(red) is the training loss. \ud835\udc40(purple) and \ud835\udc38(green) represent\nthe integer and full-precision weights, respectively.\nTo further reduce accuracy degradation, recent works [66, 69] learn quantized weights from\nscratch. Unlike the well-known quantization-aware training (QAT) [2, 13], [66, 69] use another\nquantization training framework to exploit the sparsity of the input data, which we term low-\nprecision training (LPT). As Figure 4(a) shows, QAT quantizes the full-precision weights in the\nforward pass and updates the full-precision weights with the gradients estimated by STE. As\nFigure 4(b) shows, in LPT, the weights are stored in the format of integers at training, thereby\ncompressing the training memory. The model takes the de-quantized weights as input and will\nquantize the weights back into integers after the backward propagation. Since the input one-hot\nvectors of DLRMs are highly sparse, only extremely small part of the embeddings will be de-\nquantized into floating-point values, whose memory is negligible. Xu et al. [66] uses 16-bit LPT on\nthe embedding table without sacrificing accuracy. To enhance the compression capability of LPT,\nYang et al. [69] proposes a mixed-precision scheme where most embeddings are stored in the format\nof integers, only the most recently or frequently used embeddings are stored in a full-precision\ncache. With a small cache, they achieve lossless compression with 8-bit or even 4-bit quantization.\nLi et al. [31] proposes an adaptive low-precision training scheme to learn the quantization step size\nfor better model accuracy.\n2.3\nDiscussion\nLow-precision is a simple yet effective way for embedding compression. At the inference stage,\nbinarization can reduce the memory usage by 32\u00d7 and accelerate the inference through Hamming\ndistance. However, the binary embeddings usually cause severe accuracy degradation and need to\nbe trained with the guidance of full-precision embeddings, which requires more memory usage and\ncomputing resources at training. In contrast, quantization has a limited compression capability but\ncan achieve a comparable accuracy as the full-precision embeddings. Besides, recent quantization\napproaches for embedding tables can also compress the memory usage at the training stage and\nimprove the training efficiency by reducing the communication traffic.\n3\nMIXED-DIMENSION\nEmbedding tables usually assign a uniform dimension to all the embeddings in a heuristic way, and\nit turns out to be suboptimal in both prediction accuracy and memory usage [25]. As confirmed\nin [78], a low dimensional embedding is good at handling less frequent features where a high\ndimensional embedding cannot be well trained. Therefore, to boost the model performance, it is\nimportant to assign a appropriate dimension to each feature and use mixed-dimension embeddings.\nExisting methods can obtain mixed-dimension embeddings in a structured or an unstructured\nmanner. As shown in Figure 5, structured approaches divide the embeddings into groups each of\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:8\nShiwei Li and Huifeng Guo, et al.\nFig. 5. Structures of mixed-dimension embeddings.\nwhich has a unique dimension, while unstructured approaches learn a sparse embedding table\nwhere the embeddings have various dimensions. However these mixed-dimension embeddings are\nnot friendly for the operations (e.g., inner product) which require the embeddings of the same length.\nTherefore, the mixed-dimension embeddings need to be transformed into a uniform dimension\nbefore feeding into the following networks. Such transformation is usually achieved by linear\nprojection or simply zero padding. Apart from the difference of the embedding structures, existing\nmethods also differ greatly in the way of generating mixed-dimension embeddings. In this section,\nwe will introduce three kinds of mixed-dimension approaches named rule-based approaches,\nNAS-based approaches and pruning, respectively.\n3.1\nRule-based Approaches\nIt is a common understanding that the features with higher frequencies are more informative and\nthe fields with more features occupy more memory. Thus, the embedding dimension can be set with\na heuristic rule based on the feature frequency and the field size. To deploy the item embeddings\ninto resource-constraint devices, CpRec [53] divides the items into several groups by frequency\nand assigns a predefined dimension to each group according to the frequencies of owned features.\nSimilarly, MDE [15] assigns each feature field with a unique dimension according to the number\nof features included in this field. Specifically, let \ud835\udc8f\u2208R\ud835\udc5adenotes the number of features in all\n\ud835\udc5afeature fields and \ud835\udc91= 1/\ud835\udc8f, then the embedding dimension of \ud835\udc56-th field would be \u00af\n\ud835\udc51\ud835\udc91\ud835\udc8a\ud835\udefc/||\ud835\udc91||\ud835\udefc\n\u221e,\nwhere \u00af\n\ud835\udc51is the base dimension and \ud835\udefc\u2208[0, 1] denotes the temperature. These rule-based approaches\nare simple yet effective in reducing the memory usage and alleviating the overfitting problems,\nhowever, they suffer from suboptimal performance as the heuristic rules can not be optimized by\nthe ultimate goal of minimizing the training objective.\n3.2\nNAS-based Approaches\nNAS was originally proposed to search for the optimal neural network architectures [82]. Recently,\nit has also been adopted in searching embedding dimensions for different features. Unlike the\nrule-based approaches where the dimension is set based on a priori, it is now learned. Generally,\nthere are three components in the NAS-based approaches: (1) search space: relaxing the large\noptimization space of embedding dimensions with heuristic assumptions; (2) controller: usually a\nneural network or learnable parameters, selecting candidate dimension from the search space in a\nhard or soft manner; (3) updating algorithm: updating the controller with reinforcement learning\n(RL) algorithms or differential architecture search (DARTS) [38] techniques and so on.\nWe first introduce the approaches of NIS [25], ESAPN [39] and AutoIAS [59], which adopt a\npolicy network as the controller and update the controller with RL algorithms. NIS [25] and ESAPN\n[39] are designed to learn embedding dimensions of users and items. In NIS, the authors relax the\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:9\noriginal search space \ud835\udc5b\u00d7 \ud835\udc51to \ud835\udc3a\u00d7 \ud835\udc35(\ud835\udc3a< \ud835\udc5band \ud835\udc35< \ud835\udc51) by dividing the features into \ud835\udc3agroups and\ncutting the embedding dimension of each group into \ud835\udc35blocks. Then, they use the controller to\nselect several blocks and generate the final embeddings. In ESAPN, the authors predefine the search\nspace as a set of candidate embedding dimensions D = {\ud835\udc511,\ud835\udc512, ...,\ud835\udc51\ud835\udc58}, where \ud835\udc511 < \ud835\udc512 < ... < \ud835\udc51\ud835\udc58.\nInspired by the fact that the frequencies of features are monotonically increasing in the data stream,\nthey decide to increase or keep the current embedding dimension instead of selecting one in D.\nThe decision is made by the controller based on the feature frequency and the current embedding\ndimension. Different from NIS and ESAPN, AutoIAS searches not only the embedding dimensions\nof feature fields but also the following neural network architectures. The authors design a search\nspace for each model component (e.g., the search space of embedding dimensions is similar as\nD in ESAPN.). To boost the training efficiency, they maintain a supernet at training and use the\ncontroller to generate sub-architectures by inheriting parameters from the supernet.\nThe RL-based approaches described above perform a hard selection by selecting only one\nembedding dimension for each feature or field at a time. Instead, inspired by DARTS, AutoEmb [78]\nand AutoDim [79] make a soft selection by weighted summing over the embeddings of the candidate\ndimensions in D = {\ud835\udc511,\ud835\udc512, ...,\ud835\udc51\ud835\udc58}. Let \ud835\udf4e\u2208[0, 1]\ud835\udc58denote the vector composed of the weighting\ncoefficients. AutoEmb searches for the embedding dimensions of individual features, while AutoDim\nsearches for the embedding dimensions of the feature fields. In AutoEmb, the controller is a neural\nnetwork and generates \ud835\udf4ebased on the feature frequency. While AutoDim directly assigns each\nfield with a learnable vector \ud835\udf4e, and it further approximates the hard selection by performing\ngumbel-softmax [22] on \ud835\udf4e. At training, the controller in AutoEmb and the learnable vectors in\nAutoDim are optimized through DARTS techniques. After training, the corresponding dimension\nof the largest weight in \ud835\udf4eis selected and the model will be retrained for a better accuracy.\nConsidering that the training process of the controller is quite time-consuming, recent works [3,\n43] search for the optimal embedding dimensions after training the models, without using any\ncontroller. They first sample some structures from the search space and then explore the entire search\nspace by using evolutionary search strategies on the sampled structures. Specifically, RULE [3]\ncuts the embedding table into \ud835\udc3a\u00d7 \ud835\udc35blocks similar as NIS and adds a diversity regularization\nto the blocks in the same group for maximizing expressiveness. After training, RULE selects the\nmost suitable embedding blocks under a memory budget (i.e., the maximum number of blocks).\nOptEmbed [43] trains a supernet while removing non-informative embeddings. After training, it\nthen assigns each field with a binary mask \ud835\udc8e\u2208{0, 1}\ud835\udc51to obtain mixed-dimension embeddings,\nwhere \ud835\udc51is the original dimension. The block selections in RULE and the masks in OptEmbed are\ndetermined and evolved by the search strategies.\n3.3\nPruning\nInstead of shortening the length of embeddings, pruning can obtain a sparse embedding table and\nthus get mixed-dimension embeddings. For instance, DeepLight [10] prunes the embedding table\nin a certain proportion. During training, it prunes and retrains the embedding table alternatively\nso that the mistakenly pruned weights can grow back. In addition, DeepLight will increase the\npruning proportion gradually as training proceeds.\nAnother way to prune the embeddings is to train the embeddings with learnable masks. Specifi-\ncally, an embedding \ud835\udc86is pruned as \u02c6\n\ud835\udc86= \ud835\udc8e\u2299\ud835\udc86for the forward pass, where \ud835\udc8eis the mask and \u2299is\nthe element-wise product. DNIS [7] divides features into groups by frequency and assigns each\ngroup with a learnable mask \ud835\udc8e\u2208[0, 1]\ud835\udc51. AMTL [68] develops a network to generate a binary\nmask \ud835\udc8e\u2208{0, 1}\ud835\udc51for each feature based on its frequency. Similarly, PEP [41] generates a binary\nmask \ud835\udc8e\u2208{0, 1}\ud835\udc51for each feature as \ud835\udc8e= I(|\ud835\udc86| > \ud835\udc54(\ud835\udc60)), where I(\u00b7) is the indicator function and\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:10\nShiwei Li and Huifeng Guo, et al.\n\ud835\udc54(\ud835\udc60) (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc60)) serves as a learnable threshold. Specially, in PEP, the embeddings should\nminus \ud835\udc54(\ud835\udc60) before being pruned by the masks (i.e. \u02c6\n\ud835\udc86= \ud835\udc8e\u2299(\ud835\udc86\u2212\ud835\udc54(\ud835\udc60))). At training, the network\nin AMTL and the learnable threshold in PEP are optimized together with the model parameters\nby gradient descent, while the learnable masks in DNIS are optimized by DARTS. After training,\nAMTL and PEP preserve \u02c6\n\ud835\udc86as the final embeddings, while DNIS need pruning \u02c6\n\ud835\udc86with a threshold\nas the redundant weights in \u02c6\n\ud835\udc86are not exact zero. The differences between the above methods in\ngenerating masks are summarized in Algorithm 2.\nAlgorithm 2: Comparison between DNIS [7], AMTL [68] and PEP [41].\nInput: the full-precision embedding \ud835\udc86and feature frequency \ud835\udc53.\nOutput: the pruned embedding \u02c6\n\ud835\udc86. // \u02c6\n\ud835\udc52will be fed into following networks.\nFunc DNIS(\ud835\udc86):\n\ud835\udc8e:= [0, 1]\ud835\udc51\n// \ud835\udc8eis a learnable mask.\n\u02c6\n\ud835\udc86= \ud835\udc8e\u2299\ud835\udc86\n// \ud835\udc8eis shared by features with similar frequency.\nFunc AMTL(\ud835\udc86):\n\ud835\udc8e:= \ud835\udc4e\ud835\udc5a\ud835\udc61\ud835\udc59(\ud835\udc53) // \ud835\udc8eis generated by a network \ud835\udc4e\ud835\udc5a\ud835\udc61\ud835\udc59() with the frequency \ud835\udc53.\n\u02c6\n\ud835\udc86= \ud835\udc8e\u2299\ud835\udc86\nFunc PEP(\ud835\udc86):\n\ud835\udc8e= I(|\ud835\udc86| > \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc60))\n// \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc60) serves as a learnable threshold.\n\u02c6\n\ud835\udc86= \ud835\udc8e\u2299(\ud835\udc86\u2212\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc60))\nTo get rid of the extra training process of optimizing the masks, SSEDS [48] develop a single-shot\npruning algorithm to prune on the pretrained models. For a pretrained model, SSEDS will prune the\ncolumns of the embedding matrix for each field and produce structured embeddings. After training,\nSSEDS assigns each feature field with a mask and form a mask matrix M = {1} \u02c6\n\ud835\udc5b\u00d7\ud835\udc51, where \u02c6\n\ud835\udc5bis the\nnumber of fields and \ud835\udc51is the original embedding dimension. Instead of learning M in the training\nprocess, SSEDS use \ud835\udc54\ud835\udc56\ud835\udc57= \ud835\udf15\ud835\udc53(M, E)/\ud835\udf15M\ud835\udc56\ud835\udc57to identify the importance of \ud835\udc57-th dimension in \ud835\udc56-th field,\nwhere \ud835\udc54\ud835\udc56\ud835\udc57is the gradient of the loss function with respect to M\ud835\udc56\ud835\udc57. Specifically, a larger magnitude of\n|\ud835\udc54\ud835\udc56\ud835\udc57| means that the corresponding dimension has a greater impact on the loss function. Note that\nall |\ud835\udc54\ud835\udc56\ud835\udc57| can be computed efficiently via only one forward-backward pass. Given a memory budget,\nSSEDS calculates a saliency score for each dimension as \ud835\udc60\ud835\udc56\ud835\udc57= |\ud835\udc54\ud835\udc56\ud835\udc57|/\u00cd \u02c6\n\ud835\udc5b\n\ud835\udc56=0\n\u00cd\ud835\udc51\n\ud835\udc57=0 |\ud835\udc54\ud835\udc56\ud835\udc57| and prunes the\ndimensions with the lowest saliency scores.\n3.4\nDiscussion\nMixed-dimension approaches can alleviate the overfitting problems and obtain better accuracy, but\nusually have worse efficiency at the training and the inference stage. At inference, the structured\napproaches usually suffer from extra computing cost due to the linear transformation and the\nunstructured approaches store the sparse embedding table using sparse matrix storage, which will\ncost extra effort to access. At training, NAS-based approaches require extremely long time for\nsearching and pruning usually needs to retrain the pruned models for better accuracy which doubles\nthe training time. In contrast, rule-based approaches have little influence on the efficiency and can\nsave memory also at the training stage. However, they cannot achieve the optimal accuracy.\n4\nWEIGHT-SHARING\nLow-precision approaches reduce the number of bits in a weight and mixed-dimension approaches\nreduce the number of weights in an embedding. Unlike them, weight-sharing approaches share\nweights among the embedding table, thereby reducing the actual number of parameters within it.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:11\nExisting weight-sharing approaches usually generate embeddings with several shared vectors.\nIn this section, we relax the definition of weight-sharing and formulate a weight-sharing par-\nadigm based on existing approaches. Specifically, the embedding generation is formulated as\n\ud835\udc86= \u00d0/\u00cd\ud835\udc60\n\ud835\udc56=1 \ud835\udc70\ud835\udc56\u00d7 T\ud835\udc56, where \u00d0/\u00cd denotes concatenation or summation, T\ud835\udc56is a matrix composed of\nshared vectors and \ud835\udc70\ud835\udc56is an index vector for selecting shared vectors in T\ud835\udc56. For ease of expression,\nwe refer to the shared vectors as meta-embeddings and the matrices of shared vectors as meta-\ntables. According to the principle of constructing the index vectors, we introduce three kinds of\nweight-sharing methods named hashing, vector quantization, and decomposition, respectively.\n4.1\nHashing\nHashing methods generate the index vectors by processing the original feature id with hash\nfunctions. For instance, the naive hashing method [60] compresses the embedding table with\na simple hash function (e.g., the reminder function). Specifically, given the original size of the\nembedding table \ud835\udc5b\u00d7 \ud835\udc51, each feature has an embedding \ud835\udc86= \ud835\udc70\u00d7 T, where T \u2208R\ud835\udc5a\u00d7\ud835\udc51(\ud835\udc5a< \ud835\udc5b) and\n\ud835\udc70= one-hot(id %\ud835\udc5a) \u2208{0, 1}\ud835\udc5a. Note that \ud835\udc70\u00d7 T is actually achieved by table look-up when \ud835\udc70is a\none-hot vector. However, [60] naively maps multiple features to the same embedding. The collisions\nbetween features will result in loss of information and drop of accuracy.\nAlgorithm 3: Comparison between QR [50], MEmCom [47], BCH [67] and FDH [72].\nInput: the feature id (id \u2264\ud835\udc5b).\nOutput: the generated embedding \u02c6\n\ud835\udc86.\n// \u02c6\n\ud835\udc52will be fed into following networks.\nFunc QR(id):\n\ud835\udc701 = one-hot(id %\ud835\udc5a), \ud835\udc702 = one-hot(id | \ud835\udc5a)\n// \ud835\udc5ais a predefined parameter.\n\u02c6\n\ud835\udc86= \ud835\udc701 \u00d7 T1 + \ud835\udc702 \u00d7 T2\n// T1 \u2208R\ud835\udc5a\u00d7\ud835\udc51and T2 \u2208R(\ud835\udc5b|\ud835\udc5a)\u00d7\ud835\udc51.\nFunc MEmCom(id):\n\ud835\udc701 = one-hot(id %\ud835\udc5a), \ud835\udc702 = one-hot(id)\n\u02c6\n\ud835\udc86= (\ud835\udc701 \u00d7 T1) \u00b7 (\ud835\udc702 \u00d7 T2)\n// T1 \u2208R\ud835\udc5a\u00d7\ud835\udc51and T2 \u2208R\ud835\udc5b\u00d71.\nFunc BCH(id):\ndivide the bits within id into \ud835\udc60sub-ids {id1, ..., id\ud835\udc60}.\n\u02c6\n\ud835\udc86= \u00cd\ud835\udc60\n\ud835\udc56=1 one-hot(id\ud835\udc56) \u00d7 T\n// T is a shared meta-table.\nFunc FDH(id):\nif feature id is frequent then\n\u02c6\n\ud835\udc86:= R\ud835\udc51\n// frequent features have unique embeddings.\nelse\n\u02c6\n\ud835\udc86= QR(id)\nTo reduce the collisions, existing hashing methods use multiple hash functions to process the\nfeature id. They usually maintain multiple meta-tables (T1, ..., T\ud835\udc60) and generate multiple index\nvectors as \ud835\udc70\ud835\udc56= one-hot(hash\ud835\udc56(id)), where \ud835\udc56\u2208[1,\ud835\udc60] and {hash\ud835\udc56}\ud835\udc60\n\ud835\udc56=1 is a group of hash functions.\nFor example, QR [50] maintain two meta-tables and use the quotient function and the reminder\nfunction to generate two index vectors. Similarly, MEmCom [47] also maintain two meta-tables\n(T1 \u2208R\ud835\udc5a\u00d7\ud835\udc51, T2 \u2208R\ud835\udc5b\u00d71) and generate two index vectors as \ud835\udc701 = one-hot(id %\ud835\udc5a), \ud835\udc702 = one-hot(id).\nTo better distinguish the features, MEmCom multiplies two meta-embeddings as the final embedding.\nFurther, Yan et al. [67] use Binary Code based Hash (BCH) functions to process the feature id at\nbit level. It divides the 64 bits of a feature id into \ud835\udc60groups and restructures them into \ud835\udc60sub-ids\n(id1, ..., id\ud835\udc60). Each sub-id corresponds to an index vector (i.e., \ud835\udc70\ud835\udc56= one-hot(id\ud835\udc56), \ud835\udc56\u2208[1,\ud835\udc60]) and\nobtains a meta-embedding. Additionally, to enhance the compression capability, BCH keeps one\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:12\nShiwei Li and Huifeng Guo, et al.\nsingle meta-table and shares it among all T\ud835\udc56,\ud835\udc56\u2208[1,\ud835\udc60]. Although the hashing methods described\nabove are efficient in memory reduction, they still suffer from accuracy degradation and extra\ncomputing cost of the hash functions. To alleviate these problems, Zhang et al. [72] develop a\nFrequency-based Double Hashing (FDH) method, which only uses hashing on the features with\nlow frequencies. In this way, fewer features need to be processed by the hash function. With a little\nextra storage for the most frequent features, FDH not only improves the prediction accuracy but\nalso the inference efficiency. The difference between the above methods in generating embeddings\nis reflected in Algorithm 3.\nInstead of generating embeddings with meta-embeddings, LMA [12] and ROBE [11] use hash\nfunctions to map each weight in the embedding table into a shared memory \ud835\udc40. For a weight \ud835\udc64\ud835\udc56,\ud835\udc57in\nthe embedding table, they take both \ud835\udc56and \ud835\udc57as the input of hash functions. LMA utilizes locality\nsensitive hashing (LSH) to map the weights of each embedding to \ud835\udc40randomly. ROBE organizes \ud835\udc40\nas a circular array and divides the flattened embedding table (i.e. concatenate all rows) into blocks\nof size \ud835\udc4d. The head of each block is mapped to \ud835\udc40randomly and the following weights in the block\nwill be mapped to the position next to the head.\n4.2\nVector Quantization\nHashing methods typically get the index vector by processing the feature id with hash functions,\nwhich fail to capture the similarity between features themselves [26]. To capture the similarity,\nvector quantization (VQ) constructs the index vectors through approximated nearest neighbor\nsearch (ANNS). Specifically, for a feature with an original embedding of \ud835\udc86, VQ gets its index vector\nas \ud835\udc70= one-hot(arg max\ud835\udc58sim(\ud835\udc86, T\ud835\udc58)) \u2208{0, 1}\ud835\udc5a, where T\ud835\udc58is the \ud835\udc58-th meta-embedding in the meta-\ntable T \u2208R\ud835\udc5a\u00d7\ud835\udc51and sim(\u00b7) is a similarity function (e.g., Euclidean distance). In other words, VQ\ntakes the original embedding as input and quantizes it into its most similar meta-embedding. Note\nthat the meta-table and the meta-embedding are commonly referred to codebook and codeword in\nrecent literature on VQ. Here we use meta-table and meta-embedding for consistency.\nSaec [62] generates a meta-table T by clustering the most frequent embeddings of a pretrained\nmodel and then quantizes each original embedding into a meta-embedding in T. However, assigning\nthe same meta-embedding to different features (i.e., collisions) still results in drop of accuracy,\neven though the features have some similarity. In addition, Saec cannot optimize the meta-table\ntogether with the original embeddings, which also results in suboptimal accuracy. To alleviate\nthe collisions, subsequent works adopt product quantization (PQ) [23] and additive quantization\n(AQ) [1] to quantize an embedding into multiple meta-embeddings. To optimize the meta-table\ntogether with the original embeddings, researchers usually quantize the original embeddings into\nmeta-embeddings during training and use the meta-embeddings as the input of the following\nnetwork, where the original embeddings will be optimized through STE [9].\nPQ considers an embedding as a concatenation of several segments (i.e., \ud835\udc86= \u00d0\ud835\udc60\n\ud835\udc56=1 \ud835\udc86\ud835\udc56). Each\nsegment \ud835\udc86\ud835\udc56corresponds to a meta-table T\ud835\udc56. At training, an embedding \ud835\udc86is quantized as \u00d0\ud835\udc60\n\ud835\udc56=1 \ud835\udc70\ud835\udc56\u00d7 T\ud835\udc56,\nwhere \ud835\udc70\ud835\udc56= one-hot(arg max\ud835\udc58sim(\ud835\udc86\ud835\udc56, T\ud835\udc56\n\ud835\udc58)). In other words, PQ quantizes each segment into its most\nsimilar meta-embedding in the corresponding meta-table. After training, the original embeddings are\ndiscarded and only the meta-tables are preserved. Since the selection of a meta-embedding in a meta-\ntable can be compactly encoded by log \ud835\udc41bits, where \ud835\udc41is the size of the meta-table, an embedding\ncan now be stored by \ud835\udc60log \ud835\udc41bits with the help of the meta-tables. Further, MGQE [26] takes the\nfeature frequency into consideration when using PQ. Specifically, it divides the embeddings of items\ninto\ud835\udc5agroups in ascending order of frequency as G = {E1, E2, ..., E\ud835\udc5a} and defines N = {\ud835\udc5b1,\ud835\udc5b2, ...,\ud835\udc5b\ud835\udc5a}\nwhere \ud835\udc5b1 < \ud835\udc5b2 < ... < \ud835\udc5b\ud835\udc5a. The embeddings in \ud835\udc56-th group can only be quantized into the first \ud835\udc5b\ud835\udc56\nmeta-embeddings in each meta-table. Similarly, xLightFM [24] performs PQ in each feature field.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:13\nConsidering that the feature fields have various size, xLightFM searches for the optimal size (i.e.,\nthe number of meta-embeddings) of the meta-tables for each field. The search process is achieved\nby the DARTS algorithm which is similar as the embedding dimension search in Section 3.2.\nSimilar to PQ, AQ considers an embedding as a summation of \ud835\udc60vectors: \ud835\udc86= \u00cd\ud835\udc60\n\ud835\udc56=1 \ud835\udc86\ud835\udc56. AQ generates\nits quantized embeddings by \u00cd\ud835\udc60\n\ud835\udc56=1 \ud835\udc70\ud835\udc56\u00d7 T\ud835\udc56, \ud835\udc70\ud835\udc56= one-hot(arg max\ud835\udc58sim(\ud835\udc86\u2212\u00cd\ud835\udc56\u22121\n\ud835\udc63=1 T\ud835\udc63\n\ud835\udc58\ud835\udc63, T\ud835\udc56\n\ud835\udc58)) where \ud835\udc58\ud835\udc63\nis the index of the selected meta-embedding in the \ud835\udc63-th meta-table. Specifically, the first meta-table\ntakes the embedding \ud835\udc86as input, and outputs its nearest meta-embedding T1\n\ud835\udc581, the second meta-\ntable then quantizes the residual part (\ud835\udc86\u2212T1\n\ud835\udc581) into T2\n\ud835\udc582 and so on. The final output embedding\n\u02c6\n\ud835\udc86= \u00cd\ud835\udc60\n\ud835\udc56=1 T\ud835\udc56\n\ud835\udc58\ud835\udc56. LightRec [34] adopts AQ to compress the item embeddings and uses a pretrained\nmodel as a teacher to train the meta-tables effectively. LISA [63] utilizes AQ to compress the DLRMs\nwhere self-attention is performed for sequence processing. Note that there is a mass of inner product\nbetween embeddings in self-attention which suffer from extremely expensive computing costs. To\nalleviates this problem, LISA pre-calculates the inner product between meta-embeddings in the\nsame meta-table and stores the results in a small table after training. Then, the inner product of\nembeddings in self-attention can be calculated by summing the inner product of meta-embeddings\nwhich can accelerate the inference significantly.\n4.3\nDecomposition\nHashing and vector quantization use one-hot index vectors to perform a hard selection (i.e., selecting\nonly one meta-embedding) in the meta-table and alleviate the collisions between features by\nmaintaining multiply meta-tables. On the contrary, decomposition approaches make a soft selection\nby summing over all the meta-embeddings in a meta-table T \u2208R\ud835\udc5a\u00d7\ud835\udc51with a real-valued index vector\n\ud835\udc70\u2208R\ud835\udc5a. Due to the wide representation space of the real-valued index vectors, one meta-table is\nsufficient to resolve the collisions between features. Each feature will have a unique index vector\nstored in the index matrix I\ud835\udc40\u2208R\ud835\udc5b\u00d7\ud835\udc5awhen formulating the decomposition as E = I\ud835\udc40\u00d7 T.\nMLET [14] factorizes the embedding table E \u2208R\ud835\udc5b\u00d7\ud835\udc51in terms of I\ud835\udc40\u2208R\ud835\udc5b\u00d7\ud835\udc5aand T \u2208\ud835\udc45\ud835\udc5a\u00d7\ud835\udc51.\nDifferent from the low-rank decomposition where \ud835\udc5a< \ud835\udc51, MLET decomposes the embedding\ntable into larger matrices (i.e., \ud835\udc5a> \ud835\udc51) at training to ensure a larger optimization space. After\ntraining, MLET generates the embedding table as E = I\ud835\udc40\u00d7 T for memory reduction and fast\nretrieval. ANT [35] adopt a better initialization for T and imposes a sparse constraint on I\ud835\udc40.\nSpecifically, ANT initializes the meta-table T by clustering the embeddings of a pretrained model.\nIn addition, to reduce redundancy, ANT use an \u21131 penalty on I\ud835\udc40and constrain its domain to be\nnon-negative. Instead of learning and storing the index vectors at training, DHE [27] develop\na hash encoder H : N \u2192R\ud835\udc5ato map each feature id into an index vector \ud835\udc70\u2208R\ud835\udc5aon the fly.\nSpecifically, H (\ud835\udc65) = [\u210e1(\ud835\udc65),\u210e2(\ud835\udc65), ...,\u210e\ud835\udc5a(\ud835\udc65)], where {\u210e\ud835\udc56}\ud835\udc5a\n\ud835\udc56=1 is a group of hash functions. With\nthe hash encoder, DHE can eliminate the storage and optimization of I\ud835\udc40. Moreover, considering\nthat the index vectors are deterministic and cannot be optimized, DHE further decomposes the\nmeta-table \ud835\udc47into a multi-layer neural network to enhance its expressive ability.\nDifferent from the above naive decomposition, [56, 64, 71] use tensor train decomposition (TTD)\nto decompose the embedding tables. As shown in Figure 6, the embedding table E \u2208R\ud835\udc5b\u00d7\ud835\udc51will\nfirst be reshaped into E \u2208R(\ud835\udc5b1\ud835\udc511)\u00d7(\ud835\udc5b2\ud835\udc512)\u00d7...\u00d7(\ud835\udc5b\ud835\udc60\ud835\udc51\ud835\udc60), where \ud835\udc5b= \u00ce\ud835\udc60\n\ud835\udc56=1 \ud835\udc5b\ud835\udc56and \ud835\udc51= \u00ce\ud835\udc60\n\ud835\udc56=1 \ud835\udc51\ud835\udc56. Then, E will\nbe decomposed as E = G1 \u00d7 G2 \u00d7 ... \u00d7 G\ud835\udc60, where G\ud835\udc56\u2208R\ud835\udc5f\ud835\udc56\u22121\u00d7\ud835\udc5b\ud835\udc56\ud835\udc51\ud835\udc56\u00d7\ud835\udc5f\ud835\udc56. {G\ud835\udc56}\ud835\udc60\n\ud835\udc56=1 are called TT-cores\nand {\ud835\udc5f\ud835\udc56}\ud835\udc60\n\ud835\udc56=0 are called TT-ranks, in particular \ud835\udc5f0 = \ud835\udc5f\ud835\udc60= 1. TT-Rec [71] is the first to use TTD on\nthe embedding tables of DLRMs. It implements optimized kernels of TTD for embedding tables.\nLLRec [56] uses TTD on the embedding tables while maintaining the prediction accuracy by\nknowledge distillation. To enhance the compression capability of TTD, [64] further develop semi-\ntensor product based tensor train decomposition (STTD). Semi-tensor product is a generalization of\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:14\nShiwei Li and Huifeng Guo, et al.\nFig. 6. Example of TTD and STTD, where \ud835\udc5f3 = 1, \ud835\udc5f1 = \ud835\udc5f2 = \ud835\udc58= 2, \ud835\udc5f0 is 1 in TTD and is 2 in STTD.\nmatrix product. Specifically, given \ud835\udc82\u2208R1\u00d7\ud835\udc5b\ud835\udc5dand \ud835\udc83\u2208R\ud835\udc5d, \ud835\udc82can be cut into \ud835\udc5dblocks {\ud835\udc82\ud835\udc56\u2208R1\u00d7\ud835\udc5b}\ud835\udc5d\n\ud835\udc56=1\nand \ud835\udc82\u22c9\ud835\udc83= \u00cd\ud835\udc5d\n\ud835\udc56=1 \ud835\udc82\ud835\udc56\u00d7 \ud835\udc83\ud835\udc56\u2208R1\u00d7\ud835\udc5b, where \u22c9is the left semi-tensor product. For matrices A \u2208R\u210e\u00d7\ud835\udc5b\ud835\udc5d\nand B \u2208R\ud835\udc5d\u00d7\ud835\udc5e, A \u22c9B \u2208R\u210e\u00d7\ud835\udc5b\ud835\udc5econtains \u210e\u00d7 \ud835\udc5eblocks and each block is the semi-tensor product\nbetween a row of A and a column of B. [64] replaces the conventional matrix tensor product of\nTTD with the left semi-tensor product. As Figure 6 shows, in STTD, E = \u02c6\nG1 \u22c9\u02c6\nG2 \u22c9... \u22c9\u02c6\nG\ud835\udc60, where\n\u02c6\nG\ud835\udc56\u2208R\n\ud835\udc5f\ud835\udc56\u22121\n\ud835\udc58\n\u00d7 \ud835\udc5b\ud835\udc56\ud835\udc51\ud835\udc56\n\ud835\udc58\n\u00d7\ud835\udc5f\ud835\udc56,\ud835\udc5f0 = \ud835\udc58and \ud835\udc5f\ud835\udc60= 1. In addition, [64] uses self-supervised knowledge distillation\nto to reduce accuracy loss from compression.\n4.4\nDiscussion\nWeight sharing approaches usually make remarkable reduction to the memory usage. However,\nthey suffer from low efficiency at training due to extra computing cost for generating embeddings,\nespecially the nearest neighbor search in vector quantization and the matrix multiplication in\ndecomposition approaches. The extra computing cost will also slow down the inference speed except\nin vector quantization where we can store the results of inner product between meta-embeddings\nto accelerate the inference. Nevertheless, vector quantization maintains the original embeddings\nduring training which requires extra memory usage. Moreover, these methods usually cannot\nimprove the prediction accuracy, especially hashing usually causes severe drop of accuracy.\n5\nSUMMARY\nEmbedding tables usually constitute a large portion of model parameters in DLRMs, which need\nto be compressed for efficient and economical deployment. As recommender systems continue to\ngrow in scale, embedding compression has attracted more and more attention. In this survey, we\nprovide a comprehensive review of the embedding compression methods in recommender systems,\naccompanied by a systematic and rational organization of existing studies.\nThe embedding table can be conceptualized as a matrix with three dimensions, namely the\nprecision of weights, the dimension of embeddings, and the number of embeddings. Consequently,\nwe classify embedding compression methods into three primary categories according to the dimen-\nsions they compress, which are low-precision, mixed-dimension, and weight-sharing, respectively.\nLow-precision methods reduce the memory of each weight by decreasing its bit width, including\nbinarization and quantization. Mixed-dimension methods reduce the memory of specific embed-\ndings by decreasing their dimensions, including rule-based approaches, NAS-based approaches and\npruning. Weight-sharing methods reduce the actual parameters of the embedding table by sharing\nweights among different embeddings, including hashing, vector quantization and decomposition.\n5.1\nGeneral Suggestions\nIn the above sections, we have discussed the pros and cons of different compression methods in\ndetail. However, there are no golden criteria to measure which one is the best. How to choose\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:15\na proper compression method depends greatly on the application scenarios and requirements.\nTherefore, we offer some general suggestions for the common requirements on the key metrics\ndiscussed in Section 1.2, namely model accuracy, inference efficiency, training efficiency, and\ntraining memory usage, respectively.\n\u2022 Model accuracy. In scenarios that demand high model accuracy, any accuracy degradation\ncaused by compression is deemed unacceptable. In such cases, mixed-dimension methods are\nrecommended as they have been reported to remove redundant parameters and avoid model\noverfitting. With an appropriate compression ratio, mixed-dimension methods can effectively\ncompress embeddings while maintaining or even improving accuracy. Furthermore, accuracy\ncan also be preserved when compressing embeddings by quantization with a higher bit width.\nFor instance, using a 16-bit representation has been proven to be sufficient for achieving accurate\nresults. On the contrary, for scenarios that do not require high prediction accuracy, quantization\nwith lower bit width or even binarization (1-bit) can be employed to achieve stronger compression.\n\u2022 Inference efficiency. In scenarios such as online inference, model inference efficiency is of\nparamount importance. Generally speaking, most embedding compression methods will not have\na great negative impact on the inference speed. However, in several decomposition methods\nwhere the embedding table is decomposed into multiple small matrices, the process of recovering\nembeddings may introduce significant inference latency and should be avoided in this context.\nTo improve inference efficiency while compressing embeddings, vector quantization is suggested,\nas the feature interaction (e.g., inner-product) of embeddings can be pre-calculated to accelerate\nthe inference process. Additionally, binarization is also worth considering when there is no\nhigh requirement on model accuracy. The calculation of feature interactions between binary\nembeddings is faster compared to that between full-precision embeddings.\n\u2022 Training efficiency. In scenarios where the models are supposed to be updated in a timely man-\nner, training efficiency becomes a critical factor. However, it is unfortunate that most embedding\ncompression methods do not contribute to improving the training efficiency. In fact, some of\nthem may significantly reduce training efficiency, particularly NAS-based approaches, pruning,\nvector quantization, and decomposition. Specifically, NAS-based approaches involve complex\ncalculations to search for optimal embedding dimensions, which can be computationally intensive\nand time-consuming. Pruning often necessitates retraining to achieve higher accuracy, resulting\nin additional training overhead. Vector quantization also involves cumbersome calculations for\nnearest neighbor searches. Decomposition may require multiple matrix multiplications to recover\nand retrieve embeddings. Therefore, in scenarios that prioritize training efficiency and timely\nmodel updates, these methods are not recommended.\n\u2022 Training memory usage. In scenarios where computing devices have limited memory, it is\ndesirable to compress the training memory usage of embeddings or, at the very least, avoid\nincreasing it. In such cases, we suggest using rule-based approaches, hashing, or decomposition,\nas they can compress the embedding table before training. Besides, the low-precision training of\nquantization is also worth considering, as the embeddings are stored in the format of integers\nduring training. On the contrary, NAS-based approaches and vector quantization are not recom-\nmended in this context. They often require storing a significant number of intermediate results\nto guide the training process, which will consume more memory.\n5.2\nFuture Prospects\nEmbedding compression in recommender systems has witnessed rapid development and notable\nachievements, although there are still several challenging issues that require attention. We identify\nseveral potential directions for further research in this field.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:16\nShiwei Li and Huifeng Guo, et al.\n\u2022 Low-precision. The key problem faced by low-precision methods is the severe accuracy degrada-\ntion at extremely lower bit widths. In view of the extensive and advanced research on quantization\nand binarization in the deep learning community, we can refer to related techniques to alleviate\nthe accuracy loss when compressing embeddings, which is quite challenging and valuable.\n\u2022 Mixed-dimension. In recent advanced mixed-dimension methods, there is a need to enhance\nthe training efficiency of NAS-based approaches and pruning. To address this, we recommend\ndesigning lighter NAS frameworks that can efficiently search for the optimal embedding dimen-\nsion. On the other hand, finding solutions to avoid retraining pruned models is also crucial for\nenhancing training efficiency. Furthermore, while numerous studies have demonstrated the sig-\nnificant impact of the embedding dimension on model accuracy, there is still a lack of theoretical\nunderstanding regarding how the embedding dimension precisely affects model accuracy. Having\na solid theoretical basis would be invaluable in guiding the optimal selection of embedding\ndimensions, enabling more efficient and effective model training.\n\u2022 Weight-sharing. To approach the limit of weight sharing methods, we believe that an intriguing\ndirection to explore is the use of embedding generation networks. Considering the powerful\nrepresentation capabilities of neural networks, we may learn a powerful neural network to\ngenerate embeddings, instead of directly learning and maintaining the embeddings themselves.\n\u2022 Hybrid approaches. Since the methods within the three primary categories compress the\nembeddings from different dimensions and enjoy different advantages, we expect future research\nto establish a unified method for compressing multiple dimensions, or develop hybrid approaches\ncombining these techniques. By integrating the strengths of different compression methods, it is\npossible to create more powerful and comprehensive compression algorithms.\n\u2022 Open benchmarks. This review offers a thorough discussion of embedding compression meth-\nods. However, we did not undertake an experimental comparison across these methods. On one\nhand, distinct methods are applied to different tasks in recommender systems, each of which has\nunique accuracy metrics. For example, in click-through rate (CTR) prediction, the commonly\nused metric is the Area Under the Curve (AUC); whereas for rating prediction, Root Mean Square\nError (RMSE) and Mean Absolute Error (MAE) are typically employed; for Top-N recommen-\ndations, Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)\nare commonly utilized as accuracy metrics. On the other hand, a majority of research relies on\nproprietary datasets without sharing open-source code, presenting obstacles to reproducibility\nand comparative analyses. Nonetheless, the implementation of these methods is not inherently\ncomplex. Given the focus on the embedding tables, a solution involves the definition of a new\nembedding module during implementation, coupled with the rewriting of the lookup operation\nfor the embedding vector. Therefore, it is necessary to establish a foundational benchmark to\nevaluate the effectiveness of distinct methods across a spectrum of tasks, like BARS [81], a\nbenchmark designed for recommendations. We posit that this would substantially expedite the\napplication and advancement of this field.\nACKNOWLEDGMENTS\nThis work is supported in part by National Natural Science Foundation of China under grants\n62376103, 62302184, 62206102, and Science and Technology Support Program of Hubei Province\nunder grant 2022BAA046.\nREFERENCES\n[1] Artem Babenko and Victor S. Lempitsky. 2014. Additive Quantization for Extreme Vector Compression. In 2014\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014. IEEE\nComputer Society, Columbus, OH, USA, 931\u2013938.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:17\n[2] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. 2020. LSQ+: Improving low-bit\nquantization through learnable offsets and better initialization. In 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020. Computer Vision Foundation / IEEE,\n2978\u20132985.\n[3] Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, and Meng Wang. 2021. Learning Elastic Embeddings for\nCustomizing On-Device Recommenders. In KDD \u201921: The 27th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining, Virtual Event, Singapore, August 14-18, 2021. ACM, 138\u2013147.\n[4] Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. 2016. Compressing Neural Language Models by Sparse Word\nRepresentations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,\nAugust 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.\n[5] Yankai Chen, Yifei Zhang, Yingxue Zhang, Huifeng Guo, Jingjie Li, Ruiming Tang, Xiuqiang He, and Irwin King. 2021.\nTowards Low-loss 1-bit Quantization of User-item Representations for Top-K Recommendation. CoRR abs/2112.01944\n(2021).\n[6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg\nCorrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal\nShah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for\nRecommender Systems, DLRS@RecSys 2016, Boston, MA, USA, September 15, 2016. ACM, 7\u201310.\n[7] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Differentiable Neural Input Search for Recommender Systems.\nCoRR abs/2006.04466 (2020).\n[8] Tejalal Choudhary, Vipul Kumar Mishra, Anurag Goswami, and Jagannathan Sarangapani. 2020. A comprehensive\nsurvey on model compression and acceleration. Artif. Intell. Rev. 53, 7 (2020), 5113\u20135155.\n[9] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. BinaryConnect: Training Deep Neural Networks\nwith binary weights during propagations. In Advances in Neural Information Processing Systems 28: Annual Conference\non Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada. 3123\u20133131.\n[10] Wei Deng, Junwei Pan, Tian Zhou, Deguang Kong, Aaron Flores, and Guang Lin. 2021. DeepLight: Deep Lightweight\nFeature Interactions for Accelerating CTR Predictions in Ad Serving. In WSDM \u201921, The Fourteenth ACM International\nConference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021. ACM, 922\u2013930.\n[11] Aditya Desai, Li Chou, and Anshumali Shrivastava. 2022. Random Offset Block Embedding (ROBE) for compressed\nembedding tables in deep learning recommendation systems. Proceedings of Machine Learning and Systems 4 (2022),\n762\u2013778.\n[12] Aditya Desai, Yanzhou Pan, Kuangyuan Sun, et al. 2021. Semantically Constrained Memory Allocation (SCMA) for\nEmbedding in Efficient Recommendation Systems. CoRR abs/2103.06124 (2021).\n[13] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. 2020.\nLearned Step Size quantization. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\n[14] Benjamin Ghaemmaghami, Zihao Deng, Benjamin Y. Cho, et al. 2020. Training with Multi-Layer Embeddings for\nModel Reduction. CoRR abs/2006.05623 (2020).\n[15] Antonio A. Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021. Mixed Dimension\nEmbeddings with Application to Memory-Efficient Recommendation Systems. In IEEE International Symposium on\nInformation Theory, ISIT 2021, Melbourne, Australia, July 12-20, 2021. IEEE, 2786\u20132791.\n[16] Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, and Hector Yuen. 2019. Post-Training 4-bit Quantization on\nEmbedding Tables. CoRR abs/1911.02079 (2019).\n[17] Huifeng Guo, Wei Guo, Yong Gao, Ruiming Tang, Xiuqiang He, and Wenzhi Liu. 2021. ScaleFreeCTR: MixCache-based\nDistributed Training System for CTR Models with Huge Embedding Table. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021. ACM,\n1269\u20131278.\n[18] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine\nbased Neural Network for CTR Prediction. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial\nIntelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017. ijcai.org, 1725\u20131731.\n[19] Manish Gupta and Puneet Agrawal. 2022. Compression of Deep Learning Models for Text: A Survey. ACM Trans.\nKnowl. Discov. Data 16, 4 (2022), 61:1\u201361:55.\n[20] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. 2020. GhostNet: More Features From\nCheap Operations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,\nUSA, June 13-19, 2020. 1577\u20131586.\n[21] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A Survey of the State-of-the-Art. Knowledge-Based Systems\n212 (2021), 106622.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:18\nShiwei Li and Huifeng Guo, et al.\n[22] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization with Gumbel-Softmax. In 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\nOpenReview.net.\n[23] Herv\u00e9 J\u00e9gou, Matthijs Douze, and Cordelia Schmid. 2011. Product Quantization for Nearest Neighbor Search. IEEE\nTrans. Pattern Anal. Mach. Intell. 33, 1 (2011), 117\u2013128.\n[24] Gangwei Jiang, Hao Wang, Jin Chen, Haoyu Wang, Defu Lian, and Enhong Chen. 2021. xLightFM: Extremely Memory-\nEfficient Factorization Machine. In SIGIR \u201921: The 44th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Virtual Event, Canada, July 11-15, 2021. ACM, 337\u2013346.\n[25] Manas R. Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav Khaitan, Jiahui Liu, and\nQuoc V. Le. 2020. Neural Input Search for Large Scale Recommendation Models. In KDD \u201920: The 26th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020. ACM, 2387\u20132397.\n[26] Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin, Lichan Hong, and Ed H. Chi. 2020.\nLearning Multi-granular Quantized Embeddings for Large-Vocab Categorical Features in Recommender Systems. In\nCompanion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020. ACM / IW3C2, 562\u2013566.\n[27] Wang-Cheng Kang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Ting Chen, Lichan Hong, and Ed H. Chi.\n2021. Learning to Embed Categorical Features without Embedding Tables for Recommendation. In KDD \u201921: The 27th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021. ACM,\n840\u2013850.\n[28] Wang-Cheng Kang and Julian John McAuley. 2019. Candidate Generation with Binary Codes for Large-Scale Top-N\nRecommendation. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management,\nCIKM 2019, Beijing, China, November 3-7, 2019. ACM, 1523\u20131532.\n[29] Farhan Khawar, Xu Hang, Ruiming Tang, Bin Liu, Zhenguo Li, and Xiuqiang He. 2020. AutoFeature: Searching for\nFeature Interactions and Their Architectures for Click-through Rate Prediction. In CIKM \u201920: The 29th ACM International\nConference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020. ACM, 625\u2013634.\n[30] Houyi Li, Zhihong Chen, Chenliang Li, Rong Xiao, Hongbo Deng, Peng Zhang, Yongchao Liu, and Haihong Tang.\n2021. Path-based Deep Network for Candidate Item Matching in Recommenders. In SIGIR \u201921: The 44th International\nACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021.\nACM, 1493\u20131502.\n[31] Shiwei Li, Huifeng Guo, Lu Hou, Wei Zhang, Xing Tang, Ruiming Tang, Rui Zhang, and Ruixuan Li. 2023. Adaptive\nLow-Precision Training for Embeddings in Click-Through Rate Prediction. In Thirty-Seventh AAAI Conference on\nArtificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023,\nThirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February\n7-14, 2023. AAAI Press, 4435\u20134443.\n[32] Defu Lian, Yong Ge, Fuzheng Zhang, Nicholas Jing Yuan, Xing Xie, Tao Zhou, and Yong Rui. 2015. Content-Aware\nCollaborative Filtering for Location Recommendation Based on Human Mobility Data. In 2015 IEEE International\nConference on Data Mining, ICDM 2015, Atlantic City, NJ, USA, November 14-17, 2015. IEEE Computer Society, 261\u2013270.\n[33] Defu Lian, Rui Liu, Yong Ge, Kai Zheng, Xing Xie, and Longbing Cao. 2017. Discrete Content-aware Matrix Factorization.\nIn Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS,\nCanada, August 13 - 17, 2017. ACM, 325\u2013334.\n[34] Defu Lian, Haoyu Wang, Zheng Liu, Jianxun Lian, Enhong Chen, and Xing Xie. 2020. LightRec: A Memory and\nSearch-Efficient Recommender System. In WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020. ACM\n/ IW3C2, 695\u2013705.\n[35] Paul Pu Liang, Manzil Zaheer, Yuan Wang, and Amr Ahmed. 2021. Anchor & Transform: Learning Sparse Embeddings\nfor Large Vocabularies. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\n[36] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature Generation by\nConvolutional Neural Network for Click-Through Rate Prediction. In The World Wide Web Conference, WWW 2019,\nSan Francisco, CA, USA, May 13-17, 2019. ACM, 1119\u20131129.\n[37] Han Liu, Xiangnan He, Fuli Feng, Liqiang Nie, Rui Liu, and Hanwang Zhang. 2018. Discrete Factorization Machines for\nFast Feature-based Recommendation. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial\nIntelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, 3449\u20133455.\n[38] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2019. DARTS: Differentiable Architecture Search. In 7th International\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\n[39] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020. Automated Embedding Size Search in\nDeep Recommender Systems. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020. ACM, 2307\u20132316.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:19\n[40] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A Convolutional Click Prediction Model. In Proceedings of the\n24th ACM International Conference on Information and Knowledge Management, CIKM 2015, Melbourne, VIC, Australia,\nOctober 19 - 23, 2015. ACM, 1743\u20131746.\n[41] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable Embedding sizes for Recommender\nSystems. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net.\n[42] Xianglong Liu, Junfeng He, Cheng Deng, and Bo Lang. 2014. Collaborative Hashing. In 2014 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014. IEEE Computer Society,\n2147\u20132154.\n[43] Fuyuan Lyu, Xing Tang, Hong Zhu, Huifeng Guo, Yingxue Zhang, Ruiming Tang, and Xue Liu. 2022. OptEmbed:\nLearning Optimal Embedding Table for Click-through Rate Prediction. In Proceedings of the 31st ACM International\nConference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022. ACM, 1399\u20131409.\n[44] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire Space Multi-\nTask Model: An Effective Approach for Estimating Post-Click Conversion Rate. In The 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018, Kevyn\nCollins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine Yilmaz (Eds.). ACM, 1137\u20131140.\n[45] Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao, and Xiuqiang He. 2021. SimpleX:\nA Simple and Strong Baseline for Collaborative Filtering. In CIKM \u201921: The 30th ACM International Conference on\nInformation and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021. ACM, 1243\u20131252.\n[46] H. Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips,\nEugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom\nBoulos, and Jeremy Kubica. 2013. Ad click prediction: a view from the trenches. In The 19th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013. ACM, 1222\u20131230.\n[47] Niketan Pansare, Jay Katukuri, Aditya Arora, Frank Cipollone, Riyaaz Shaik, Noyan Tokgozoglu, and Chandru\nVenkataraman. 2022. Learning Compressed Embeddings for On-Device Inference. In Proceedings of Machine Learning\nand Systems 2022, MLSys 2022, Santa Clara, CA, USA, August 29 - September 1, 2022. mlsys.org.\n[48] Liang Qu, Yonghong Ye, Ningzhi Tang, Lixin Zhang, Yuhui Shi, and Hongzhi Yin. 2022. Single-shot Embedding\nDimension Search in Recommender System. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. ACM, 513\u2013522.\n[49] Steffen Rendle. 2010. Factorization Machines. In ICDM 2010, The 10th IEEE International Conference on Data Mining,\nSydney, Australia, 14-17 December 2010. IEEE Computer Society, 995\u20131000.\n[50] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional Embeddings Using\nComplementary Partitions for Memory-Efficient Recommendation Systems. In KDD \u201920: The 26th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020. ACM, 165\u2013175.\n[51] Yixin Su, Rui Zhang, Sarah M. Erfani, and Zhenghua Xu. 2021. Detecting Beneficial Feature Interactions for Rec-\nommender Systems. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference\non Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 4357\u20134365.\n[52] Yixin Su, Yunxiang Zhao, Sarah M. Erfani, Junhao Gan, and Rui Zhang. 2022. Detecting Arbitrary Order Beneficial\nFeature Interactions for Recommender Systems. In KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Washington, DC, USA, August 14 - 18, 2022. ACM, 1676\u20131686.\n[53] Yang Sun, Fajie Yuan, Min Yang, Guoao Wei, Zhou Zhao, and Duo Liu. 2020. A Generic Network Compression\nFramework for Sequential Recommender Systems. In Proceedings of the 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020. ACM, 1299\u20131308.\n[54] Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu. 2020. Learning to Hash with Graph\nNeural Networks for Recommender Systems. In WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020.\nACM / IW3C2, 1988\u20131998.\n[55] Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng,\nand Shu-Tao Xia. 2023. MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation\nfor Recommendation. In Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON,\nCanada, 29 October 2023- 3 November 2023. ACM, 6548\u20136557.\n[56] Qinyong Wang, Hongzhi Yin, Tong Chen, Zi Huang, Hao Wang, Yanchang Zhao, and Nguyen Quoc Viet Hung. 2020.\nNext Point-of-Interest Recommendation on Resource-Constrained Mobile Devices. In WWW \u201920: The Web Conference\n2020, Taipei, Taiwan, April 20-24, 2020. ACM / IW3C2, 906\u2013916.\n[57] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In\nProceedings of the ADKDD\u201917, Halifax, NS, Canada, August 13 - 17, 2017. ACM, 12:1\u201312:7.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:20\nShiwei Li and Huifeng Guo, et al.\n[58] Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed H. Chi. 2021. DCN\nV2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW \u201921:\nThe Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. ACM / IW3C2, 1785\u20131797.\n[59] Zhikun Wei, Xin Wang, and Wenwu Zhu. 2021. AutoIAS: Automatic Integrated Architecture Searcher for Click-Trough\nRate Prediction. In CIKM \u201921: The 30th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Queensland, Australia, November 1 - 5, 2021. ACM, 2101\u20132110.\n[60] Kilian Q. Weinberger, Anirban Dasgupta, John Langford, Alexander J. Smola, and Josh Attenberg. 2009. Feature hashing\nfor large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning,\nICML 2009, Montreal, Quebec, Canada, June 14-18, 2009 (ACM International Conference Proceeding Series, Vol. 382). ACM,\n1113\u20131120.\n[61] Le Wu, Xiangnan He, Xiang Wang, Kun Zhang, and Meng Wang. 2023. A Survey on Accuracy-Oriented Neural\nRecommendation: From Collaborative Filtering to Information-Rich Recommendation. IEEE Trans. Knowl. Data Eng.\n35, 5 (2023), 4425\u20134445.\n[62] Xiaorui Wu, Hong Xu, Honglin Zhang, Huaming Chen, and Jian Wang. 2020. Saec: similarity-aware embedding\ncompression in recommendation systems. In APSys \u201920: 11th ACM SIGOPS Asia-Pacific Workshop on Systems, Tsukuba,\nJapan, August 24-25, 2020. ACM, 82\u201389.\n[63] Yongji Wu, Defu Lian, Neil Zhenqiang Gong, Lu Yin, Mingyang Yin, Jingren Zhou, and Hongxia Yang. 2021. Linear-\nTime Self Attention with Codeword Histogram for Efficient Recommendation. In WWW \u201921: The Web Conference 2021,\nVirtual Event / Ljubljana, Slovenia, April 19-23, 2021. ACM / IW3C2, 1262\u20131273.\n[64] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and Quoc Viet Hung Nguyen. 2022. On-Device\nNext-Item Recommendation with Self-Supervised Knowledge Distillation. In SIGIR \u201922: The 45th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. ACM, 546\u2013555.\n[65] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional Factorization Machines:\nLearning the Weight of Feature Interactions via Attention Networks. In Proceedings of the Twenty-Sixth International\nJoint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017. ijcai.org, 3119\u20133125.\n[66] Zhiqiang Xu, Dong Li, Weijie Zhao, Xing Shen, Tianbo Huang, Xiaoyun Li, and Ping Li. 2021. Agile and Accurate CTR\nPrediction Model Training for Massive-Scale Online Advertising Systems. In SIGMOD \u201921: International Conference on\nManagement of Data, Virtual Event, China, June 20-25, 2021. ACM, 2404\u20132409.\n[67] Bencheng Yan, Pengjie Wang, Jinquan Liu, Wei Lin, Kuang-Chih Lee, Jian Xu, and Bo Zheng. 2021. Binary Code based\nHash Embedding for Web-scale Applications. In CIKM \u201921: The 30th ACM International Conference on Information and\nKnowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021. ACM, 3563\u20133567.\n[68] Bencheng Yan, Pengjie Wang, Kai Zhang, Wei Lin, Kuang-Chih Lee, Jian Xu, and Bo Zheng. 2021. Learning Effective\nand Efficient Embedding via an Adaptively-Masked Twins-based Layer. In CIKM \u201921: The 30th ACM International\nConference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021.\nACM, 3568\u20133572.\n[69] Jie Amy Yang, Jianyu Huang, Jongsoo Park, Ping Tak Peter Tang, and Andrew Tulloch. 2020. Mixed-Precision\nEmbedding Using a Cache. CoRR abs/2010.11305 (2020).\n[70] Quanming Yao, Xiangning Chen, James T. Kwok, Yong Li, and Cho-Jui Hsieh. 2020. Efficient Neural Interaction\nFunction Search for Collaborative Filtering. In WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020.\nACM / IW3C2, 1660\u20131670.\n[71] Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. 2021. TT-Rec: Tensor Train Compression for Deep Learning\nRecommendation Models. In Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual, April 5-9, 2021.\nmlsys.org.\n[72] Caojin Zhang, Yicun Liu, Yuanpu Xie, Sofia Ira Ktena, Alykhan Tejani, Akshay Gupta, Pranay Kumar Myana, Deepak\nDilipkumar, Suvadip Paul, Ikuhiro Ihara, Prasang Upadhyaya, Ferenc Huszar, and Wenzhe Shi. 2020. Model Size\nReduction Using Frequency Based Double Hashing for Recommender Systems. In RecSys 2020: Fourteenth ACM\nConference on Recommender Systems, Virtual Event, Brazil, September 22-26, 2020. ACM, 521\u2013526.\n[73] Hanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, and Tat-Seng Chua. 2016. Discrete Collaborative\nFiltering. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information\nRetrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016. ACM, 325\u2013334.\n[74] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep Learning Based Recommender System: A Survey and New\nPerspectives. ACM Comput. Surv. 52, 1 (2019), 5:1\u20135:38.\n[75] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep Learning for Click-Through Rate\nEstimation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual\nEvent / Montreal, Canada, 19-27 August 2021. ijcai.org, 4695\u20134703.\n[76] Yan Zhang, Defu Lian, and Guowu Yang. 2017. Discrete Personalized Ranking for Fast Collaborative Filtering from\nImplicit Feedback. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:21\nFrancisco, California, USA. AAAI Press, 1669\u20131675.\n[77] Zhiwei Zhang, Qifan Wang, Lingyun Ruan, and Luo Si. 2014. Preference preserving hashing for efficient recommenda-\ntion. In The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201914,\nGold Coast , QLD, Australia - July 06 - 11, 2014. ACM, 183\u2013192.\n[78] Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing\nLiu, and Xiwang Yang. 2021. AutoEmb: Automated Embedding Dimensionality Search in Streaming Recommendations.\n(2021), 896\u2013905.\n[79] Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, Sida Wang, Huiji Gao, and Bo Long. 2020.\nMemory-efficient Embedding for Recommendations. CoRR abs/2006.14827 (2020).\n[80] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai.\n2018. Deep Interest Network for Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018. ACM, 1059\u20131068.\n[81] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022. BARS:\nTowards Open Benchmarking for Recommender Systems. In SIGIR \u201922: The 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. ACM, 2912\u20132923.\n[82] Barret Zoph and Quoc V. Le. 2017. Neural Architecture Search with Reinforcement Learning. In 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\nOpenReview.net.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n",
    "ref": [
        "2112.01944",
        "2006.04466",
        "2103.06124",
        "2006.05623",
        "1911.02079",
        "2010.11305",
        "2006.14827"
    ]
}